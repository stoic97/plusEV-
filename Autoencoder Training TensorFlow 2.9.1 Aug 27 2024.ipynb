{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "29213bd0-f52e-48bc-b8d6-fef77b8c457f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-01T05:00:59.624039Z",
     "iopub.status.busy": "2024-11-01T05:00:59.623249Z",
     "iopub.status.idle": "2024-11-01T05:00:59.628057Z",
     "shell.execute_reply": "2024-11-01T05:00:59.627469Z",
     "shell.execute_reply.started": "2024-11-01T05:00:59.624018Z"
    }
   },
   "outputs": [],
   "source": [
    "### Importing the important libraries\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import os\n",
    "import datetime\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a19af088-bd0a-4971-9d3d-0c6dab58718c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-01T05:01:14.352444Z",
     "iopub.status.busy": "2024-11-01T05:01:14.351896Z",
     "iopub.status.idle": "2024-11-01T05:01:14.355169Z",
     "shell.execute_reply": "2024-11-01T05:01:14.354694Z",
     "shell.execute_reply.started": "2024-11-01T05:01:14.352419Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import mixed_precision\n",
    "\n",
    "# Set the mixed precision policy to 'mixed_float16\n",
    "\n",
    "\n",
    "tf.keras.mixed_precision.set_global_policy('float32')\n",
    "\n",
    "\n",
    "# pd.set_option('display.max_columns', None)\n",
    "# pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "52632356-42b1-4b71-a698-995eb0491748",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-01T05:01:16.728951Z",
     "iopub.status.busy": "2024-11-01T05:01:16.728392Z",
     "iopub.status.idle": "2024-11-01T05:01:16.732116Z",
     "shell.execute_reply": "2024-11-01T05:01:16.731617Z",
     "shell.execute_reply.started": "2024-11-01T05:01:16.728926Z"
    }
   },
   "outputs": [],
   "source": [
    "def add_lag_features(df, lags):\n",
    "    lag_data = {f\"{column}_lag_{lag}\": df[column].shift(lag) for column in df.columns for lag in lags}\n",
    "    lag_df = pd.DataFrame(lag_data, index=df.index)\n",
    "    df = pd.concat([df, lag_df], axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "af0c2f60-7345-454a-9091-b5e36ccb244b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-01T05:01:17.501340Z",
     "iopub.status.busy": "2024-11-01T05:01:17.500797Z",
     "iopub.status.idle": "2024-11-01T05:01:17.891311Z",
     "shell.execute_reply": "2024-11-01T05:01:17.890815Z",
     "shell.execute_reply.started": "2024-11-01T05:01:17.501308Z"
    }
   },
   "outputs": [],
   "source": [
    "df_5min = pd.read_csv('nifty50_rik_5min.csv')   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dd382cc0-d7b9-4ff9-aedb-41ef2a664505",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-01T05:01:19.291911Z",
     "iopub.status.busy": "2024-11-01T05:01:19.291258Z",
     "iopub.status.idle": "2024-11-01T05:01:19.294657Z",
     "shell.execute_reply": "2024-11-01T05:01:19.294220Z",
     "shell.execute_reply.started": "2024-11-01T05:01:19.291890Z"
    }
   },
   "outputs": [],
   "source": [
    "# Initialize the periods\n",
    "periods = [i for i in range(4, 60)]\n",
    "################################################################################################\n",
    "################ USE THIS IF YOUR INPUT DATA IS LESS THAN 5MIN OHLC PERIOD ######################\n",
    "################################################################################################\n",
    "# df['date'] = pd.to_datetime(df['date'])\n",
    "# df.set_index('date', inplace=True)\n",
    "\n",
    "# # Remove duplicated timestamps\n",
    "# if df.index.duplicated().any():\n",
    "#     print(\"There are duplicated timestamps.\")\n",
    "#     df = df[~df.index.duplicated(keep='first')]\n",
    "\n",
    "# # Resample the data to 5-minute intervals\n",
    "# df_5min = df.resample('5min').agg({\n",
    "#     'open': 'first',\n",
    "#     'high': 'max',\n",
    "#     'low': 'min',\n",
    "#     'close': 'last'\n",
    "# })\n",
    "# df_5min.dropna(how='all', inplace=True)\n",
    "# df_5min.reset_index(inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "92dee751-58aa-4579-889d-8a1392151e4c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-01T05:01:34.419187Z",
     "iopub.status.busy": "2024-11-01T05:01:34.418585Z",
     "iopub.status.idle": "2024-11-01T05:01:37.715555Z",
     "shell.execute_reply": "2024-11-01T05:01:37.714953Z",
     "shell.execute_reply.started": "2024-11-01T05:01:34.419163Z"
    }
   },
   "outputs": [],
   "source": [
    "#### adding sma and ema and their lagged values\n",
    "def add_moving_averages(df_5min, periods):\n",
    "    moving_averages = {f'SMA_{period}': df_5min['close'].rolling(window=period).mean() for period in periods}\n",
    "    moving_averages.update({f'EMA_{period}': df_5min['close'].ewm(span=period, adjust=False).mean() for period in periods})\n",
    "    moving_averages_df = pd.DataFrame(moving_averages)\n",
    "    return moving_averages_df\n",
    "df1 = add_moving_averages(df_5min,periods)\n",
    "lags = [1,2,3,4]\n",
    "df1 = add_lag_features(df1, lags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2b6776a3-c784-4b59-abcc-314b0ae875cd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-01T05:01:43.915885Z",
     "iopub.status.busy": "2024-11-01T05:01:43.915434Z",
     "iopub.status.idle": "2024-11-01T05:01:45.278673Z",
     "shell.execute_reply": "2024-11-01T05:01:45.278254Z",
     "shell.execute_reply.started": "2024-11-01T05:01:43.915864Z"
    }
   },
   "outputs": [],
   "source": [
    "#### adding Rsi and their lagged values\n",
    "def calculate_rsi(data, window):\n",
    "    diff = data.diff(1)\n",
    "    gain = (diff.where(diff > 0, 0)).fillna(0)\n",
    "    loss = (-diff.where(diff < 0, 0)).fillna(0)\n",
    "    avg_gain = gain.rolling(window=window, min_periods=1).mean()\n",
    "    avg_loss = loss.rolling(window=window, min_periods=1).mean()\n",
    "    rs = avg_gain / avg_loss\n",
    "    rsi = 100 - (100 / (1 + rs))\n",
    "    return rsi\n",
    "\n",
    "def add_rsi_features(df, periods):\n",
    "    rsi_data = {f'RSI_{period}': calculate_rsi(df['close'], window=period) for period in periods}\n",
    "    rsi_df = pd.DataFrame(rsi_data)\n",
    "    return rsi_df\n",
    "df2 = add_rsi_features(df_5min,periods)\n",
    "lags = [1,2,3,4]\n",
    "df2 = add_lag_features(df2, lags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c195c20d-bab2-4121-b357-d6f1e5f51220",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-01T05:01:52.223632Z",
     "iopub.status.busy": "2024-11-01T05:01:52.222901Z",
     "iopub.status.idle": "2024-11-01T05:01:58.296163Z",
     "shell.execute_reply": "2024-11-01T05:01:58.295743Z",
     "shell.execute_reply.started": "2024-11-01T05:01:52.223610Z"
    }
   },
   "outputs": [],
   "source": [
    "#### adding bolinger bads features and their lagged values\n",
    "def add_bollinger_bands(df, periods):\n",
    "    bollinger_dict = {}\n",
    "    for period in periods:\n",
    "        column_prefix = f'bb_{period}'\n",
    "        middle_band = df['close'].rolling(window=period).mean()\n",
    "        std_dev = df['close'].rolling(window=period).std()\n",
    "        bollinger_dict.update({\n",
    "            f'{column_prefix}_middle_band': middle_band,\n",
    "            f'{column_prefix}_upper_band': middle_band + (std_dev * 2),\n",
    "            f'{column_prefix}_lower_band': middle_band - (std_dev * 2),\n",
    "            f'{column_prefix}_bandwidth': (middle_band + (std_dev * 2)) - (middle_band - (std_dev * 2)),\n",
    "            f'{column_prefix}_percent_b': (df['close'] - (middle_band - (std_dev * 2))) / ((middle_band + (std_dev * 2)) - (middle_band - (std_dev * 2))),\n",
    "            f'{column_prefix}_cross_up': ((df['close'] > (middle_band + (std_dev * 2))) & (df['close'].shift(1) <= (middle_band + (std_dev * 2)).shift(1))).astype(int),\n",
    "            f'{column_prefix}_cross_down': ((df['close'] < (middle_band - (std_dev * 2))) & (df['close'].shift(1) >= (middle_band - (std_dev * 2)).shift(1))).astype(int)\n",
    "        })\n",
    "    bollinger_df = pd.DataFrame(bollinger_dict)\n",
    "    return bollinger_df\n",
    "df3 = add_bollinger_bands(df_5min,periods)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "492e984e-6338-4295-95a0-0993834dddbd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-01T05:02:04.451215Z",
     "iopub.status.busy": "2024-11-01T05:02:04.450461Z",
     "iopub.status.idle": "2024-11-01T05:02:07.312725Z",
     "shell.execute_reply": "2024-11-01T05:02:07.312233Z",
     "shell.execute_reply.started": "2024-11-01T05:02:04.451188Z"
    }
   },
   "outputs": [],
   "source": [
    "#### adding historical volatailty and their lagged values\n",
    "def add_historical_volatility(df, periods):\n",
    "    log_returns = np.log(df['close'] / df['close'].shift(1))\n",
    "    volatility_data = {f'historical_volatility_{period}': log_returns.rolling(window=period).std() * np.sqrt(252 * 78) for period in periods}\n",
    "    volatility_df = pd.DataFrame(volatility_data, index=df.index)\n",
    "    return volatility_df\n",
    "df4 = add_historical_volatility(df_5min,periods)\n",
    "lags = [1,2,3,4]\n",
    "df4 = add_lag_features(df4, lags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8541ff42-a7cc-4776-b3e1-412cc6c32b91",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-01T05:02:12.927627Z",
     "iopub.status.busy": "2024-11-01T05:02:12.926933Z",
     "iopub.status.idle": "2024-11-01T05:02:15.324096Z",
     "shell.execute_reply": "2024-11-01T05:02:15.323672Z",
     "shell.execute_reply.started": "2024-11-01T05:02:12.927606Z"
    }
   },
   "outputs": [],
   "source": [
    "#### adding atr and their lagged values\n",
    "def add_atr(df, periods):\n",
    "    high_low = df['high'] - df['low']\n",
    "    high_close = np.abs(df['high'] - df['close'].shift(1))\n",
    "    low_close = np.abs(df['low'] - df['close'].shift(1))\n",
    "    tr = pd.DataFrame({'High_Low': high_low, 'High_Close': high_close, 'Low_Close': low_close}).max(axis=1)\n",
    "    atr_data = {f'ATR_{period}': tr.rolling(window=period, min_periods=1).mean() for period in periods}\n",
    "    atr_df = pd.DataFrame(atr_data, index=df.index)\n",
    "    return atr_df\n",
    "\n",
    "df5 = add_atr(df_5min,periods)\n",
    "lags = [1,2,3,4]\n",
    "df5 = add_lag_features(df5, lags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "de0546ff-a607-4f1e-a81b-729adcb81a53",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-01T05:02:32.239244Z",
     "iopub.status.busy": "2024-11-01T05:02:32.238499Z",
     "iopub.status.idle": "2024-11-01T05:02:32.281974Z",
     "shell.execute_reply": "2024-11-01T05:02:32.281571Z",
     "shell.execute_reply.started": "2024-11-01T05:02:32.239221Z"
    }
   },
   "outputs": [],
   "source": [
    "#### adding distances from nearest Round number\n",
    "def calculate_distances(df, column, scales):\n",
    "    distance_data = {f'dist_to_lower_{scale}': df[column] - (df[column] // scale * scale) for scale in scales}\n",
    "    distance_data.update({f'dist_to_upper_{scale}': ((df[column] // scale + 1) * scale) - df[column] for scale in scales})\n",
    "    return distance_data\n",
    "scales = [10, 100, 1000, 10000]\n",
    "df6 = pd.DataFrame(calculate_distances(df_5min, 'close', scales))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6f186326-ecc4-4a08-b45a-5d6cc08afdaf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-01T05:02:44.554489Z",
     "iopub.status.busy": "2024-11-01T05:02:44.553886Z",
     "iopub.status.idle": "2024-11-01T05:02:46.704439Z",
     "shell.execute_reply": "2024-11-01T05:02:46.703883Z",
     "shell.execute_reply.started": "2024-11-01T05:02:44.554467Z"
    }
   },
   "outputs": [],
   "source": [
    "# Convert 'date' column to datetime and set as index\n",
    "df_5min['date1'] = pd.to_datetime(df_5min['date'])\n",
    "df_5min.set_index('date1', inplace=True)\n",
    "\n",
    "# Ensure index is of datetime type\n",
    "if not pd.api.types.is_datetime64_any_dtype(df_5min.index):\n",
    "    df_5min.index = pd.to_datetime(df_5min.index)\n",
    "\n",
    "# Extract day from datetime index\n",
    "df_5min['day'] = df_5min.index.strftime('%d-%m-%y')\n",
    "\n",
    "# Calculate daily max and low\n",
    "daily_max = df_5min.groupby('day')['high'].max().rename('daily_max')\n",
    "daily_low = df_5min.groupby('day')['low'].min().rename('daily_low')  # Add daily low calculation\n",
    "\n",
    "# Join daily max and low to the original DataFrame\n",
    "df = df_5min.join(daily_max, on='day')\n",
    "df = df.join(daily_low, on='day')  # Join daily low\n",
    "\n",
    "# Calculate distance from previous day's max and low\n",
    "df['max_to_prev_day'] = df['daily_max'].rolling(window=2, min_periods=1).max()\n",
    "df['low_to_prev_day'] = df['daily_low'].rolling(window=2, min_periods=1).min()  # Add low to previous day\n",
    "\n",
    "# Calculate distance from close to previous day's max and low\n",
    "df['dist_from_max_to_prev_day'] = df['max_to_prev_day'] - df['close']\n",
    "df['dist_from_low_to_prev_day'] = df['close'] - df['low_to_prev_day']  # Distance from low to previous day\n",
    "\n",
    "# Calculate distance from daily max and daily low\n",
    "df['dist_from_daily_max'] = df['daily_max'] - df['close']\n",
    "df['dist_from_daily_low'] = df['close'] - df['daily_low']  # Distance from daily low\n",
    "\n",
    "# Calculate rolling max and low over the last 30 days\n",
    "df['max_last_30_days'] = df['daily_max'].rolling(window='30D', min_periods=1).max()\n",
    "df['low_last_30_days'] = df['daily_low'].rolling(window='30D', min_periods=1).min()  # Add rolling low for 30 days\n",
    "\n",
    "# Calculate distance from max and low over the last 30 days\n",
    "df['dist_from_max_to_last30days'] = df['max_last_30_days'] - df['close']\n",
    "df['dist_from_low_to_last30days'] = df['close'] - df['low_last_30_days']  # Distance from low to last 30 days\n",
    "\n",
    "# Drop unnecessary columns\n",
    "df.drop(['daily_max', 'daily_low', 'max_to_prev_day', 'low_to_prev_day', 'max_last_30_days', 'low_last_30_days', 'day', 'date', 'open', 'high', 'low', 'close'], axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0642425c-a56a-4001-908c-882679f5fece",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4eb7f310-c871-4590-bdee-6a8d5395f926",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-01T05:02:48.651661Z",
     "iopub.status.busy": "2024-11-01T05:02:48.651122Z",
     "iopub.status.idle": "2024-11-01T05:02:49.457961Z",
     "shell.execute_reply": "2024-11-01T05:02:49.457395Z",
     "shell.execute_reply.started": "2024-11-01T05:02:48.651642Z"
    }
   },
   "outputs": [],
   "source": [
    "df7 = df_5min.copy()\n",
    "# Convert date to datetime if not already\n",
    "df7['date'] = pd.to_datetime(df7['date'])\n",
    "\n",
    "# Create trading_day column\n",
    "df7['trading_day'] = df7['date'].dt.date\n",
    "\n",
    "# Sort by date to ensure chronological order\n",
    "df7 = df7.sort_values('date')\n",
    "\n",
    "# Group by trading day and assign candle numbers\n",
    "df7['candle_number'] = df7.groupby('trading_day').cumcount() + 1\n",
    "\n",
    "# Clean up\n",
    "df7 = df7.drop('trading_day', axis=1)\n",
    "df7 = df7[\"candle_number\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e3896ca7-f1ed-481e-aa3a-5b28e0929b91",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-01T05:02:51.245064Z",
     "iopub.status.busy": "2024-11-01T05:02:51.244840Z",
     "iopub.status.idle": "2024-11-01T05:02:52.026938Z",
     "shell.execute_reply": "2024-11-01T05:02:52.026409Z",
     "shell.execute_reply.started": "2024-11-01T05:02:51.245047Z"
    }
   },
   "outputs": [],
   "source": [
    "###### adding week-month feature\n",
    "df_5min['date'] = pd.to_datetime(df_5min['date'])\n",
    "df_5min['day_of_week'] = df_5min['date'].dt.dayofweek\n",
    "days = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "for i, day in enumerate(days):\n",
    "    df_5min[f'is_{day.lower()}'] = (df_5min['day_of_week'] == i).astype(int)\n",
    "df_5min['month'] = df_5min['date'].dt.month\n",
    "months = ['january', 'february', 'march', 'april', 'may', 'june', 'july', 'august', 'september', 'october', 'november', 'december']\n",
    "for i, month in enumerate(months, start=1):\n",
    "    df_5min[f'is_{month.lower()}'] = (df_5min['month'] == i).astype(int)\n",
    "df_5min.drop('day_of_week', axis=1, inplace=True)\n",
    "df_5min.drop('month', axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8a1ea50-22f6-48f1-a278-2b13a7096a78",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1c74877e-5f0f-4043-beb7-81facfa9be0e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-01T05:02:53.995436Z",
     "iopub.status.busy": "2024-11-01T05:02:53.995181Z",
     "iopub.status.idle": "2024-11-01T05:03:17.624739Z",
     "shell.execute_reply": "2024-11-01T05:03:17.624115Z",
     "shell.execute_reply.started": "2024-11-01T05:02:53.995416Z"
    }
   },
   "outputs": [],
   "source": [
    "# Reset the index for all DataFrames if the index is not meaningful\n",
    "df_5min.reset_index(drop=True, inplace=True)\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "df1.reset_index(drop=True, inplace=True)\n",
    "df2.reset_index(drop=True, inplace=True)\n",
    "df3.reset_index(drop=True, inplace=True)\n",
    "df4.reset_index(drop=True, inplace=True)\n",
    "df5.reset_index(drop=True, inplace=True)\n",
    "df6.reset_index(drop=True, inplace=True)\n",
    "df7.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Now concatenate the DataFrames along axis=1\n",
    "dff = pd.concat([df_5min, df, df1, df2, df3, df4, df5, df6, df7], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "77558e90-4643-48f5-8e58-a4968dd1a9a1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-01T05:03:17.626048Z",
     "iopub.status.busy": "2024-11-01T05:03:17.625896Z",
     "iopub.status.idle": "2024-11-01T05:03:17.631159Z",
     "shell.execute_reply": "2024-11-01T05:03:17.630693Z",
     "shell.execute_reply.started": "2024-11-01T05:03:17.626048Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(178118, 1834)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dff.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7439bbc0-61cb-4656-9690-499c34a4c43b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-01T05:03:17.632143Z",
     "iopub.status.busy": "2024-11-01T05:03:17.631984Z",
     "iopub.status.idle": "2024-11-01T05:03:17.670283Z",
     "shell.execute_reply": "2024-11-01T05:03:17.669777Z",
     "shell.execute_reply.started": "2024-11-01T05:03:17.632129Z"
    }
   },
   "outputs": [],
   "source": [
    "### adding MACD feature\n",
    "dff['macd_line'] = dff['EMA_12'] - dff['EMA_26']\n",
    "dff['signal_line'] = dff['macd_line'].ewm(span=9, adjust=False).mean()\n",
    "dff['macd_histogram'] = dff['macd_line'] - dff['signal_line']\n",
    "for lag in [1, 2, 3]:\n",
    "    dff[f'macd_line_lag_{lag}'] = dff['macd_line'].shift(lag)\n",
    "    dff[f'signal_line_lag_{lag}'] = dff['signal_line'].shift(lag)\n",
    "    dff[f'macd_histogram_lag_{lag}'] = dff['macd_histogram'].shift(lag)\n",
    "dff['macd_signal_crossover'] = 0\n",
    "dff.loc[(dff['macd_line'] > dff['signal_line']) & (dff['macd_line'].shift(1) <= dff['signal_line'].shift(1)), 'macd_signal_crossover'] = 1\n",
    "dff.loc[(dff['macd_line'] < dff['signal_line']) & (dff['macd_line'].shift(1) >= dff['signal_line'].shift(1)), 'macd_signal_crossover'] = -1\n",
    "dff['MACD_Distance_Zero'] = dff['macd_line'].abs()\n",
    "dff['MACD_Above_Zero'] = (dff['macd_line'] > 0).astype(int)\n",
    "dff['MACD_Trend_Zero'] = np.sign(dff['macd_line'].diff())\n",
    "dff['MACD_Trend_Zero'] = dff['MACD_Trend_Zero'].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab948e7d-6209-4868-9d77-8318976b09a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "67b39b7c-2d88-43c8-b2f1-3f40a0093c6e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-01T05:03:17.671854Z",
     "iopub.status.busy": "2024-11-01T05:03:17.671397Z",
     "iopub.status.idle": "2024-11-01T05:03:17.674720Z",
     "shell.execute_reply": "2024-11-01T05:03:17.674384Z",
     "shell.execute_reply.started": "2024-11-01T05:03:17.671840Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(178118, 1850)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dff.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d751229a-5564-4374-b6b3-409adb0dcaf9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-01T05:03:17.675414Z",
     "iopub.status.busy": "2024-11-01T05:03:17.675283Z",
     "iopub.status.idle": "2024-11-01T05:03:17.678310Z",
     "shell.execute_reply": "2024-11-01T05:03:17.677821Z",
     "shell.execute_reply.started": "2024-11-01T05:03:17.675402Z"
    }
   },
   "outputs": [],
   "source": [
    "# def analyze_single_day(day_data, debug=True):\n",
    "#     \"\"\"\n",
    "#     Analyze a single day's data to find all optimal buy points.\n",
    "    \n",
    "#     Parameters:\n",
    "#     -----------\n",
    "#     day_data : pandas DataFrame\n",
    "#         Single day's OHLC data\n",
    "#     debug : bool\n",
    "#         If True, prints detailed analysis\n",
    "        \n",
    "#     Returns:\n",
    "#     --------\n",
    "#     set\n",
    "#         Indices of optimal buy points\n",
    "#     \"\"\"\n",
    "#     buy_points = set()\n",
    "    \n",
    "#     if debug:\n",
    "#         print(f\"\\nAnalyzing date: {day_data['date'].iloc[0]}\")\n",
    "#         print(f\"Total bars in day: {len(day_data)}\")\n",
    "    \n",
    "#     # For each potential sell point\n",
    "#     for sell_idx in range(1, len(day_data)):\n",
    "#         sell_price = day_data.iloc[sell_idx]['close']\n",
    "        \n",
    "#         # Check all potential buy points before this sell point\n",
    "#         for buy_idx in range(sell_idx):\n",
    "#             buy_price = day_data.iloc[buy_idx]['close']\n",
    "#             return_pct = (sell_price - buy_price) * 100 / buy_price\n",
    "            \n",
    "#             if return_pct >= 0.6:\n",
    "#                 buy_points.add(day_data.index[buy_idx])\n",
    "#                 if debug:\n",
    "#                     print(f\"Buy point found at {day_data.iloc[buy_idx]['date']}\")\n",
    "#                     print(f\"Buy price: {buy_price:.2f}, Sell price: {sell_price:.2f}\")\n",
    "#                     print(f\"Return: {return_pct:.2f}%\")\n",
    "    \n",
    "#     if debug:\n",
    "#         print(f\"Total buy points found in day: {len(buy_points)}\")\n",
    "    \n",
    "#     return buy_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ef6cde65-479c-4885-bdfb-951c375e1060",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-01T05:03:17.679419Z",
     "iopub.status.busy": "2024-11-01T05:03:17.679240Z",
     "iopub.status.idle": "2024-11-01T05:03:17.682802Z",
     "shell.execute_reply": "2024-11-01T05:03:17.682414Z",
     "shell.execute_reply.started": "2024-11-01T05:03:17.679402Z"
    }
   },
   "outputs": [],
   "source": [
    "# from tqdm import tqdm  # For progress bar\n",
    "\n",
    "# def calculate_optimal_buy_points(df):\n",
    "#     \"\"\"\n",
    "#     Calculate optimal buy points with progress tracking.\n",
    "#     \"\"\"\n",
    "#     df = df.copy()\n",
    "#     df['target'] = 0\n",
    "    \n",
    "#     # Convert date to datetime and create trade_date\n",
    "#     df['date'] = pd.to_datetime(df['date'])\n",
    "#     df['trade_date'] = df['date'].dt.date\n",
    "    \n",
    "#     # Get unique dates for progress tracking\n",
    "#     unique_dates = df['trade_date'].unique()\n",
    "#     total_days = len(unique_dates)\n",
    "    \n",
    "#     print(f\"Processing {total_days} trading days...\")\n",
    "    \n",
    "#     # Use tqdm for progress bar\n",
    "#     for date in tqdm(unique_dates, desc=\"Finding buy points\"):\n",
    "#         group = df[df['trade_date'] == date]\n",
    "#         group_indices = group.index\n",
    "#         buy_points = set()\n",
    "        \n",
    "#         # For each potential sell point\n",
    "#         for sell_idx in range(1, len(group)):\n",
    "#             sell_price = group['close'].iloc[sell_idx]\n",
    "            \n",
    "#             # Check all potential buy points before this sell point\n",
    "#             for buy_idx in range(sell_idx):\n",
    "#                 buy_price = group['close'].iloc[buy_idx]\n",
    "#                 return_pct = (sell_price - buy_price) * 100 / buy_price\n",
    "                \n",
    "#                 if return_pct >= 0.6:\n",
    "#                     buy_points.add(group_indices[buy_idx])\n",
    "        \n",
    "#         # Mark buy points in the original dataframe\n",
    "#         df.loc[list(buy_points), 'target'] = 1\n",
    "    \n",
    "#     # Print summary statistics\n",
    "#     total_buy_points = df['target'].sum()\n",
    "#     print(f\"\\nFound {total_buy_points} buy points across {total_days} days\")\n",
    "#     print(f\"Average {total_buy_points/total_days:.2f} buy points per day\")\n",
    "    \n",
    "#     # Clean up\n",
    "#     df.drop('trade_date', axis=1, inplace=True)\n",
    "#     return df['target']\n",
    "\n",
    "# # Run with progress tracking\n",
    "# dff['target'] = calculate_optimal_buy_points(dff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3c12aa4c-1f81-4991-a0b4-203f33acc0cb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-01T05:03:17.683524Z",
     "iopub.status.busy": "2024-11-01T05:03:17.683370Z",
     "iopub.status.idle": "2024-11-01T05:03:17.685551Z",
     "shell.execute_reply": "2024-11-01T05:03:17.685249Z",
     "shell.execute_reply.started": "2024-11-01T05:03:17.683524Z"
    }
   },
   "outputs": [],
   "source": [
    "# (dff[\"target\"]).to_csv(\"target_variable.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "13ac5629-141d-4dd5-907c-eeed62c9a879",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-01T05:03:17.686243Z",
     "iopub.status.busy": "2024-11-01T05:03:17.686123Z",
     "iopub.status.idle": "2024-11-01T05:03:17.725292Z",
     "shell.execute_reply": "2024-11-01T05:03:17.724793Z",
     "shell.execute_reply.started": "2024-11-01T05:03:17.686243Z"
    }
   },
   "outputs": [],
   "source": [
    "tg = pd.read_csv(\"target_variable.csv\",index_col = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e9f7df2a-3c58-4463-b6c8-06744ae8443f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-01T05:03:17.726645Z",
     "iopub.status.busy": "2024-11-01T05:03:17.726135Z",
     "iopub.status.idle": "2024-11-01T05:03:47.807724Z",
     "shell.execute_reply": "2024-11-01T05:03:47.807251Z",
     "shell.execute_reply.started": "2024-11-01T05:03:17.726622Z"
    }
   },
   "outputs": [],
   "source": [
    "dff = pd.concat([dff, tg], axis=1, join='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c15f0e71-b080-419a-ad43-8ea825ff013c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-01T05:03:47.811712Z",
     "iopub.status.busy": "2024-11-01T05:03:47.811094Z",
     "iopub.status.idle": "2024-11-01T05:03:47.814845Z",
     "shell.execute_reply": "2024-11-01T05:03:47.814487Z",
     "shell.execute_reply.started": "2024-11-01T05:03:47.811692Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# def validate_and_analyze_results(df, target_series):\n",
    "#     \"\"\"\n",
    "#     Validate and provide detailed analysis of the buy points.\n",
    "    \n",
    "#     Parameters:\n",
    "#     -----------\n",
    "#     df : pandas DataFrame\n",
    "#         Original DataFrame with price data\n",
    "#     target_series : pandas Series\n",
    "#         Series containing target labels\n",
    "#     \"\"\"\n",
    "#     buy_points = df[target_series == 1].index\n",
    "#     points_by_date = {}\n",
    "#     valid_points = 0\n",
    "#     invalid_points = 0\n",
    "    \n",
    "#     for buy_idx in buy_points:\n",
    "#         buy_price = df.loc[buy_idx, 'close']\n",
    "#         day = pd.to_datetime(df.loc[buy_idx, 'date']).date()\n",
    "        \n",
    "#         # Track points by date\n",
    "#         if day not in points_by_date:\n",
    "#             points_by_date[day] = {'total': 0, 'valid': 0}\n",
    "#         points_by_date[day]['total'] += 1\n",
    "        \n",
    "#         # Get future prices until end of day\n",
    "#         future_prices = df[\n",
    "#             (pd.to_datetime(df['date']).dt.date == day) & \n",
    "#             (df.index > buy_idx)\n",
    "#         ]['close']\n",
    "        \n",
    "#         if len(future_prices) > 0:\n",
    "#             max_return = (future_prices.max() - buy_price) * 100 / buy_price\n",
    "#             if max_return >= 0.6:\n",
    "#                 valid_points += 1\n",
    "#                 points_by_date[day]['valid'] += 1\n",
    "#             else:\n",
    "#                 invalid_points += 1\n",
    "    \n",
    "#     print(\"\\nDetailed Analysis:\")\n",
    "#     print(f\"Total days analyzed: {len(points_by_date)}\")\n",
    "#     print(f\"Total buy points: {len(buy_points)}\")\n",
    "#     print(f\"Valid buy points: {valid_points}\")\n",
    "#     print(f\"Invalid buy points: {invalid_points}\")\n",
    "    \n",
    "#     if len(buy_points) > 0:\n",
    "#         print(f\"Percentage valid: {valid_points/len(buy_points)*100:.2f}%\")\n",
    "    \n",
    "#     print(\"\\nDaily Statistics:\")\n",
    "#     points_per_day = [data['total'] for data in points_by_date.values()]\n",
    "#     if points_per_day:\n",
    "#         print(f\"Average buy points per day: {np.mean(points_per_day):.2f}\")\n",
    "#         print(f\"Maximum buy points in a day: {max(points_per_day)}\")\n",
    "#         print(f\"Minimum buy points in a day: {min(points_per_day)}\")\n",
    "    \n",
    "#     return points_by_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "891280c8-6b02-41c3-9677-009490c43aba",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-01T05:03:47.815779Z",
     "iopub.status.busy": "2024-11-01T05:03:47.815334Z",
     "iopub.status.idle": "2024-11-01T05:03:47.826729Z",
     "shell.execute_reply": "2024-11-01T05:03:47.826371Z",
     "shell.execute_reply.started": "2024-11-01T05:03:47.815762Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "target\n",
       "0    155212\n",
       "1     22906\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dff[\"target\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b4fec82c-5720-45e4-bfbb-5151a7a29835",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-01T05:03:47.827625Z",
     "iopub.status.busy": "2024-11-01T05:03:47.827233Z",
     "iopub.status.idle": "2024-11-01T05:03:59.101130Z",
     "shell.execute_reply": "2024-11-01T05:03:59.100603Z",
     "shell.execute_reply.started": "2024-11-01T05:03:47.827609Z"
    }
   },
   "outputs": [],
   "source": [
    "dff = dff.drop(columns=['Unnamed: 0'], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d7ff48e7-a7a9-421b-9fb6-d7c37319e85a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-01T05:03:59.101924Z",
     "iopub.status.busy": "2024-11-01T05:03:59.101770Z",
     "iopub.status.idle": "2024-11-01T05:04:10.759703Z",
     "shell.execute_reply": "2024-11-01T05:04:10.759126Z",
     "shell.execute_reply.started": "2024-11-01T05:03:59.101909Z"
    }
   },
   "outputs": [],
   "source": [
    "df_features = dff.drop(columns=['target'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "71abc5a7-9dfb-49ff-9d38-e48d1442e9fe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-01T05:04:10.760869Z",
     "iopub.status.busy": "2024-11-01T05:04:10.760490Z",
     "iopub.status.idle": "2024-11-01T05:05:11.039504Z",
     "shell.execute_reply": "2024-11-01T05:05:11.038285Z",
     "shell.execute_reply.started": "2024-11-01T05:04:10.760850Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import joblib  # For saving and loading the scaler\n",
    "\n",
    "# Step 1: Separate the DataFrame into Scalable and Non-Scalable Columns\n",
    "def separate_columns(df):\n",
    "    # Identify columns that are numeric and can be scaled\n",
    "    numeric_columns = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "    # Create DataFrame with numeric columns for scaling\n",
    "    df_scalable = df[numeric_columns]\n",
    "\n",
    "    # Create DataFrame with non-scalable columns (e.g., date column, categorical columns)\n",
    "    df_non_scalable = df.drop(columns=numeric_columns)\n",
    "\n",
    "    return df_scalable, df_non_scalable\n",
    "\n",
    "# Step 2: Apply MinMaxScaler and save it\n",
    "def scale_dataframe(df_scalable, save_scaler=False, scaler_path='scaler.pkl'):\n",
    "    scaler = MinMaxScaler()\n",
    "\n",
    "    # Fit the scaler to the data and transform it\n",
    "    df_scaled = pd.DataFrame(scaler.fit_transform(df_scalable), columns=df_scalable.columns, index=df_scalable.index)\n",
    "\n",
    "    # Save the scaler if required\n",
    "    if save_scaler:\n",
    "        joblib.dump(scaler, scaler_path)\n",
    "        print(f\"Scaler saved to {scaler_path}\")\n",
    "\n",
    "    return df_scaled\n",
    "\n",
    "# Step 3: Concatenate the Scaled DataFrame with Non-Scaled Columns\n",
    "def process_large_dataframe(df, save_scaler=False, scaler_path='scaler.pkl'):\n",
    "    # Separate the columns\n",
    "    df_scalable, df_non_scalable = separate_columns(df)\n",
    "    \n",
    "    # Scale the numeric columns and optionally save the scaler\n",
    "    df_scaled = scale_dataframe(df_scalable, save_scaler=save_scaler, scaler_path=scaler_path)\n",
    "    \n",
    "    # Concatenate scaled and non-scaled DataFrames\n",
    "    df_final = pd.concat([df_non_scalable, df_scaled], axis=1)\n",
    "    \n",
    "    return df_scaled, df_final\n",
    "\n",
    "# Example Usage:\n",
    "# df_features is your DataFrame\n",
    "df_scaled, df_final = process_large_dataframe(df_features, save_scaler=False, scaler_path='before_autoencoder_scaler.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "aedb82f0-bea0-4268-9ee5-f6afc609d535",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-01T05:05:22.373669Z",
     "iopub.status.busy": "2024-11-01T05:05:22.373482Z",
     "iopub.status.idle": "2024-11-01T05:05:31.136608Z",
     "shell.execute_reply": "2024-11-01T05:05:31.136007Z",
     "shell.execute_reply.started": "2024-11-01T05:05:22.373654Z"
    }
   },
   "outputs": [],
   "source": [
    "# Assuming dff[\"target\"] is a DataFrame or Series and df_scaled is also a DataFrame\n",
    "main_df = pd.concat([dff[\"target\"], df_scaled], axis=1)  # axis=1 to concatenate along columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f9f324fc-7912-4dca-a507-79ce4ee90395",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-01T05:05:31.137493Z",
     "iopub.status.busy": "2024-11-01T05:05:31.137312Z",
     "iopub.status.idle": "2024-11-01T05:05:55.265307Z",
     "shell.execute_reply": "2024-11-01T05:05:55.264361Z",
     "shell.execute_reply.started": "2024-11-01T05:05:31.137478Z"
    }
   },
   "outputs": [],
   "source": [
    "main_df = main_df.dropna()\n",
    "df_scaled = main_df.drop(columns=['target'])\n",
    "target_df = main_df[\"target\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5214dbe7-7f26-4b17-922b-c5a2ce387055",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-01T05:05:55.268305Z",
     "iopub.status.busy": "2024-11-01T05:05:55.268127Z",
     "iopub.status.idle": "2024-11-01T05:05:55.276984Z",
     "shell.execute_reply": "2024-11-01T05:05:55.276510Z",
     "shell.execute_reply.started": "2024-11-01T05:05:55.268291Z"
    }
   },
   "outputs": [],
   "source": [
    "target_df = target_df.reset_index().drop(columns=['index'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "de2777f9-fc86-4586-a17e-3708e6b3ceaf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-01T05:05:55.277764Z",
     "iopub.status.busy": "2024-11-01T05:05:55.277617Z",
     "iopub.status.idle": "2024-11-01T05:06:15.280752Z",
     "shell.execute_reply": "2024-11-01T05:06:15.280167Z",
     "shell.execute_reply.started": "2024-11-01T05:05:55.277751Z"
    }
   },
   "outputs": [],
   "source": [
    "df_scaled = df_scaled.reset_index().drop(columns=['index']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a82be3b5-a531-4d2d-b548-c9bb159c8152",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-01T05:06:15.282263Z",
     "iopub.status.busy": "2024-11-01T05:06:15.282043Z",
     "iopub.status.idle": "2024-11-01T05:06:15.298922Z",
     "shell.execute_reply": "2024-11-01T05:06:15.298429Z",
     "shell.execute_reply.started": "2024-11-01T05:06:15.282263Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177991</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177992</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177993</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177994</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177995</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>177996 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        target\n",
       "0            0\n",
       "1            0\n",
       "2            0\n",
       "3            0\n",
       "4            0\n",
       "...        ...\n",
       "177991       0\n",
       "177992       0\n",
       "177993       0\n",
       "177994       0\n",
       "177995       0\n",
       "\n",
       "[177996 rows x 1 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c7f1396f-03bc-4676-8985-d97ede0bc221",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-01T05:06:15.299710Z",
     "iopub.status.busy": "2024-11-01T05:06:15.299556Z",
     "iopub.status.idle": "2024-11-01T05:06:15.308925Z",
     "shell.execute_reply": "2024-11-01T05:06:15.308506Z",
     "shell.execute_reply.started": "2024-11-01T05:06:15.299710Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "87.16488010966539"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main_df[\"target\"].value_counts()[0]*100/len(main_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "274da3b8-7012-4c69-8459-09d5a7bcda43",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-01T05:06:15.309676Z",
     "iopub.status.busy": "2024-11-01T05:06:15.309502Z",
     "iopub.status.idle": "2024-11-01T05:06:15.314768Z",
     "shell.execute_reply": "2024-11-01T05:06:15.314263Z",
     "shell.execute_reply.started": "2024-11-01T05:06:15.309654Z"
    }
   },
   "outputs": [],
   "source": [
    "date_index_mapping = dff[\"date\"].reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ba9ff9a8-832d-4515-983e-fbe87e78ef97",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-01T05:06:15.315593Z",
     "iopub.status.busy": "2024-11-01T05:06:15.315373Z",
     "iopub.status.idle": "2024-11-01T05:06:15.330641Z",
     "shell.execute_reply": "2024-11-01T05:06:15.330128Z",
     "shell.execute_reply.started": "2024-11-01T05:06:15.315543Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2015-01-09 09:15:00+05:30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2015-01-09 09:20:00+05:30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2015-01-09 09:25:00+05:30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>2015-01-09 09:30:00+05:30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>2015-01-09 09:35:00+05:30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178113</th>\n",
       "      <td>178113</td>\n",
       "      <td>2024-08-28 15:05:00+05:30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178114</th>\n",
       "      <td>178114</td>\n",
       "      <td>2024-08-28 15:10:00+05:30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178115</th>\n",
       "      <td>178115</td>\n",
       "      <td>2024-08-28 15:15:00+05:30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178116</th>\n",
       "      <td>178116</td>\n",
       "      <td>2024-08-28 15:20:00+05:30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178117</th>\n",
       "      <td>178117</td>\n",
       "      <td>2024-08-28 15:25:00+05:30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>178118 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         index                      date\n",
       "0            0 2015-01-09 09:15:00+05:30\n",
       "1            1 2015-01-09 09:20:00+05:30\n",
       "2            2 2015-01-09 09:25:00+05:30\n",
       "3            3 2015-01-09 09:30:00+05:30\n",
       "4            4 2015-01-09 09:35:00+05:30\n",
       "...        ...                       ...\n",
       "178113  178113 2024-08-28 15:05:00+05:30\n",
       "178114  178114 2024-08-28 15:10:00+05:30\n",
       "178115  178115 2024-08-28 15:15:00+05:30\n",
       "178116  178116 2024-08-28 15:20:00+05:30\n",
       "178117  178117 2024-08-28 15:25:00+05:30\n",
       "\n",
       "[178118 rows x 2 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "date_index_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d376eff6-276a-4b4c-aef2-7c649ed86318",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-01T05:06:15.331549Z",
     "iopub.status.busy": "2024-11-01T05:06:15.331366Z",
     "iopub.status.idle": "2024-11-01T05:06:15.560165Z",
     "shell.execute_reply": "2024-11-01T05:06:15.559697Z",
     "shell.execute_reply.started": "2024-11-01T05:06:15.331545Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(177996, 1846)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "print(df_scaled.shape)\n",
    "df_scaled.head()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "588a40f9-39a7-41e0-be0e-ada5a801a48b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-01T05:06:15.561100Z",
     "iopub.status.busy": "2024-11-01T05:06:15.560904Z",
     "iopub.status.idle": "2024-11-01T05:06:15.573647Z",
     "shell.execute_reply": "2024-11-01T05:06:15.572843Z",
     "shell.execute_reply.started": "2024-11-01T05:06:15.561100Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. open\n",
      "2. high\n",
      "3. low\n",
      "4. close\n",
      "5. is_monday\n",
      "6. is_tuesday\n",
      "7. is_wednesday\n",
      "8. is_thursday\n",
      "9. is_friday\n",
      "10. is_saturday\n",
      "11. is_sunday\n",
      "12. is_january\n",
      "13. is_february\n",
      "14. is_march\n",
      "15. is_april\n",
      "16. is_may\n",
      "17. is_june\n",
      "18. is_july\n",
      "19. is_august\n",
      "20. is_september\n",
      "21. is_october\n",
      "22. is_november\n",
      "23. is_december\n",
      "24. dist_from_max_to_prev_day\n",
      "25. dist_from_low_to_prev_day\n",
      "26. dist_from_daily_max\n",
      "27. dist_from_daily_low\n",
      "28. dist_from_max_to_last30days\n",
      "29. dist_from_low_to_last30days\n",
      "30. SMA_4\n",
      "31. SMA_5\n",
      "32. SMA_6\n",
      "33. SMA_7\n",
      "34. SMA_8\n",
      "35. SMA_9\n",
      "36. SMA_10\n",
      "37. SMA_11\n",
      "38. SMA_12\n",
      "39. SMA_13\n",
      "40. SMA_14\n",
      "41. SMA_15\n",
      "42. SMA_16\n",
      "43. SMA_17\n",
      "44. SMA_18\n",
      "45. SMA_19\n",
      "46. SMA_20\n",
      "47. SMA_21\n",
      "48. SMA_22\n",
      "49. SMA_23\n",
      "50. SMA_24\n",
      "51. SMA_25\n",
      "52. SMA_26\n",
      "53. SMA_27\n",
      "54. SMA_28\n",
      "55. SMA_29\n",
      "56. SMA_30\n",
      "57. SMA_31\n",
      "58. SMA_32\n",
      "59. SMA_33\n",
      "60. SMA_34\n",
      "61. SMA_35\n",
      "62. SMA_36\n",
      "63. SMA_37\n",
      "64. SMA_38\n",
      "65. SMA_39\n",
      "66. SMA_40\n",
      "67. SMA_41\n",
      "68. SMA_42\n",
      "69. SMA_43\n",
      "70. SMA_44\n",
      "71. SMA_45\n",
      "72. SMA_46\n",
      "73. SMA_47\n",
      "74. SMA_48\n",
      "75. SMA_49\n",
      "76. SMA_50\n",
      "77. SMA_51\n",
      "78. SMA_52\n",
      "79. SMA_53\n",
      "80. SMA_54\n",
      "81. SMA_55\n",
      "82. SMA_56\n",
      "83. SMA_57\n",
      "84. SMA_58\n",
      "85. SMA_59\n",
      "86. EMA_4\n",
      "87. EMA_5\n",
      "88. EMA_6\n",
      "89. EMA_7\n",
      "90. EMA_8\n",
      "91. EMA_9\n",
      "92. EMA_10\n",
      "93. EMA_11\n",
      "94. EMA_12\n",
      "95. EMA_13\n",
      "96. EMA_14\n",
      "97. EMA_15\n",
      "98. EMA_16\n",
      "99. EMA_17\n",
      "100. EMA_18\n",
      "101. EMA_19\n",
      "102. EMA_20\n",
      "103. EMA_21\n",
      "104. EMA_22\n",
      "105. EMA_23\n",
      "106. EMA_24\n",
      "107. EMA_25\n",
      "108. EMA_26\n",
      "109. EMA_27\n",
      "110. EMA_28\n",
      "111. EMA_29\n",
      "112. EMA_30\n",
      "113. EMA_31\n",
      "114. EMA_32\n",
      "115. EMA_33\n",
      "116. EMA_34\n",
      "117. EMA_35\n",
      "118. EMA_36\n",
      "119. EMA_37\n",
      "120. EMA_38\n",
      "121. EMA_39\n",
      "122. EMA_40\n",
      "123. EMA_41\n",
      "124. EMA_42\n",
      "125. EMA_43\n",
      "126. EMA_44\n",
      "127. EMA_45\n",
      "128. EMA_46\n",
      "129. EMA_47\n",
      "130. EMA_48\n",
      "131. EMA_49\n",
      "132. EMA_50\n",
      "133. EMA_51\n",
      "134. EMA_52\n",
      "135. EMA_53\n",
      "136. EMA_54\n",
      "137. EMA_55\n",
      "138. EMA_56\n",
      "139. EMA_57\n",
      "140. EMA_58\n",
      "141. EMA_59\n",
      "142. SMA_4_lag_1\n",
      "143. SMA_4_lag_2\n",
      "144. SMA_4_lag_3\n",
      "145. SMA_4_lag_4\n",
      "146. SMA_5_lag_1\n",
      "147. SMA_5_lag_2\n",
      "148. SMA_5_lag_3\n",
      "149. SMA_5_lag_4\n",
      "150. SMA_6_lag_1\n",
      "151. SMA_6_lag_2\n",
      "152. SMA_6_lag_3\n",
      "153. SMA_6_lag_4\n",
      "154. SMA_7_lag_1\n",
      "155. SMA_7_lag_2\n",
      "156. SMA_7_lag_3\n",
      "157. SMA_7_lag_4\n",
      "158. SMA_8_lag_1\n",
      "159. SMA_8_lag_2\n",
      "160. SMA_8_lag_3\n",
      "161. SMA_8_lag_4\n",
      "162. SMA_9_lag_1\n",
      "163. SMA_9_lag_2\n",
      "164. SMA_9_lag_3\n",
      "165. SMA_9_lag_4\n",
      "166. SMA_10_lag_1\n",
      "167. SMA_10_lag_2\n",
      "168. SMA_10_lag_3\n",
      "169. SMA_10_lag_4\n",
      "170. SMA_11_lag_1\n",
      "171. SMA_11_lag_2\n",
      "172. SMA_11_lag_3\n",
      "173. SMA_11_lag_4\n",
      "174. SMA_12_lag_1\n",
      "175. SMA_12_lag_2\n",
      "176. SMA_12_lag_3\n",
      "177. SMA_12_lag_4\n",
      "178. SMA_13_lag_1\n",
      "179. SMA_13_lag_2\n",
      "180. SMA_13_lag_3\n",
      "181. SMA_13_lag_4\n",
      "182. SMA_14_lag_1\n",
      "183. SMA_14_lag_2\n",
      "184. SMA_14_lag_3\n",
      "185. SMA_14_lag_4\n",
      "186. SMA_15_lag_1\n",
      "187. SMA_15_lag_2\n",
      "188. SMA_15_lag_3\n",
      "189. SMA_15_lag_4\n",
      "190. SMA_16_lag_1\n",
      "191. SMA_16_lag_2\n",
      "192. SMA_16_lag_3\n",
      "193. SMA_16_lag_4\n",
      "194. SMA_17_lag_1\n",
      "195. SMA_17_lag_2\n",
      "196. SMA_17_lag_3\n",
      "197. SMA_17_lag_4\n",
      "198. SMA_18_lag_1\n",
      "199. SMA_18_lag_2\n",
      "200. SMA_18_lag_3\n",
      "201. SMA_18_lag_4\n",
      "202. SMA_19_lag_1\n",
      "203. SMA_19_lag_2\n",
      "204. SMA_19_lag_3\n",
      "205. SMA_19_lag_4\n",
      "206. SMA_20_lag_1\n",
      "207. SMA_20_lag_2\n",
      "208. SMA_20_lag_3\n",
      "209. SMA_20_lag_4\n",
      "210. SMA_21_lag_1\n",
      "211. SMA_21_lag_2\n",
      "212. SMA_21_lag_3\n",
      "213. SMA_21_lag_4\n",
      "214. SMA_22_lag_1\n",
      "215. SMA_22_lag_2\n",
      "216. SMA_22_lag_3\n",
      "217. SMA_22_lag_4\n",
      "218. SMA_23_lag_1\n",
      "219. SMA_23_lag_2\n",
      "220. SMA_23_lag_3\n",
      "221. SMA_23_lag_4\n",
      "222. SMA_24_lag_1\n",
      "223. SMA_24_lag_2\n",
      "224. SMA_24_lag_3\n",
      "225. SMA_24_lag_4\n",
      "226. SMA_25_lag_1\n",
      "227. SMA_25_lag_2\n",
      "228. SMA_25_lag_3\n",
      "229. SMA_25_lag_4\n",
      "230. SMA_26_lag_1\n",
      "231. SMA_26_lag_2\n",
      "232. SMA_26_lag_3\n",
      "233. SMA_26_lag_4\n",
      "234. SMA_27_lag_1\n",
      "235. SMA_27_lag_2\n",
      "236. SMA_27_lag_3\n",
      "237. SMA_27_lag_4\n",
      "238. SMA_28_lag_1\n",
      "239. SMA_28_lag_2\n",
      "240. SMA_28_lag_3\n",
      "241. SMA_28_lag_4\n",
      "242. SMA_29_lag_1\n",
      "243. SMA_29_lag_2\n",
      "244. SMA_29_lag_3\n",
      "245. SMA_29_lag_4\n",
      "246. SMA_30_lag_1\n",
      "247. SMA_30_lag_2\n",
      "248. SMA_30_lag_3\n",
      "249. SMA_30_lag_4\n",
      "250. SMA_31_lag_1\n",
      "251. SMA_31_lag_2\n",
      "252. SMA_31_lag_3\n",
      "253. SMA_31_lag_4\n",
      "254. SMA_32_lag_1\n",
      "255. SMA_32_lag_2\n",
      "256. SMA_32_lag_3\n",
      "257. SMA_32_lag_4\n",
      "258. SMA_33_lag_1\n",
      "259. SMA_33_lag_2\n",
      "260. SMA_33_lag_3\n",
      "261. SMA_33_lag_4\n",
      "262. SMA_34_lag_1\n",
      "263. SMA_34_lag_2\n",
      "264. SMA_34_lag_3\n",
      "265. SMA_34_lag_4\n",
      "266. SMA_35_lag_1\n",
      "267. SMA_35_lag_2\n",
      "268. SMA_35_lag_3\n",
      "269. SMA_35_lag_4\n",
      "270. SMA_36_lag_1\n",
      "271. SMA_36_lag_2\n",
      "272. SMA_36_lag_3\n",
      "273. SMA_36_lag_4\n",
      "274. SMA_37_lag_1\n",
      "275. SMA_37_lag_2\n",
      "276. SMA_37_lag_3\n",
      "277. SMA_37_lag_4\n",
      "278. SMA_38_lag_1\n",
      "279. SMA_38_lag_2\n",
      "280. SMA_38_lag_3\n",
      "281. SMA_38_lag_4\n",
      "282. SMA_39_lag_1\n",
      "283. SMA_39_lag_2\n",
      "284. SMA_39_lag_3\n",
      "285. SMA_39_lag_4\n",
      "286. SMA_40_lag_1\n",
      "287. SMA_40_lag_2\n",
      "288. SMA_40_lag_3\n",
      "289. SMA_40_lag_4\n",
      "290. SMA_41_lag_1\n",
      "291. SMA_41_lag_2\n",
      "292. SMA_41_lag_3\n",
      "293. SMA_41_lag_4\n",
      "294. SMA_42_lag_1\n",
      "295. SMA_42_lag_2\n",
      "296. SMA_42_lag_3\n",
      "297. SMA_42_lag_4\n",
      "298. SMA_43_lag_1\n",
      "299. SMA_43_lag_2\n",
      "300. SMA_43_lag_3\n",
      "301. SMA_43_lag_4\n",
      "302. SMA_44_lag_1\n",
      "303. SMA_44_lag_2\n",
      "304. SMA_44_lag_3\n",
      "305. SMA_44_lag_4\n",
      "306. SMA_45_lag_1\n",
      "307. SMA_45_lag_2\n",
      "308. SMA_45_lag_3\n",
      "309. SMA_45_lag_4\n",
      "310. SMA_46_lag_1\n",
      "311. SMA_46_lag_2\n",
      "312. SMA_46_lag_3\n",
      "313. SMA_46_lag_4\n",
      "314. SMA_47_lag_1\n",
      "315. SMA_47_lag_2\n",
      "316. SMA_47_lag_3\n",
      "317. SMA_47_lag_4\n",
      "318. SMA_48_lag_1\n",
      "319. SMA_48_lag_2\n",
      "320. SMA_48_lag_3\n",
      "321. SMA_48_lag_4\n",
      "322. SMA_49_lag_1\n",
      "323. SMA_49_lag_2\n",
      "324. SMA_49_lag_3\n",
      "325. SMA_49_lag_4\n",
      "326. SMA_50_lag_1\n",
      "327. SMA_50_lag_2\n",
      "328. SMA_50_lag_3\n",
      "329. SMA_50_lag_4\n",
      "330. SMA_51_lag_1\n",
      "331. SMA_51_lag_2\n",
      "332. SMA_51_lag_3\n",
      "333. SMA_51_lag_4\n",
      "334. SMA_52_lag_1\n",
      "335. SMA_52_lag_2\n",
      "336. SMA_52_lag_3\n",
      "337. SMA_52_lag_4\n",
      "338. SMA_53_lag_1\n",
      "339. SMA_53_lag_2\n",
      "340. SMA_53_lag_3\n",
      "341. SMA_53_lag_4\n",
      "342. SMA_54_lag_1\n",
      "343. SMA_54_lag_2\n",
      "344. SMA_54_lag_3\n",
      "345. SMA_54_lag_4\n",
      "346. SMA_55_lag_1\n",
      "347. SMA_55_lag_2\n",
      "348. SMA_55_lag_3\n",
      "349. SMA_55_lag_4\n",
      "350. SMA_56_lag_1\n",
      "351. SMA_56_lag_2\n",
      "352. SMA_56_lag_3\n",
      "353. SMA_56_lag_4\n",
      "354. SMA_57_lag_1\n",
      "355. SMA_57_lag_2\n",
      "356. SMA_57_lag_3\n",
      "357. SMA_57_lag_4\n",
      "358. SMA_58_lag_1\n",
      "359. SMA_58_lag_2\n",
      "360. SMA_58_lag_3\n",
      "361. SMA_58_lag_4\n",
      "362. SMA_59_lag_1\n",
      "363. SMA_59_lag_2\n",
      "364. SMA_59_lag_3\n",
      "365. SMA_59_lag_4\n",
      "366. EMA_4_lag_1\n",
      "367. EMA_4_lag_2\n",
      "368. EMA_4_lag_3\n",
      "369. EMA_4_lag_4\n",
      "370. EMA_5_lag_1\n",
      "371. EMA_5_lag_2\n",
      "372. EMA_5_lag_3\n",
      "373. EMA_5_lag_4\n",
      "374. EMA_6_lag_1\n",
      "375. EMA_6_lag_2\n",
      "376. EMA_6_lag_3\n",
      "377. EMA_6_lag_4\n",
      "378. EMA_7_lag_1\n",
      "379. EMA_7_lag_2\n",
      "380. EMA_7_lag_3\n",
      "381. EMA_7_lag_4\n",
      "382. EMA_8_lag_1\n",
      "383. EMA_8_lag_2\n",
      "384. EMA_8_lag_3\n",
      "385. EMA_8_lag_4\n",
      "386. EMA_9_lag_1\n",
      "387. EMA_9_lag_2\n",
      "388. EMA_9_lag_3\n",
      "389. EMA_9_lag_4\n",
      "390. EMA_10_lag_1\n",
      "391. EMA_10_lag_2\n",
      "392. EMA_10_lag_3\n",
      "393. EMA_10_lag_4\n",
      "394. EMA_11_lag_1\n",
      "395. EMA_11_lag_2\n",
      "396. EMA_11_lag_3\n",
      "397. EMA_11_lag_4\n",
      "398. EMA_12_lag_1\n",
      "399. EMA_12_lag_2\n",
      "400. EMA_12_lag_3\n",
      "401. EMA_12_lag_4\n",
      "402. EMA_13_lag_1\n",
      "403. EMA_13_lag_2\n",
      "404. EMA_13_lag_3\n",
      "405. EMA_13_lag_4\n",
      "406. EMA_14_lag_1\n",
      "407. EMA_14_lag_2\n",
      "408. EMA_14_lag_3\n",
      "409. EMA_14_lag_4\n",
      "410. EMA_15_lag_1\n",
      "411. EMA_15_lag_2\n",
      "412. EMA_15_lag_3\n",
      "413. EMA_15_lag_4\n",
      "414. EMA_16_lag_1\n",
      "415. EMA_16_lag_2\n",
      "416. EMA_16_lag_3\n",
      "417. EMA_16_lag_4\n",
      "418. EMA_17_lag_1\n",
      "419. EMA_17_lag_2\n",
      "420. EMA_17_lag_3\n",
      "421. EMA_17_lag_4\n",
      "422. EMA_18_lag_1\n",
      "423. EMA_18_lag_2\n",
      "424. EMA_18_lag_3\n",
      "425. EMA_18_lag_4\n",
      "426. EMA_19_lag_1\n",
      "427. EMA_19_lag_2\n",
      "428. EMA_19_lag_3\n",
      "429. EMA_19_lag_4\n",
      "430. EMA_20_lag_1\n",
      "431. EMA_20_lag_2\n",
      "432. EMA_20_lag_3\n",
      "433. EMA_20_lag_4\n",
      "434. EMA_21_lag_1\n",
      "435. EMA_21_lag_2\n",
      "436. EMA_21_lag_3\n",
      "437. EMA_21_lag_4\n",
      "438. EMA_22_lag_1\n",
      "439. EMA_22_lag_2\n",
      "440. EMA_22_lag_3\n",
      "441. EMA_22_lag_4\n",
      "442. EMA_23_lag_1\n",
      "443. EMA_23_lag_2\n",
      "444. EMA_23_lag_3\n",
      "445. EMA_23_lag_4\n",
      "446. EMA_24_lag_1\n",
      "447. EMA_24_lag_2\n",
      "448. EMA_24_lag_3\n",
      "449. EMA_24_lag_4\n",
      "450. EMA_25_lag_1\n",
      "451. EMA_25_lag_2\n",
      "452. EMA_25_lag_3\n",
      "453. EMA_25_lag_4\n",
      "454. EMA_26_lag_1\n",
      "455. EMA_26_lag_2\n",
      "456. EMA_26_lag_3\n",
      "457. EMA_26_lag_4\n",
      "458. EMA_27_lag_1\n",
      "459. EMA_27_lag_2\n",
      "460. EMA_27_lag_3\n",
      "461. EMA_27_lag_4\n",
      "462. EMA_28_lag_1\n",
      "463. EMA_28_lag_2\n",
      "464. EMA_28_lag_3\n",
      "465. EMA_28_lag_4\n",
      "466. EMA_29_lag_1\n",
      "467. EMA_29_lag_2\n",
      "468. EMA_29_lag_3\n",
      "469. EMA_29_lag_4\n",
      "470. EMA_30_lag_1\n",
      "471. EMA_30_lag_2\n",
      "472. EMA_30_lag_3\n",
      "473. EMA_30_lag_4\n",
      "474. EMA_31_lag_1\n",
      "475. EMA_31_lag_2\n",
      "476. EMA_31_lag_3\n",
      "477. EMA_31_lag_4\n",
      "478. EMA_32_lag_1\n",
      "479. EMA_32_lag_2\n",
      "480. EMA_32_lag_3\n",
      "481. EMA_32_lag_4\n",
      "482. EMA_33_lag_1\n",
      "483. EMA_33_lag_2\n",
      "484. EMA_33_lag_3\n",
      "485. EMA_33_lag_4\n",
      "486. EMA_34_lag_1\n",
      "487. EMA_34_lag_2\n",
      "488. EMA_34_lag_3\n",
      "489. EMA_34_lag_4\n",
      "490. EMA_35_lag_1\n",
      "491. EMA_35_lag_2\n",
      "492. EMA_35_lag_3\n",
      "493. EMA_35_lag_4\n",
      "494. EMA_36_lag_1\n",
      "495. EMA_36_lag_2\n",
      "496. EMA_36_lag_3\n",
      "497. EMA_36_lag_4\n",
      "498. EMA_37_lag_1\n",
      "499. EMA_37_lag_2\n",
      "500. EMA_37_lag_3\n",
      "501. EMA_37_lag_4\n",
      "502. EMA_38_lag_1\n",
      "503. EMA_38_lag_2\n",
      "504. EMA_38_lag_3\n",
      "505. EMA_38_lag_4\n",
      "506. EMA_39_lag_1\n",
      "507. EMA_39_lag_2\n",
      "508. EMA_39_lag_3\n",
      "509. EMA_39_lag_4\n",
      "510. EMA_40_lag_1\n",
      "511. EMA_40_lag_2\n",
      "512. EMA_40_lag_3\n",
      "513. EMA_40_lag_4\n",
      "514. EMA_41_lag_1\n",
      "515. EMA_41_lag_2\n",
      "516. EMA_41_lag_3\n",
      "517. EMA_41_lag_4\n",
      "518. EMA_42_lag_1\n",
      "519. EMA_42_lag_2\n",
      "520. EMA_42_lag_3\n",
      "521. EMA_42_lag_4\n",
      "522. EMA_43_lag_1\n",
      "523. EMA_43_lag_2\n",
      "524. EMA_43_lag_3\n",
      "525. EMA_43_lag_4\n",
      "526. EMA_44_lag_1\n",
      "527. EMA_44_lag_2\n",
      "528. EMA_44_lag_3\n",
      "529. EMA_44_lag_4\n",
      "530. EMA_45_lag_1\n",
      "531. EMA_45_lag_2\n",
      "532. EMA_45_lag_3\n",
      "533. EMA_45_lag_4\n",
      "534. EMA_46_lag_1\n",
      "535. EMA_46_lag_2\n",
      "536. EMA_46_lag_3\n",
      "537. EMA_46_lag_4\n",
      "538. EMA_47_lag_1\n",
      "539. EMA_47_lag_2\n",
      "540. EMA_47_lag_3\n",
      "541. EMA_47_lag_4\n",
      "542. EMA_48_lag_1\n",
      "543. EMA_48_lag_2\n",
      "544. EMA_48_lag_3\n",
      "545. EMA_48_lag_4\n",
      "546. EMA_49_lag_1\n",
      "547. EMA_49_lag_2\n",
      "548. EMA_49_lag_3\n",
      "549. EMA_49_lag_4\n",
      "550. EMA_50_lag_1\n",
      "551. EMA_50_lag_2\n",
      "552. EMA_50_lag_3\n",
      "553. EMA_50_lag_4\n",
      "554. EMA_51_lag_1\n",
      "555. EMA_51_lag_2\n",
      "556. EMA_51_lag_3\n",
      "557. EMA_51_lag_4\n",
      "558. EMA_52_lag_1\n",
      "559. EMA_52_lag_2\n",
      "560. EMA_52_lag_3\n",
      "561. EMA_52_lag_4\n",
      "562. EMA_53_lag_1\n",
      "563. EMA_53_lag_2\n",
      "564. EMA_53_lag_3\n",
      "565. EMA_53_lag_4\n",
      "566. EMA_54_lag_1\n",
      "567. EMA_54_lag_2\n",
      "568. EMA_54_lag_3\n",
      "569. EMA_54_lag_4\n",
      "570. EMA_55_lag_1\n",
      "571. EMA_55_lag_2\n",
      "572. EMA_55_lag_3\n",
      "573. EMA_55_lag_4\n",
      "574. EMA_56_lag_1\n",
      "575. EMA_56_lag_2\n",
      "576. EMA_56_lag_3\n",
      "577. EMA_56_lag_4\n",
      "578. EMA_57_lag_1\n",
      "579. EMA_57_lag_2\n",
      "580. EMA_57_lag_3\n",
      "581. EMA_57_lag_4\n",
      "582. EMA_58_lag_1\n",
      "583. EMA_58_lag_2\n",
      "584. EMA_58_lag_3\n",
      "585. EMA_58_lag_4\n",
      "586. EMA_59_lag_1\n",
      "587. EMA_59_lag_2\n",
      "588. EMA_59_lag_3\n",
      "589. EMA_59_lag_4\n",
      "590. RSI_4\n",
      "591. RSI_5\n",
      "592. RSI_6\n",
      "593. RSI_7\n",
      "594. RSI_8\n",
      "595. RSI_9\n",
      "596. RSI_10\n",
      "597. RSI_11\n",
      "598. RSI_12\n",
      "599. RSI_13\n",
      "600. RSI_14\n",
      "601. RSI_15\n",
      "602. RSI_16\n",
      "603. RSI_17\n",
      "604. RSI_18\n",
      "605. RSI_19\n",
      "606. RSI_20\n",
      "607. RSI_21\n",
      "608. RSI_22\n",
      "609. RSI_23\n",
      "610. RSI_24\n",
      "611. RSI_25\n",
      "612. RSI_26\n",
      "613. RSI_27\n",
      "614. RSI_28\n",
      "615. RSI_29\n",
      "616. RSI_30\n",
      "617. RSI_31\n",
      "618. RSI_32\n",
      "619. RSI_33\n",
      "620. RSI_34\n",
      "621. RSI_35\n",
      "622. RSI_36\n",
      "623. RSI_37\n",
      "624. RSI_38\n",
      "625. RSI_39\n",
      "626. RSI_40\n",
      "627. RSI_41\n",
      "628. RSI_42\n",
      "629. RSI_43\n",
      "630. RSI_44\n",
      "631. RSI_45\n",
      "632. RSI_46\n",
      "633. RSI_47\n",
      "634. RSI_48\n",
      "635. RSI_49\n",
      "636. RSI_50\n",
      "637. RSI_51\n",
      "638. RSI_52\n",
      "639. RSI_53\n",
      "640. RSI_54\n",
      "641. RSI_55\n",
      "642. RSI_56\n",
      "643. RSI_57\n",
      "644. RSI_58\n",
      "645. RSI_59\n",
      "646. RSI_4_lag_1\n",
      "647. RSI_4_lag_2\n",
      "648. RSI_4_lag_3\n",
      "649. RSI_4_lag_4\n",
      "650. RSI_5_lag_1\n",
      "651. RSI_5_lag_2\n",
      "652. RSI_5_lag_3\n",
      "653. RSI_5_lag_4\n",
      "654. RSI_6_lag_1\n",
      "655. RSI_6_lag_2\n",
      "656. RSI_6_lag_3\n",
      "657. RSI_6_lag_4\n",
      "658. RSI_7_lag_1\n",
      "659. RSI_7_lag_2\n",
      "660. RSI_7_lag_3\n",
      "661. RSI_7_lag_4\n",
      "662. RSI_8_lag_1\n",
      "663. RSI_8_lag_2\n",
      "664. RSI_8_lag_3\n",
      "665. RSI_8_lag_4\n",
      "666. RSI_9_lag_1\n",
      "667. RSI_9_lag_2\n",
      "668. RSI_9_lag_3\n",
      "669. RSI_9_lag_4\n",
      "670. RSI_10_lag_1\n",
      "671. RSI_10_lag_2\n",
      "672. RSI_10_lag_3\n",
      "673. RSI_10_lag_4\n",
      "674. RSI_11_lag_1\n",
      "675. RSI_11_lag_2\n",
      "676. RSI_11_lag_3\n",
      "677. RSI_11_lag_4\n",
      "678. RSI_12_lag_1\n",
      "679. RSI_12_lag_2\n",
      "680. RSI_12_lag_3\n",
      "681. RSI_12_lag_4\n",
      "682. RSI_13_lag_1\n",
      "683. RSI_13_lag_2\n",
      "684. RSI_13_lag_3\n",
      "685. RSI_13_lag_4\n",
      "686. RSI_14_lag_1\n",
      "687. RSI_14_lag_2\n",
      "688. RSI_14_lag_3\n",
      "689. RSI_14_lag_4\n",
      "690. RSI_15_lag_1\n",
      "691. RSI_15_lag_2\n",
      "692. RSI_15_lag_3\n",
      "693. RSI_15_lag_4\n",
      "694. RSI_16_lag_1\n",
      "695. RSI_16_lag_2\n",
      "696. RSI_16_lag_3\n",
      "697. RSI_16_lag_4\n",
      "698. RSI_17_lag_1\n",
      "699. RSI_17_lag_2\n",
      "700. RSI_17_lag_3\n",
      "701. RSI_17_lag_4\n",
      "702. RSI_18_lag_1\n",
      "703. RSI_18_lag_2\n",
      "704. RSI_18_lag_3\n",
      "705. RSI_18_lag_4\n",
      "706. RSI_19_lag_1\n",
      "707. RSI_19_lag_2\n",
      "708. RSI_19_lag_3\n",
      "709. RSI_19_lag_4\n",
      "710. RSI_20_lag_1\n",
      "711. RSI_20_lag_2\n",
      "712. RSI_20_lag_3\n",
      "713. RSI_20_lag_4\n",
      "714. RSI_21_lag_1\n",
      "715. RSI_21_lag_2\n",
      "716. RSI_21_lag_3\n",
      "717. RSI_21_lag_4\n",
      "718. RSI_22_lag_1\n",
      "719. RSI_22_lag_2\n",
      "720. RSI_22_lag_3\n",
      "721. RSI_22_lag_4\n",
      "722. RSI_23_lag_1\n",
      "723. RSI_23_lag_2\n",
      "724. RSI_23_lag_3\n",
      "725. RSI_23_lag_4\n",
      "726. RSI_24_lag_1\n",
      "727. RSI_24_lag_2\n",
      "728. RSI_24_lag_3\n",
      "729. RSI_24_lag_4\n",
      "730. RSI_25_lag_1\n",
      "731. RSI_25_lag_2\n",
      "732. RSI_25_lag_3\n",
      "733. RSI_25_lag_4\n",
      "734. RSI_26_lag_1\n",
      "735. RSI_26_lag_2\n",
      "736. RSI_26_lag_3\n",
      "737. RSI_26_lag_4\n",
      "738. RSI_27_lag_1\n",
      "739. RSI_27_lag_2\n",
      "740. RSI_27_lag_3\n",
      "741. RSI_27_lag_4\n",
      "742. RSI_28_lag_1\n",
      "743. RSI_28_lag_2\n",
      "744. RSI_28_lag_3\n",
      "745. RSI_28_lag_4\n",
      "746. RSI_29_lag_1\n",
      "747. RSI_29_lag_2\n",
      "748. RSI_29_lag_3\n",
      "749. RSI_29_lag_4\n",
      "750. RSI_30_lag_1\n",
      "751. RSI_30_lag_2\n",
      "752. RSI_30_lag_3\n",
      "753. RSI_30_lag_4\n",
      "754. RSI_31_lag_1\n",
      "755. RSI_31_lag_2\n",
      "756. RSI_31_lag_3\n",
      "757. RSI_31_lag_4\n",
      "758. RSI_32_lag_1\n",
      "759. RSI_32_lag_2\n",
      "760. RSI_32_lag_3\n",
      "761. RSI_32_lag_4\n",
      "762. RSI_33_lag_1\n",
      "763. RSI_33_lag_2\n",
      "764. RSI_33_lag_3\n",
      "765. RSI_33_lag_4\n",
      "766. RSI_34_lag_1\n",
      "767. RSI_34_lag_2\n",
      "768. RSI_34_lag_3\n",
      "769. RSI_34_lag_4\n",
      "770. RSI_35_lag_1\n",
      "771. RSI_35_lag_2\n",
      "772. RSI_35_lag_3\n",
      "773. RSI_35_lag_4\n",
      "774. RSI_36_lag_1\n",
      "775. RSI_36_lag_2\n",
      "776. RSI_36_lag_3\n",
      "777. RSI_36_lag_4\n",
      "778. RSI_37_lag_1\n",
      "779. RSI_37_lag_2\n",
      "780. RSI_37_lag_3\n",
      "781. RSI_37_lag_4\n",
      "782. RSI_38_lag_1\n",
      "783. RSI_38_lag_2\n",
      "784. RSI_38_lag_3\n",
      "785. RSI_38_lag_4\n",
      "786. RSI_39_lag_1\n",
      "787. RSI_39_lag_2\n",
      "788. RSI_39_lag_3\n",
      "789. RSI_39_lag_4\n",
      "790. RSI_40_lag_1\n",
      "791. RSI_40_lag_2\n",
      "792. RSI_40_lag_3\n",
      "793. RSI_40_lag_4\n",
      "794. RSI_41_lag_1\n",
      "795. RSI_41_lag_2\n",
      "796. RSI_41_lag_3\n",
      "797. RSI_41_lag_4\n",
      "798. RSI_42_lag_1\n",
      "799. RSI_42_lag_2\n",
      "800. RSI_42_lag_3\n",
      "801. RSI_42_lag_4\n",
      "802. RSI_43_lag_1\n",
      "803. RSI_43_lag_2\n",
      "804. RSI_43_lag_3\n",
      "805. RSI_43_lag_4\n",
      "806. RSI_44_lag_1\n",
      "807. RSI_44_lag_2\n",
      "808. RSI_44_lag_3\n",
      "809. RSI_44_lag_4\n",
      "810. RSI_45_lag_1\n",
      "811. RSI_45_lag_2\n",
      "812. RSI_45_lag_3\n",
      "813. RSI_45_lag_4\n",
      "814. RSI_46_lag_1\n",
      "815. RSI_46_lag_2\n",
      "816. RSI_46_lag_3\n",
      "817. RSI_46_lag_4\n",
      "818. RSI_47_lag_1\n",
      "819. RSI_47_lag_2\n",
      "820. RSI_47_lag_3\n",
      "821. RSI_47_lag_4\n",
      "822. RSI_48_lag_1\n",
      "823. RSI_48_lag_2\n",
      "824. RSI_48_lag_3\n",
      "825. RSI_48_lag_4\n",
      "826. RSI_49_lag_1\n",
      "827. RSI_49_lag_2\n",
      "828. RSI_49_lag_3\n",
      "829. RSI_49_lag_4\n",
      "830. RSI_50_lag_1\n",
      "831. RSI_50_lag_2\n",
      "832. RSI_50_lag_3\n",
      "833. RSI_50_lag_4\n",
      "834. RSI_51_lag_1\n",
      "835. RSI_51_lag_2\n",
      "836. RSI_51_lag_3\n",
      "837. RSI_51_lag_4\n",
      "838. RSI_52_lag_1\n",
      "839. RSI_52_lag_2\n",
      "840. RSI_52_lag_3\n",
      "841. RSI_52_lag_4\n",
      "842. RSI_53_lag_1\n",
      "843. RSI_53_lag_2\n",
      "844. RSI_53_lag_3\n",
      "845. RSI_53_lag_4\n",
      "846. RSI_54_lag_1\n",
      "847. RSI_54_lag_2\n",
      "848. RSI_54_lag_3\n",
      "849. RSI_54_lag_4\n",
      "850. RSI_55_lag_1\n",
      "851. RSI_55_lag_2\n",
      "852. RSI_55_lag_3\n",
      "853. RSI_55_lag_4\n",
      "854. RSI_56_lag_1\n",
      "855. RSI_56_lag_2\n",
      "856. RSI_56_lag_3\n",
      "857. RSI_56_lag_4\n",
      "858. RSI_57_lag_1\n",
      "859. RSI_57_lag_2\n",
      "860. RSI_57_lag_3\n",
      "861. RSI_57_lag_4\n",
      "862. RSI_58_lag_1\n",
      "863. RSI_58_lag_2\n",
      "864. RSI_58_lag_3\n",
      "865. RSI_58_lag_4\n",
      "866. RSI_59_lag_1\n",
      "867. RSI_59_lag_2\n",
      "868. RSI_59_lag_3\n",
      "869. RSI_59_lag_4\n",
      "870. bb_4_middle_band\n",
      "871. bb_4_upper_band\n",
      "872. bb_4_lower_band\n",
      "873. bb_4_bandwidth\n",
      "874. bb_4_percent_b\n",
      "875. bb_4_cross_up\n",
      "876. bb_4_cross_down\n",
      "877. bb_5_middle_band\n",
      "878. bb_5_upper_band\n",
      "879. bb_5_lower_band\n",
      "880. bb_5_bandwidth\n",
      "881. bb_5_percent_b\n",
      "882. bb_5_cross_up\n",
      "883. bb_5_cross_down\n",
      "884. bb_6_middle_band\n",
      "885. bb_6_upper_band\n",
      "886. bb_6_lower_band\n",
      "887. bb_6_bandwidth\n",
      "888. bb_6_percent_b\n",
      "889. bb_6_cross_up\n",
      "890. bb_6_cross_down\n",
      "891. bb_7_middle_band\n",
      "892. bb_7_upper_band\n",
      "893. bb_7_lower_band\n",
      "894. bb_7_bandwidth\n",
      "895. bb_7_percent_b\n",
      "896. bb_7_cross_up\n",
      "897. bb_7_cross_down\n",
      "898. bb_8_middle_band\n",
      "899. bb_8_upper_band\n",
      "900. bb_8_lower_band\n",
      "901. bb_8_bandwidth\n",
      "902. bb_8_percent_b\n",
      "903. bb_8_cross_up\n",
      "904. bb_8_cross_down\n",
      "905. bb_9_middle_band\n",
      "906. bb_9_upper_band\n",
      "907. bb_9_lower_band\n",
      "908. bb_9_bandwidth\n",
      "909. bb_9_percent_b\n",
      "910. bb_9_cross_up\n",
      "911. bb_9_cross_down\n",
      "912. bb_10_middle_band\n",
      "913. bb_10_upper_band\n",
      "914. bb_10_lower_band\n",
      "915. bb_10_bandwidth\n",
      "916. bb_10_percent_b\n",
      "917. bb_10_cross_up\n",
      "918. bb_10_cross_down\n",
      "919. bb_11_middle_band\n",
      "920. bb_11_upper_band\n",
      "921. bb_11_lower_band\n",
      "922. bb_11_bandwidth\n",
      "923. bb_11_percent_b\n",
      "924. bb_11_cross_up\n",
      "925. bb_11_cross_down\n",
      "926. bb_12_middle_band\n",
      "927. bb_12_upper_band\n",
      "928. bb_12_lower_band\n",
      "929. bb_12_bandwidth\n",
      "930. bb_12_percent_b\n",
      "931. bb_12_cross_up\n",
      "932. bb_12_cross_down\n",
      "933. bb_13_middle_band\n",
      "934. bb_13_upper_band\n",
      "935. bb_13_lower_band\n",
      "936. bb_13_bandwidth\n",
      "937. bb_13_percent_b\n",
      "938. bb_13_cross_up\n",
      "939. bb_13_cross_down\n",
      "940. bb_14_middle_band\n",
      "941. bb_14_upper_band\n",
      "942. bb_14_lower_band\n",
      "943. bb_14_bandwidth\n",
      "944. bb_14_percent_b\n",
      "945. bb_14_cross_up\n",
      "946. bb_14_cross_down\n",
      "947. bb_15_middle_band\n",
      "948. bb_15_upper_band\n",
      "949. bb_15_lower_band\n",
      "950. bb_15_bandwidth\n",
      "951. bb_15_percent_b\n",
      "952. bb_15_cross_up\n",
      "953. bb_15_cross_down\n",
      "954. bb_16_middle_band\n",
      "955. bb_16_upper_band\n",
      "956. bb_16_lower_band\n",
      "957. bb_16_bandwidth\n",
      "958. bb_16_percent_b\n",
      "959. bb_16_cross_up\n",
      "960. bb_16_cross_down\n",
      "961. bb_17_middle_band\n",
      "962. bb_17_upper_band\n",
      "963. bb_17_lower_band\n",
      "964. bb_17_bandwidth\n",
      "965. bb_17_percent_b\n",
      "966. bb_17_cross_up\n",
      "967. bb_17_cross_down\n",
      "968. bb_18_middle_band\n",
      "969. bb_18_upper_band\n",
      "970. bb_18_lower_band\n",
      "971. bb_18_bandwidth\n",
      "972. bb_18_percent_b\n",
      "973. bb_18_cross_up\n",
      "974. bb_18_cross_down\n",
      "975. bb_19_middle_band\n",
      "976. bb_19_upper_band\n",
      "977. bb_19_lower_band\n",
      "978. bb_19_bandwidth\n",
      "979. bb_19_percent_b\n",
      "980. bb_19_cross_up\n",
      "981. bb_19_cross_down\n",
      "982. bb_20_middle_band\n",
      "983. bb_20_upper_band\n",
      "984. bb_20_lower_band\n",
      "985. bb_20_bandwidth\n",
      "986. bb_20_percent_b\n",
      "987. bb_20_cross_up\n",
      "988. bb_20_cross_down\n",
      "989. bb_21_middle_band\n",
      "990. bb_21_upper_band\n",
      "991. bb_21_lower_band\n",
      "992. bb_21_bandwidth\n",
      "993. bb_21_percent_b\n",
      "994. bb_21_cross_up\n",
      "995. bb_21_cross_down\n",
      "996. bb_22_middle_band\n",
      "997. bb_22_upper_band\n",
      "998. bb_22_lower_band\n",
      "999. bb_22_bandwidth\n",
      "1000. bb_22_percent_b\n",
      "1001. bb_22_cross_up\n",
      "1002. bb_22_cross_down\n",
      "1003. bb_23_middle_band\n",
      "1004. bb_23_upper_band\n",
      "1005. bb_23_lower_band\n",
      "1006. bb_23_bandwidth\n",
      "1007. bb_23_percent_b\n",
      "1008. bb_23_cross_up\n",
      "1009. bb_23_cross_down\n",
      "1010. bb_24_middle_band\n",
      "1011. bb_24_upper_band\n",
      "1012. bb_24_lower_band\n",
      "1013. bb_24_bandwidth\n",
      "1014. bb_24_percent_b\n",
      "1015. bb_24_cross_up\n",
      "1016. bb_24_cross_down\n",
      "1017. bb_25_middle_band\n",
      "1018. bb_25_upper_band\n",
      "1019. bb_25_lower_band\n",
      "1020. bb_25_bandwidth\n",
      "1021. bb_25_percent_b\n",
      "1022. bb_25_cross_up\n",
      "1023. bb_25_cross_down\n",
      "1024. bb_26_middle_band\n",
      "1025. bb_26_upper_band\n",
      "1026. bb_26_lower_band\n",
      "1027. bb_26_bandwidth\n",
      "1028. bb_26_percent_b\n",
      "1029. bb_26_cross_up\n",
      "1030. bb_26_cross_down\n",
      "1031. bb_27_middle_band\n",
      "1032. bb_27_upper_band\n",
      "1033. bb_27_lower_band\n",
      "1034. bb_27_bandwidth\n",
      "1035. bb_27_percent_b\n",
      "1036. bb_27_cross_up\n",
      "1037. bb_27_cross_down\n",
      "1038. bb_28_middle_band\n",
      "1039. bb_28_upper_band\n",
      "1040. bb_28_lower_band\n",
      "1041. bb_28_bandwidth\n",
      "1042. bb_28_percent_b\n",
      "1043. bb_28_cross_up\n",
      "1044. bb_28_cross_down\n",
      "1045. bb_29_middle_band\n",
      "1046. bb_29_upper_band\n",
      "1047. bb_29_lower_band\n",
      "1048. bb_29_bandwidth\n",
      "1049. bb_29_percent_b\n",
      "1050. bb_29_cross_up\n",
      "1051. bb_29_cross_down\n",
      "1052. bb_30_middle_band\n",
      "1053. bb_30_upper_band\n",
      "1054. bb_30_lower_band\n",
      "1055. bb_30_bandwidth\n",
      "1056. bb_30_percent_b\n",
      "1057. bb_30_cross_up\n",
      "1058. bb_30_cross_down\n",
      "1059. bb_31_middle_band\n",
      "1060. bb_31_upper_band\n",
      "1061. bb_31_lower_band\n",
      "1062. bb_31_bandwidth\n",
      "1063. bb_31_percent_b\n",
      "1064. bb_31_cross_up\n",
      "1065. bb_31_cross_down\n",
      "1066. bb_32_middle_band\n",
      "1067. bb_32_upper_band\n",
      "1068. bb_32_lower_band\n",
      "1069. bb_32_bandwidth\n",
      "1070. bb_32_percent_b\n",
      "1071. bb_32_cross_up\n",
      "1072. bb_32_cross_down\n",
      "1073. bb_33_middle_band\n",
      "1074. bb_33_upper_band\n",
      "1075. bb_33_lower_band\n",
      "1076. bb_33_bandwidth\n",
      "1077. bb_33_percent_b\n",
      "1078. bb_33_cross_up\n",
      "1079. bb_33_cross_down\n",
      "1080. bb_34_middle_band\n",
      "1081. bb_34_upper_band\n",
      "1082. bb_34_lower_band\n",
      "1083. bb_34_bandwidth\n",
      "1084. bb_34_percent_b\n",
      "1085. bb_34_cross_up\n",
      "1086. bb_34_cross_down\n",
      "1087. bb_35_middle_band\n",
      "1088. bb_35_upper_band\n",
      "1089. bb_35_lower_band\n",
      "1090. bb_35_bandwidth\n",
      "1091. bb_35_percent_b\n",
      "1092. bb_35_cross_up\n",
      "1093. bb_35_cross_down\n",
      "1094. bb_36_middle_band\n",
      "1095. bb_36_upper_band\n",
      "1096. bb_36_lower_band\n",
      "1097. bb_36_bandwidth\n",
      "1098. bb_36_percent_b\n",
      "1099. bb_36_cross_up\n",
      "1100. bb_36_cross_down\n",
      "1101. bb_37_middle_band\n",
      "1102. bb_37_upper_band\n",
      "1103. bb_37_lower_band\n",
      "1104. bb_37_bandwidth\n",
      "1105. bb_37_percent_b\n",
      "1106. bb_37_cross_up\n",
      "1107. bb_37_cross_down\n",
      "1108. bb_38_middle_band\n",
      "1109. bb_38_upper_band\n",
      "1110. bb_38_lower_band\n",
      "1111. bb_38_bandwidth\n",
      "1112. bb_38_percent_b\n",
      "1113. bb_38_cross_up\n",
      "1114. bb_38_cross_down\n",
      "1115. bb_39_middle_band\n",
      "1116. bb_39_upper_band\n",
      "1117. bb_39_lower_band\n",
      "1118. bb_39_bandwidth\n",
      "1119. bb_39_percent_b\n",
      "1120. bb_39_cross_up\n",
      "1121. bb_39_cross_down\n",
      "1122. bb_40_middle_band\n",
      "1123. bb_40_upper_band\n",
      "1124. bb_40_lower_band\n",
      "1125. bb_40_bandwidth\n",
      "1126. bb_40_percent_b\n",
      "1127. bb_40_cross_up\n",
      "1128. bb_40_cross_down\n",
      "1129. bb_41_middle_band\n",
      "1130. bb_41_upper_band\n",
      "1131. bb_41_lower_band\n",
      "1132. bb_41_bandwidth\n",
      "1133. bb_41_percent_b\n",
      "1134. bb_41_cross_up\n",
      "1135. bb_41_cross_down\n",
      "1136. bb_42_middle_band\n",
      "1137. bb_42_upper_band\n",
      "1138. bb_42_lower_band\n",
      "1139. bb_42_bandwidth\n",
      "1140. bb_42_percent_b\n",
      "1141. bb_42_cross_up\n",
      "1142. bb_42_cross_down\n",
      "1143. bb_43_middle_band\n",
      "1144. bb_43_upper_band\n",
      "1145. bb_43_lower_band\n",
      "1146. bb_43_bandwidth\n",
      "1147. bb_43_percent_b\n",
      "1148. bb_43_cross_up\n",
      "1149. bb_43_cross_down\n",
      "1150. bb_44_middle_band\n",
      "1151. bb_44_upper_band\n",
      "1152. bb_44_lower_band\n",
      "1153. bb_44_bandwidth\n",
      "1154. bb_44_percent_b\n",
      "1155. bb_44_cross_up\n",
      "1156. bb_44_cross_down\n",
      "1157. bb_45_middle_band\n",
      "1158. bb_45_upper_band\n",
      "1159. bb_45_lower_band\n",
      "1160. bb_45_bandwidth\n",
      "1161. bb_45_percent_b\n",
      "1162. bb_45_cross_up\n",
      "1163. bb_45_cross_down\n",
      "1164. bb_46_middle_band\n",
      "1165. bb_46_upper_band\n",
      "1166. bb_46_lower_band\n",
      "1167. bb_46_bandwidth\n",
      "1168. bb_46_percent_b\n",
      "1169. bb_46_cross_up\n",
      "1170. bb_46_cross_down\n",
      "1171. bb_47_middle_band\n",
      "1172. bb_47_upper_band\n",
      "1173. bb_47_lower_band\n",
      "1174. bb_47_bandwidth\n",
      "1175. bb_47_percent_b\n",
      "1176. bb_47_cross_up\n",
      "1177. bb_47_cross_down\n",
      "1178. bb_48_middle_band\n",
      "1179. bb_48_upper_band\n",
      "1180. bb_48_lower_band\n",
      "1181. bb_48_bandwidth\n",
      "1182. bb_48_percent_b\n",
      "1183. bb_48_cross_up\n",
      "1184. bb_48_cross_down\n",
      "1185. bb_49_middle_band\n",
      "1186. bb_49_upper_band\n",
      "1187. bb_49_lower_band\n",
      "1188. bb_49_bandwidth\n",
      "1189. bb_49_percent_b\n",
      "1190. bb_49_cross_up\n",
      "1191. bb_49_cross_down\n",
      "1192. bb_50_middle_band\n",
      "1193. bb_50_upper_band\n",
      "1194. bb_50_lower_band\n",
      "1195. bb_50_bandwidth\n",
      "1196. bb_50_percent_b\n",
      "1197. bb_50_cross_up\n",
      "1198. bb_50_cross_down\n",
      "1199. bb_51_middle_band\n",
      "1200. bb_51_upper_band\n",
      "1201. bb_51_lower_band\n",
      "1202. bb_51_bandwidth\n",
      "1203. bb_51_percent_b\n",
      "1204. bb_51_cross_up\n",
      "1205. bb_51_cross_down\n",
      "1206. bb_52_middle_band\n",
      "1207. bb_52_upper_band\n",
      "1208. bb_52_lower_band\n",
      "1209. bb_52_bandwidth\n",
      "1210. bb_52_percent_b\n",
      "1211. bb_52_cross_up\n",
      "1212. bb_52_cross_down\n",
      "1213. bb_53_middle_band\n",
      "1214. bb_53_upper_band\n",
      "1215. bb_53_lower_band\n",
      "1216. bb_53_bandwidth\n",
      "1217. bb_53_percent_b\n",
      "1218. bb_53_cross_up\n",
      "1219. bb_53_cross_down\n",
      "1220. bb_54_middle_band\n",
      "1221. bb_54_upper_band\n",
      "1222. bb_54_lower_band\n",
      "1223. bb_54_bandwidth\n",
      "1224. bb_54_percent_b\n",
      "1225. bb_54_cross_up\n",
      "1226. bb_54_cross_down\n",
      "1227. bb_55_middle_band\n",
      "1228. bb_55_upper_band\n",
      "1229. bb_55_lower_band\n",
      "1230. bb_55_bandwidth\n",
      "1231. bb_55_percent_b\n",
      "1232. bb_55_cross_up\n",
      "1233. bb_55_cross_down\n",
      "1234. bb_56_middle_band\n",
      "1235. bb_56_upper_band\n",
      "1236. bb_56_lower_band\n",
      "1237. bb_56_bandwidth\n",
      "1238. bb_56_percent_b\n",
      "1239. bb_56_cross_up\n",
      "1240. bb_56_cross_down\n",
      "1241. bb_57_middle_band\n",
      "1242. bb_57_upper_band\n",
      "1243. bb_57_lower_band\n",
      "1244. bb_57_bandwidth\n",
      "1245. bb_57_percent_b\n",
      "1246. bb_57_cross_up\n",
      "1247. bb_57_cross_down\n",
      "1248. bb_58_middle_band\n",
      "1249. bb_58_upper_band\n",
      "1250. bb_58_lower_band\n",
      "1251. bb_58_bandwidth\n",
      "1252. bb_58_percent_b\n",
      "1253. bb_58_cross_up\n",
      "1254. bb_58_cross_down\n",
      "1255. bb_59_middle_band\n",
      "1256. bb_59_upper_band\n",
      "1257. bb_59_lower_band\n",
      "1258. bb_59_bandwidth\n",
      "1259. bb_59_percent_b\n",
      "1260. bb_59_cross_up\n",
      "1261. bb_59_cross_down\n",
      "1262. historical_volatility_4\n",
      "1263. historical_volatility_5\n",
      "1264. historical_volatility_6\n",
      "1265. historical_volatility_7\n",
      "1266. historical_volatility_8\n",
      "1267. historical_volatility_9\n",
      "1268. historical_volatility_10\n",
      "1269. historical_volatility_11\n",
      "1270. historical_volatility_12\n",
      "1271. historical_volatility_13\n",
      "1272. historical_volatility_14\n",
      "1273. historical_volatility_15\n",
      "1274. historical_volatility_16\n",
      "1275. historical_volatility_17\n",
      "1276. historical_volatility_18\n",
      "1277. historical_volatility_19\n",
      "1278. historical_volatility_20\n",
      "1279. historical_volatility_21\n",
      "1280. historical_volatility_22\n",
      "1281. historical_volatility_23\n",
      "1282. historical_volatility_24\n",
      "1283. historical_volatility_25\n",
      "1284. historical_volatility_26\n",
      "1285. historical_volatility_27\n",
      "1286. historical_volatility_28\n",
      "1287. historical_volatility_29\n",
      "1288. historical_volatility_30\n",
      "1289. historical_volatility_31\n",
      "1290. historical_volatility_32\n",
      "1291. historical_volatility_33\n",
      "1292. historical_volatility_34\n",
      "1293. historical_volatility_35\n",
      "1294. historical_volatility_36\n",
      "1295. historical_volatility_37\n",
      "1296. historical_volatility_38\n",
      "1297. historical_volatility_39\n",
      "1298. historical_volatility_40\n",
      "1299. historical_volatility_41\n",
      "1300. historical_volatility_42\n",
      "1301. historical_volatility_43\n",
      "1302. historical_volatility_44\n",
      "1303. historical_volatility_45\n",
      "1304. historical_volatility_46\n",
      "1305. historical_volatility_47\n",
      "1306. historical_volatility_48\n",
      "1307. historical_volatility_49\n",
      "1308. historical_volatility_50\n",
      "1309. historical_volatility_51\n",
      "1310. historical_volatility_52\n",
      "1311. historical_volatility_53\n",
      "1312. historical_volatility_54\n",
      "1313. historical_volatility_55\n",
      "1314. historical_volatility_56\n",
      "1315. historical_volatility_57\n",
      "1316. historical_volatility_58\n",
      "1317. historical_volatility_59\n",
      "1318. historical_volatility_4_lag_1\n",
      "1319. historical_volatility_4_lag_2\n",
      "1320. historical_volatility_4_lag_3\n",
      "1321. historical_volatility_4_lag_4\n",
      "1322. historical_volatility_5_lag_1\n",
      "1323. historical_volatility_5_lag_2\n",
      "1324. historical_volatility_5_lag_3\n",
      "1325. historical_volatility_5_lag_4\n",
      "1326. historical_volatility_6_lag_1\n",
      "1327. historical_volatility_6_lag_2\n",
      "1328. historical_volatility_6_lag_3\n",
      "1329. historical_volatility_6_lag_4\n",
      "1330. historical_volatility_7_lag_1\n",
      "1331. historical_volatility_7_lag_2\n",
      "1332. historical_volatility_7_lag_3\n",
      "1333. historical_volatility_7_lag_4\n",
      "1334. historical_volatility_8_lag_1\n",
      "1335. historical_volatility_8_lag_2\n",
      "1336. historical_volatility_8_lag_3\n",
      "1337. historical_volatility_8_lag_4\n",
      "1338. historical_volatility_9_lag_1\n",
      "1339. historical_volatility_9_lag_2\n",
      "1340. historical_volatility_9_lag_3\n",
      "1341. historical_volatility_9_lag_4\n",
      "1342. historical_volatility_10_lag_1\n",
      "1343. historical_volatility_10_lag_2\n",
      "1344. historical_volatility_10_lag_3\n",
      "1345. historical_volatility_10_lag_4\n",
      "1346. historical_volatility_11_lag_1\n",
      "1347. historical_volatility_11_lag_2\n",
      "1348. historical_volatility_11_lag_3\n",
      "1349. historical_volatility_11_lag_4\n",
      "1350. historical_volatility_12_lag_1\n",
      "1351. historical_volatility_12_lag_2\n",
      "1352. historical_volatility_12_lag_3\n",
      "1353. historical_volatility_12_lag_4\n",
      "1354. historical_volatility_13_lag_1\n",
      "1355. historical_volatility_13_lag_2\n",
      "1356. historical_volatility_13_lag_3\n",
      "1357. historical_volatility_13_lag_4\n",
      "1358. historical_volatility_14_lag_1\n",
      "1359. historical_volatility_14_lag_2\n",
      "1360. historical_volatility_14_lag_3\n",
      "1361. historical_volatility_14_lag_4\n",
      "1362. historical_volatility_15_lag_1\n",
      "1363. historical_volatility_15_lag_2\n",
      "1364. historical_volatility_15_lag_3\n",
      "1365. historical_volatility_15_lag_4\n",
      "1366. historical_volatility_16_lag_1\n",
      "1367. historical_volatility_16_lag_2\n",
      "1368. historical_volatility_16_lag_3\n",
      "1369. historical_volatility_16_lag_4\n",
      "1370. historical_volatility_17_lag_1\n",
      "1371. historical_volatility_17_lag_2\n",
      "1372. historical_volatility_17_lag_3\n",
      "1373. historical_volatility_17_lag_4\n",
      "1374. historical_volatility_18_lag_1\n",
      "1375. historical_volatility_18_lag_2\n",
      "1376. historical_volatility_18_lag_3\n",
      "1377. historical_volatility_18_lag_4\n",
      "1378. historical_volatility_19_lag_1\n",
      "1379. historical_volatility_19_lag_2\n",
      "1380. historical_volatility_19_lag_3\n",
      "1381. historical_volatility_19_lag_4\n",
      "1382. historical_volatility_20_lag_1\n",
      "1383. historical_volatility_20_lag_2\n",
      "1384. historical_volatility_20_lag_3\n",
      "1385. historical_volatility_20_lag_4\n",
      "1386. historical_volatility_21_lag_1\n",
      "1387. historical_volatility_21_lag_2\n",
      "1388. historical_volatility_21_lag_3\n",
      "1389. historical_volatility_21_lag_4\n",
      "1390. historical_volatility_22_lag_1\n",
      "1391. historical_volatility_22_lag_2\n",
      "1392. historical_volatility_22_lag_3\n",
      "1393. historical_volatility_22_lag_4\n",
      "1394. historical_volatility_23_lag_1\n",
      "1395. historical_volatility_23_lag_2\n",
      "1396. historical_volatility_23_lag_3\n",
      "1397. historical_volatility_23_lag_4\n",
      "1398. historical_volatility_24_lag_1\n",
      "1399. historical_volatility_24_lag_2\n",
      "1400. historical_volatility_24_lag_3\n",
      "1401. historical_volatility_24_lag_4\n",
      "1402. historical_volatility_25_lag_1\n",
      "1403. historical_volatility_25_lag_2\n",
      "1404. historical_volatility_25_lag_3\n",
      "1405. historical_volatility_25_lag_4\n",
      "1406. historical_volatility_26_lag_1\n",
      "1407. historical_volatility_26_lag_2\n",
      "1408. historical_volatility_26_lag_3\n",
      "1409. historical_volatility_26_lag_4\n",
      "1410. historical_volatility_27_lag_1\n",
      "1411. historical_volatility_27_lag_2\n",
      "1412. historical_volatility_27_lag_3\n",
      "1413. historical_volatility_27_lag_4\n",
      "1414. historical_volatility_28_lag_1\n",
      "1415. historical_volatility_28_lag_2\n",
      "1416. historical_volatility_28_lag_3\n",
      "1417. historical_volatility_28_lag_4\n",
      "1418. historical_volatility_29_lag_1\n",
      "1419. historical_volatility_29_lag_2\n",
      "1420. historical_volatility_29_lag_3\n",
      "1421. historical_volatility_29_lag_4\n",
      "1422. historical_volatility_30_lag_1\n",
      "1423. historical_volatility_30_lag_2\n",
      "1424. historical_volatility_30_lag_3\n",
      "1425. historical_volatility_30_lag_4\n",
      "1426. historical_volatility_31_lag_1\n",
      "1427. historical_volatility_31_lag_2\n",
      "1428. historical_volatility_31_lag_3\n",
      "1429. historical_volatility_31_lag_4\n",
      "1430. historical_volatility_32_lag_1\n",
      "1431. historical_volatility_32_lag_2\n",
      "1432. historical_volatility_32_lag_3\n",
      "1433. historical_volatility_32_lag_4\n",
      "1434. historical_volatility_33_lag_1\n",
      "1435. historical_volatility_33_lag_2\n",
      "1436. historical_volatility_33_lag_3\n",
      "1437. historical_volatility_33_lag_4\n",
      "1438. historical_volatility_34_lag_1\n",
      "1439. historical_volatility_34_lag_2\n",
      "1440. historical_volatility_34_lag_3\n",
      "1441. historical_volatility_34_lag_4\n",
      "1442. historical_volatility_35_lag_1\n",
      "1443. historical_volatility_35_lag_2\n",
      "1444. historical_volatility_35_lag_3\n",
      "1445. historical_volatility_35_lag_4\n",
      "1446. historical_volatility_36_lag_1\n",
      "1447. historical_volatility_36_lag_2\n",
      "1448. historical_volatility_36_lag_3\n",
      "1449. historical_volatility_36_lag_4\n",
      "1450. historical_volatility_37_lag_1\n",
      "1451. historical_volatility_37_lag_2\n",
      "1452. historical_volatility_37_lag_3\n",
      "1453. historical_volatility_37_lag_4\n",
      "1454. historical_volatility_38_lag_1\n",
      "1455. historical_volatility_38_lag_2\n",
      "1456. historical_volatility_38_lag_3\n",
      "1457. historical_volatility_38_lag_4\n",
      "1458. historical_volatility_39_lag_1\n",
      "1459. historical_volatility_39_lag_2\n",
      "1460. historical_volatility_39_lag_3\n",
      "1461. historical_volatility_39_lag_4\n",
      "1462. historical_volatility_40_lag_1\n",
      "1463. historical_volatility_40_lag_2\n",
      "1464. historical_volatility_40_lag_3\n",
      "1465. historical_volatility_40_lag_4\n",
      "1466. historical_volatility_41_lag_1\n",
      "1467. historical_volatility_41_lag_2\n",
      "1468. historical_volatility_41_lag_3\n",
      "1469. historical_volatility_41_lag_4\n",
      "1470. historical_volatility_42_lag_1\n",
      "1471. historical_volatility_42_lag_2\n",
      "1472. historical_volatility_42_lag_3\n",
      "1473. historical_volatility_42_lag_4\n",
      "1474. historical_volatility_43_lag_1\n",
      "1475. historical_volatility_43_lag_2\n",
      "1476. historical_volatility_43_lag_3\n",
      "1477. historical_volatility_43_lag_4\n",
      "1478. historical_volatility_44_lag_1\n",
      "1479. historical_volatility_44_lag_2\n",
      "1480. historical_volatility_44_lag_3\n",
      "1481. historical_volatility_44_lag_4\n",
      "1482. historical_volatility_45_lag_1\n",
      "1483. historical_volatility_45_lag_2\n",
      "1484. historical_volatility_45_lag_3\n",
      "1485. historical_volatility_45_lag_4\n",
      "1486. historical_volatility_46_lag_1\n",
      "1487. historical_volatility_46_lag_2\n",
      "1488. historical_volatility_46_lag_3\n",
      "1489. historical_volatility_46_lag_4\n",
      "1490. historical_volatility_47_lag_1\n",
      "1491. historical_volatility_47_lag_2\n",
      "1492. historical_volatility_47_lag_3\n",
      "1493. historical_volatility_47_lag_4\n",
      "1494. historical_volatility_48_lag_1\n",
      "1495. historical_volatility_48_lag_2\n",
      "1496. historical_volatility_48_lag_3\n",
      "1497. historical_volatility_48_lag_4\n",
      "1498. historical_volatility_49_lag_1\n",
      "1499. historical_volatility_49_lag_2\n",
      "1500. historical_volatility_49_lag_3\n",
      "1501. historical_volatility_49_lag_4\n",
      "1502. historical_volatility_50_lag_1\n",
      "1503. historical_volatility_50_lag_2\n",
      "1504. historical_volatility_50_lag_3\n",
      "1505. historical_volatility_50_lag_4\n",
      "1506. historical_volatility_51_lag_1\n",
      "1507. historical_volatility_51_lag_2\n",
      "1508. historical_volatility_51_lag_3\n",
      "1509. historical_volatility_51_lag_4\n",
      "1510. historical_volatility_52_lag_1\n",
      "1511. historical_volatility_52_lag_2\n",
      "1512. historical_volatility_52_lag_3\n",
      "1513. historical_volatility_52_lag_4\n",
      "1514. historical_volatility_53_lag_1\n",
      "1515. historical_volatility_53_lag_2\n",
      "1516. historical_volatility_53_lag_3\n",
      "1517. historical_volatility_53_lag_4\n",
      "1518. historical_volatility_54_lag_1\n",
      "1519. historical_volatility_54_lag_2\n",
      "1520. historical_volatility_54_lag_3\n",
      "1521. historical_volatility_54_lag_4\n",
      "1522. historical_volatility_55_lag_1\n",
      "1523. historical_volatility_55_lag_2\n",
      "1524. historical_volatility_55_lag_3\n",
      "1525. historical_volatility_55_lag_4\n",
      "1526. historical_volatility_56_lag_1\n",
      "1527. historical_volatility_56_lag_2\n",
      "1528. historical_volatility_56_lag_3\n",
      "1529. historical_volatility_56_lag_4\n",
      "1530. historical_volatility_57_lag_1\n",
      "1531. historical_volatility_57_lag_2\n",
      "1532. historical_volatility_57_lag_3\n",
      "1533. historical_volatility_57_lag_4\n",
      "1534. historical_volatility_58_lag_1\n",
      "1535. historical_volatility_58_lag_2\n",
      "1536. historical_volatility_58_lag_3\n",
      "1537. historical_volatility_58_lag_4\n",
      "1538. historical_volatility_59_lag_1\n",
      "1539. historical_volatility_59_lag_2\n",
      "1540. historical_volatility_59_lag_3\n",
      "1541. historical_volatility_59_lag_4\n",
      "1542. ATR_4\n",
      "1543. ATR_5\n",
      "1544. ATR_6\n",
      "1545. ATR_7\n",
      "1546. ATR_8\n",
      "1547. ATR_9\n",
      "1548. ATR_10\n",
      "1549. ATR_11\n",
      "1550. ATR_12\n",
      "1551. ATR_13\n",
      "1552. ATR_14\n",
      "1553. ATR_15\n",
      "1554. ATR_16\n",
      "1555. ATR_17\n",
      "1556. ATR_18\n",
      "1557. ATR_19\n",
      "1558. ATR_20\n",
      "1559. ATR_21\n",
      "1560. ATR_22\n",
      "1561. ATR_23\n",
      "1562. ATR_24\n",
      "1563. ATR_25\n",
      "1564. ATR_26\n",
      "1565. ATR_27\n",
      "1566. ATR_28\n",
      "1567. ATR_29\n",
      "1568. ATR_30\n",
      "1569. ATR_31\n",
      "1570. ATR_32\n",
      "1571. ATR_33\n",
      "1572. ATR_34\n",
      "1573. ATR_35\n",
      "1574. ATR_36\n",
      "1575. ATR_37\n",
      "1576. ATR_38\n",
      "1577. ATR_39\n",
      "1578. ATR_40\n",
      "1579. ATR_41\n",
      "1580. ATR_42\n",
      "1581. ATR_43\n",
      "1582. ATR_44\n",
      "1583. ATR_45\n",
      "1584. ATR_46\n",
      "1585. ATR_47\n",
      "1586. ATR_48\n",
      "1587. ATR_49\n",
      "1588. ATR_50\n",
      "1589. ATR_51\n",
      "1590. ATR_52\n",
      "1591. ATR_53\n",
      "1592. ATR_54\n",
      "1593. ATR_55\n",
      "1594. ATR_56\n",
      "1595. ATR_57\n",
      "1596. ATR_58\n",
      "1597. ATR_59\n",
      "1598. ATR_4_lag_1\n",
      "1599. ATR_4_lag_2\n",
      "1600. ATR_4_lag_3\n",
      "1601. ATR_4_lag_4\n",
      "1602. ATR_5_lag_1\n",
      "1603. ATR_5_lag_2\n",
      "1604. ATR_5_lag_3\n",
      "1605. ATR_5_lag_4\n",
      "1606. ATR_6_lag_1\n",
      "1607. ATR_6_lag_2\n",
      "1608. ATR_6_lag_3\n",
      "1609. ATR_6_lag_4\n",
      "1610. ATR_7_lag_1\n",
      "1611. ATR_7_lag_2\n",
      "1612. ATR_7_lag_3\n",
      "1613. ATR_7_lag_4\n",
      "1614. ATR_8_lag_1\n",
      "1615. ATR_8_lag_2\n",
      "1616. ATR_8_lag_3\n",
      "1617. ATR_8_lag_4\n",
      "1618. ATR_9_lag_1\n",
      "1619. ATR_9_lag_2\n",
      "1620. ATR_9_lag_3\n",
      "1621. ATR_9_lag_4\n",
      "1622. ATR_10_lag_1\n",
      "1623. ATR_10_lag_2\n",
      "1624. ATR_10_lag_3\n",
      "1625. ATR_10_lag_4\n",
      "1626. ATR_11_lag_1\n",
      "1627. ATR_11_lag_2\n",
      "1628. ATR_11_lag_3\n",
      "1629. ATR_11_lag_4\n",
      "1630. ATR_12_lag_1\n",
      "1631. ATR_12_lag_2\n",
      "1632. ATR_12_lag_3\n",
      "1633. ATR_12_lag_4\n",
      "1634. ATR_13_lag_1\n",
      "1635. ATR_13_lag_2\n",
      "1636. ATR_13_lag_3\n",
      "1637. ATR_13_lag_4\n",
      "1638. ATR_14_lag_1\n",
      "1639. ATR_14_lag_2\n",
      "1640. ATR_14_lag_3\n",
      "1641. ATR_14_lag_4\n",
      "1642. ATR_15_lag_1\n",
      "1643. ATR_15_lag_2\n",
      "1644. ATR_15_lag_3\n",
      "1645. ATR_15_lag_4\n",
      "1646. ATR_16_lag_1\n",
      "1647. ATR_16_lag_2\n",
      "1648. ATR_16_lag_3\n",
      "1649. ATR_16_lag_4\n",
      "1650. ATR_17_lag_1\n",
      "1651. ATR_17_lag_2\n",
      "1652. ATR_17_lag_3\n",
      "1653. ATR_17_lag_4\n",
      "1654. ATR_18_lag_1\n",
      "1655. ATR_18_lag_2\n",
      "1656. ATR_18_lag_3\n",
      "1657. ATR_18_lag_4\n",
      "1658. ATR_19_lag_1\n",
      "1659. ATR_19_lag_2\n",
      "1660. ATR_19_lag_3\n",
      "1661. ATR_19_lag_4\n",
      "1662. ATR_20_lag_1\n",
      "1663. ATR_20_lag_2\n",
      "1664. ATR_20_lag_3\n",
      "1665. ATR_20_lag_4\n",
      "1666. ATR_21_lag_1\n",
      "1667. ATR_21_lag_2\n",
      "1668. ATR_21_lag_3\n",
      "1669. ATR_21_lag_4\n",
      "1670. ATR_22_lag_1\n",
      "1671. ATR_22_lag_2\n",
      "1672. ATR_22_lag_3\n",
      "1673. ATR_22_lag_4\n",
      "1674. ATR_23_lag_1\n",
      "1675. ATR_23_lag_2\n",
      "1676. ATR_23_lag_3\n",
      "1677. ATR_23_lag_4\n",
      "1678. ATR_24_lag_1\n",
      "1679. ATR_24_lag_2\n",
      "1680. ATR_24_lag_3\n",
      "1681. ATR_24_lag_4\n",
      "1682. ATR_25_lag_1\n",
      "1683. ATR_25_lag_2\n",
      "1684. ATR_25_lag_3\n",
      "1685. ATR_25_lag_4\n",
      "1686. ATR_26_lag_1\n",
      "1687. ATR_26_lag_2\n",
      "1688. ATR_26_lag_3\n",
      "1689. ATR_26_lag_4\n",
      "1690. ATR_27_lag_1\n",
      "1691. ATR_27_lag_2\n",
      "1692. ATR_27_lag_3\n",
      "1693. ATR_27_lag_4\n",
      "1694. ATR_28_lag_1\n",
      "1695. ATR_28_lag_2\n",
      "1696. ATR_28_lag_3\n",
      "1697. ATR_28_lag_4\n",
      "1698. ATR_29_lag_1\n",
      "1699. ATR_29_lag_2\n",
      "1700. ATR_29_lag_3\n",
      "1701. ATR_29_lag_4\n",
      "1702. ATR_30_lag_1\n",
      "1703. ATR_30_lag_2\n",
      "1704. ATR_30_lag_3\n",
      "1705. ATR_30_lag_4\n",
      "1706. ATR_31_lag_1\n",
      "1707. ATR_31_lag_2\n",
      "1708. ATR_31_lag_3\n",
      "1709. ATR_31_lag_4\n",
      "1710. ATR_32_lag_1\n",
      "1711. ATR_32_lag_2\n",
      "1712. ATR_32_lag_3\n",
      "1713. ATR_32_lag_4\n",
      "1714. ATR_33_lag_1\n",
      "1715. ATR_33_lag_2\n",
      "1716. ATR_33_lag_3\n",
      "1717. ATR_33_lag_4\n",
      "1718. ATR_34_lag_1\n",
      "1719. ATR_34_lag_2\n",
      "1720. ATR_34_lag_3\n",
      "1721. ATR_34_lag_4\n",
      "1722. ATR_35_lag_1\n",
      "1723. ATR_35_lag_2\n",
      "1724. ATR_35_lag_3\n",
      "1725. ATR_35_lag_4\n",
      "1726. ATR_36_lag_1\n",
      "1727. ATR_36_lag_2\n",
      "1728. ATR_36_lag_3\n",
      "1729. ATR_36_lag_4\n",
      "1730. ATR_37_lag_1\n",
      "1731. ATR_37_lag_2\n",
      "1732. ATR_37_lag_3\n",
      "1733. ATR_37_lag_4\n",
      "1734. ATR_38_lag_1\n",
      "1735. ATR_38_lag_2\n",
      "1736. ATR_38_lag_3\n",
      "1737. ATR_38_lag_4\n",
      "1738. ATR_39_lag_1\n",
      "1739. ATR_39_lag_2\n",
      "1740. ATR_39_lag_3\n",
      "1741. ATR_39_lag_4\n",
      "1742. ATR_40_lag_1\n",
      "1743. ATR_40_lag_2\n",
      "1744. ATR_40_lag_3\n",
      "1745. ATR_40_lag_4\n",
      "1746. ATR_41_lag_1\n",
      "1747. ATR_41_lag_2\n",
      "1748. ATR_41_lag_3\n",
      "1749. ATR_41_lag_4\n",
      "1750. ATR_42_lag_1\n",
      "1751. ATR_42_lag_2\n",
      "1752. ATR_42_lag_3\n",
      "1753. ATR_42_lag_4\n",
      "1754. ATR_43_lag_1\n",
      "1755. ATR_43_lag_2\n",
      "1756. ATR_43_lag_3\n",
      "1757. ATR_43_lag_4\n",
      "1758. ATR_44_lag_1\n",
      "1759. ATR_44_lag_2\n",
      "1760. ATR_44_lag_3\n",
      "1761. ATR_44_lag_4\n",
      "1762. ATR_45_lag_1\n",
      "1763. ATR_45_lag_2\n",
      "1764. ATR_45_lag_3\n",
      "1765. ATR_45_lag_4\n",
      "1766. ATR_46_lag_1\n",
      "1767. ATR_46_lag_2\n",
      "1768. ATR_46_lag_3\n",
      "1769. ATR_46_lag_4\n",
      "1770. ATR_47_lag_1\n",
      "1771. ATR_47_lag_2\n",
      "1772. ATR_47_lag_3\n",
      "1773. ATR_47_lag_4\n",
      "1774. ATR_48_lag_1\n",
      "1775. ATR_48_lag_2\n",
      "1776. ATR_48_lag_3\n",
      "1777. ATR_48_lag_4\n",
      "1778. ATR_49_lag_1\n",
      "1779. ATR_49_lag_2\n",
      "1780. ATR_49_lag_3\n",
      "1781. ATR_49_lag_4\n",
      "1782. ATR_50_lag_1\n",
      "1783. ATR_50_lag_2\n",
      "1784. ATR_50_lag_3\n",
      "1785. ATR_50_lag_4\n",
      "1786. ATR_51_lag_1\n",
      "1787. ATR_51_lag_2\n",
      "1788. ATR_51_lag_3\n",
      "1789. ATR_51_lag_4\n",
      "1790. ATR_52_lag_1\n",
      "1791. ATR_52_lag_2\n",
      "1792. ATR_52_lag_3\n",
      "1793. ATR_52_lag_4\n",
      "1794. ATR_53_lag_1\n",
      "1795. ATR_53_lag_2\n",
      "1796. ATR_53_lag_3\n",
      "1797. ATR_53_lag_4\n",
      "1798. ATR_54_lag_1\n",
      "1799. ATR_54_lag_2\n",
      "1800. ATR_54_lag_3\n",
      "1801. ATR_54_lag_4\n",
      "1802. ATR_55_lag_1\n",
      "1803. ATR_55_lag_2\n",
      "1804. ATR_55_lag_3\n",
      "1805. ATR_55_lag_4\n",
      "1806. ATR_56_lag_1\n",
      "1807. ATR_56_lag_2\n",
      "1808. ATR_56_lag_3\n",
      "1809. ATR_56_lag_4\n",
      "1810. ATR_57_lag_1\n",
      "1811. ATR_57_lag_2\n",
      "1812. ATR_57_lag_3\n",
      "1813. ATR_57_lag_4\n",
      "1814. ATR_58_lag_1\n",
      "1815. ATR_58_lag_2\n",
      "1816. ATR_58_lag_3\n",
      "1817. ATR_58_lag_4\n",
      "1818. ATR_59_lag_1\n",
      "1819. ATR_59_lag_2\n",
      "1820. ATR_59_lag_3\n",
      "1821. ATR_59_lag_4\n",
      "1822. dist_to_lower_10\n",
      "1823. dist_to_lower_100\n",
      "1824. dist_to_lower_1000\n",
      "1825. dist_to_lower_10000\n",
      "1826. dist_to_upper_10\n",
      "1827. dist_to_upper_100\n",
      "1828. dist_to_upper_1000\n",
      "1829. dist_to_upper_10000\n",
      "1830. candle_number\n",
      "1831. macd_line\n",
      "1832. signal_line\n",
      "1833. macd_histogram\n",
      "1834. macd_line_lag_1\n",
      "1835. signal_line_lag_1\n",
      "1836. macd_histogram_lag_1\n",
      "1837. macd_line_lag_2\n",
      "1838. signal_line_lag_2\n",
      "1839. macd_histogram_lag_2\n",
      "1840. macd_line_lag_3\n",
      "1841. signal_line_lag_3\n",
      "1842. macd_histogram_lag_3\n",
      "1843. macd_signal_crossover\n",
      "1844. MACD_Distance_Zero\n",
      "1845. MACD_Above_Zero\n",
      "1846. MACD_Trend_Zero\n",
      "\n",
      "Total number of columns: 1846\n"
     ]
    }
   ],
   "source": [
    "# List all column names\n",
    "column_list = df_scaled.columns.tolist()\n",
    "\n",
    "# Print in a formatted way\n",
    "for i, column in enumerate(column_list, 1):\n",
    "    print(f\"{i}. {column}\")\n",
    "\n",
    "# Or print total count\n",
    "print(f\"\\nTotal number of columns: {len(column_list)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "79b6c762-3c2d-4c24-a69b-2389a9b823de",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-01T05:06:15.574385Z",
     "iopub.status.busy": "2024-11-01T05:06:15.574212Z",
     "iopub.status.idle": "2024-11-01T05:06:34.323640Z",
     "shell.execute_reply": "2024-11-01T05:06:34.323205Z",
     "shell.execute_reply.started": "2024-11-01T05:06:15.574370Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set shape: (106797, 1846)\n",
      "Validation set shape: (35599, 1846)\n",
      "Test set shape: (35600, 1846)\n",
      "Training T set shape: (106797, 1)\n",
      "Validation T set shape: (35599, 1)\n",
      "Test T set shape: (35600, 1)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "# Split original dataset into train+validation and test sets (80% train+validation, 20% test)\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(\n",
    "    df_scaled, target_df, test_size=0.2, random_state=42, shuffle=False\n",
    ")\n",
    "\n",
    "# Split train+validation into actual train and validation sets (75% train, 25% validation)\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train_val, y_train_val, test_size=0.25, random_state=42, shuffle=False\n",
    ")\n",
    "\n",
    "# Now you have:\n",
    "# X_train, y_train -> Training data\n",
    "# X_val, y_val -> Validation data\n",
    "# X_test, y_test -> Test data\n",
    "\n",
    "# Check the resulting shapes of the datasets\n",
    "print(f\"Training set shape: {X_train.shape}\")\n",
    "print(f\"Validation set shape: {X_val.shape}\")\n",
    "print(f\"Test set shape: {X_test.shape}\")\n",
    "\n",
    "print(f\"Training T set shape: {y_train.shape}\")\n",
    "print(f\"Validation T set shape: {y_val.shape}\")\n",
    "print(f\"Test T set shape: {y_test.shape}\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "00b7cff9-045e-4fc9-a340-bb19899a4cf7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-01T05:06:34.324894Z",
     "iopub.status.busy": "2024-11-01T05:06:34.324735Z",
     "iopub.status.idle": "2024-11-01T05:06:34.332964Z",
     "shell.execute_reply": "2024-11-01T05:06:34.332212Z",
     "shell.execute_reply.started": "2024-11-01T05:06:34.324880Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106792</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106793</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106794</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106795</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106796</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>106797 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        target\n",
       "0            0\n",
       "1            0\n",
       "2            0\n",
       "3            0\n",
       "4            0\n",
       "...        ...\n",
       "106792       1\n",
       "106793       1\n",
       "106794       0\n",
       "106795       0\n",
       "106796       0\n",
       "\n",
       "[106797 rows x 1 columns]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8e928918-e463-440b-8e33-2946e4be0863",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-01T05:06:34.334399Z",
     "iopub.status.busy": "2024-11-01T05:06:34.334205Z",
     "iopub.status.idle": "2024-11-01T05:06:38.348577Z",
     "shell.execute_reply": "2024-11-01T05:06:38.348083Z",
     "shell.execute_reply.started": "2024-11-01T05:06:34.334388Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting imblearn\n",
      "  Downloading imblearn-0.0-py2.py3-none-any.whl.metadata (355 bytes)\n",
      "Collecting imbalanced-learn (from imblearn)\n",
      "  Downloading imbalanced_learn-0.12.4-py3-none-any.whl.metadata (8.3 kB)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.11/dist-packages (from imbalanced-learn->imblearn) (1.26.3)\n",
      "Requirement already satisfied: scipy>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from imbalanced-learn->imblearn) (1.11.2)\n",
      "Requirement already satisfied: scikit-learn>=1.0.2 in /usr/local/lib/python3.11/dist-packages (from imbalanced-learn->imblearn) (1.3.0)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from imbalanced-learn->imblearn) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from imbalanced-learn->imblearn) (3.2.0)\n",
      "Downloading imblearn-0.0-py2.py3-none-any.whl (1.9 kB)\n",
      "Downloading imbalanced_learn-0.12.4-py3-none-any.whl (258 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m258.3/258.3 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: imbalanced-learn, imblearn\n",
      "Successfully installed imbalanced-learn-0.12.4 imblearn-0.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install imblearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d0c88e5e-e34b-4c32-9b89-1a79defb2cb5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-01T05:20:01.890147Z",
     "iopub.status.busy": "2024-11-01T05:20:01.888070Z",
     "iopub.status.idle": "2024-11-01T05:20:02.282543Z",
     "shell.execute_reply": "2024-11-01T05:20:02.281955Z",
     "shell.execute_reply.started": "2024-11-01T05:20:01.890122Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam, SGD, RMSprop\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from tensorflow.keras import metrics\n",
    "from tensorflow.keras.optimizers import AdamW\n",
    "from tensorflow.keras.utils import Sequence\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "# Add these imports at the top of your code\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from tensorflow.keras.utils import Sequence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a5b768c-ab66-4df1-8bdb-742594c1a953",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras.utils import Sequence\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "\n",
    "class MonitoredTimeSeriesGenerator(Sequence):\n",
    "    def __init__(\n",
    "        self, \n",
    "        data, \n",
    "        targets, \n",
    "        batch_size,\n",
    "        time_steps,\n",
    "        n_features,\n",
    "        mode='train',\n",
    "        augmentation_probability=0.5,\n",
    "        noise_scale=0.01,\n",
    "        scale_range=(0.95, 1.05),\n",
    "        magnitude_range=(0.9, 1.1),\n",
    "        time_shift_range=(-2, 2),\n",
    "        window_slice_probability=0.3,\n",
    "        window_warp_probability=0.3,\n",
    "        window_warp_scale_range=(0.8, 1.2),\n",
    "        name='default',\n",
    "        standardize=True,\n",
    "        monitor_augmentations=True\n",
    "    ):\n",
    "        self.name = name\n",
    "        self.monitor_augmentations = monitor_augmentations\n",
    "        self.augmentation_stats = defaultdict(int)\n",
    "        self.sample_augmentations = []\n",
    "        \n",
    "        # Basic initialization\n",
    "        self.data = data.values if isinstance(data, pd.DataFrame) else data\n",
    "        self.targets = targets.reset_index(drop=True).values if isinstance(targets, pd.DataFrame) or isinstance(targets, pd.Series) else targets\n",
    "        self.time_steps = time_steps\n",
    "        self.n_features = n_features\n",
    "        self.mode = mode\n",
    "        \n",
    "        # Standardize if requested\n",
    "        if standardize:\n",
    "            self.scaler = StandardScaler()\n",
    "            self.data = self.scaler.fit_transform(self.data)\n",
    "        \n",
    "        # Augmentation parameters\n",
    "        self.augmentation_probability = augmentation_probability\n",
    "        self.noise_scale = noise_scale\n",
    "        self.scale_range = scale_range\n",
    "        self.magnitude_range = magnitude_range\n",
    "        self.time_shift_range = time_shift_range\n",
    "        self.window_slice_probability = window_slice_probability\n",
    "        self.window_warp_probability = window_warp_probability\n",
    "        self.window_warp_scale_range = window_warp_scale_range\n",
    "        \n",
    "        # Get indices for each class\n",
    "        self.pos_indices = np.where(self.targets == 1)[0]\n",
    "        self.neg_indices = np.where(self.targets == 0)[0]\n",
    "        \n",
    "        # Calculate class weights\n",
    "        n_samples = len(self.targets)\n",
    "        self.class_weights = {\n",
    "            0: n_samples / (2 * len(self.neg_indices)),\n",
    "            1: n_samples / (2 * len(self.pos_indices))\n",
    "        }\n",
    "        \n",
    "        # Batch size and sampling setup\n",
    "        self.batch_size = batch_size - (batch_size % 2) if mode == 'train' else batch_size\n",
    "        \n",
    "        # Calculate balanced sampling for training\n",
    "        if mode == 'train':\n",
    "            self.pos_samples_per_batch = max(self.batch_size // 3, 1)  # At least 1/3 positive samples\n",
    "        else:\n",
    "            ratio = len(self.pos_indices) / len(self.targets)\n",
    "            self.pos_samples_per_batch = max(1, int(round(self.batch_size * ratio)))\n",
    "        \n",
    "        self.neg_samples_per_batch = self.batch_size - self.pos_samples_per_batch\n",
    "        \n",
    "        # Calculate steps\n",
    "        self.steps = min(\n",
    "            len(self.pos_indices) // self.pos_samples_per_batch,\n",
    "            len(self.neg_indices) // self.neg_samples_per_batch\n",
    "        )\n",
    "        \n",
    "        self._log_initialization()\n",
    "\n",
    "    def _window_slice(self, sequence, track=True):\n",
    "        \"\"\"Randomly slice and resize a window of the sequence\"\"\"\n",
    "        if np.random.random() > self.window_slice_probability:\n",
    "            return sequence\n",
    "            \n",
    "        # Choose window size between 1/2 and full length\n",
    "        window_size = np.random.randint(self.time_steps // 2, self.time_steps)\n",
    "        # Choose random start point\n",
    "        start_idx = np.random.randint(0, self.time_steps - window_size)\n",
    "        \n",
    "        # Extract window\n",
    "        window = sequence[start_idx:start_idx + window_size].copy()\n",
    "        \n",
    "        # Resize window to original length\n",
    "        augmented = np.zeros_like(sequence)\n",
    "        # Use linear interpolation to resize\n",
    "        for feature in range(sequence.shape[-1]):\n",
    "            augmented[:, feature] = np.interp(\n",
    "                np.linspace(0, window_size - 1, self.time_steps),\n",
    "                np.arange(window_size),\n",
    "                window[:, feature]\n",
    "            )\n",
    "        \n",
    "        if track:\n",
    "            self.augmentation_stats['window_slice'] += 1\n",
    "        \n",
    "        return augmented\n",
    "\n",
    "    def _window_warp(self, sequence, track=True):\n",
    "        \"\"\"Apply warping to a random window of the sequence\"\"\"\n",
    "        if np.random.random() > self.window_warp_probability:\n",
    "            return sequence\n",
    "            \n",
    "        # Choose window size (1/4 to 1/2 of sequence)\n",
    "        window_size = np.random.randint(self.time_steps // 4, self.time_steps // 2)\n",
    "        # Choose random start point\n",
    "        window_start = np.random.randint(0, self.time_steps - window_size)\n",
    "        \n",
    "        augmented = sequence.copy()\n",
    "        # Get random warping scale\n",
    "        warp_scale = np.random.uniform(\n",
    "            self.window_warp_scale_range[0],\n",
    "            self.window_warp_scale_range[1]\n",
    "        )\n",
    "        \n",
    "        # Apply warping to each feature\n",
    "        for feature in range(sequence.shape[-1]):\n",
    "            window = sequence[window_start:window_start + window_size, feature]\n",
    "            \n",
    "            # Create warped window\n",
    "            num_warped_points = int(window_size * warp_scale)\n",
    "            warped_points = np.linspace(0, window_size - 1, num_warped_points)\n",
    "            warped_window = np.interp(\n",
    "                warped_points,\n",
    "                np.arange(window_size),\n",
    "                window\n",
    "            )\n",
    "            \n",
    "            # Resize back to original window size\n",
    "            warped_window = np.interp(\n",
    "                np.arange(window_size),\n",
    "                np.linspace(0, window_size - 1, len(warped_window)),\n",
    "                warped_window\n",
    "            )\n",
    "            \n",
    "            augmented[window_start:window_start + window_size, feature] = warped_window\n",
    "        \n",
    "        if track:\n",
    "            self.augmentation_stats['window_warp'] += 1\n",
    "        \n",
    "        return augmented\n",
    "\n",
    "    def _augment_sequence(self, sequence, track=True):\n",
    "        \"\"\"Apply all augmentation techniques\"\"\"\n",
    "        if np.random.random() > self.augmentation_probability:\n",
    "            if track:\n",
    "                self.augmentation_stats['no_augmentation'] += 1\n",
    "            return sequence\n",
    "\n",
    "        augmented = sequence.copy()\n",
    "        original = sequence.copy()\n",
    "        augmentations_applied = []\n",
    "\n",
    "        # Apply window operations first\n",
    "        if np.random.random() > 0.5:\n",
    "            augmented = self._window_slice(augmented, track)\n",
    "            if track:\n",
    "                augmentations_applied.append('window_slice')\n",
    "                \n",
    "        if np.random.random() > 0.5:\n",
    "            augmented = self._window_warp(augmented, track)\n",
    "            if track:\n",
    "                augmentations_applied.append('window_warp')\n",
    "\n",
    "        # Apply other augmentations\n",
    "        if np.random.random() > 0.5:\n",
    "            noise = np.random.normal(0, self.noise_scale, sequence.shape)\n",
    "            augmented += noise\n",
    "            if track:\n",
    "                self.augmentation_stats['noise'] += 1\n",
    "                augmentations_applied.append('noise')\n",
    "\n",
    "        if np.random.random() > 0.5:\n",
    "            scale_factor = np.random.uniform(self.scale_range[0], self.scale_range[1])\n",
    "            augmented *= scale_factor\n",
    "            if track:\n",
    "                self.augmentation_stats['scaling'] += 1\n",
    "                augmentations_applied.append('scaling')\n",
    "\n",
    "        if np.random.random() > 0.5:\n",
    "            shift = np.random.randint(self.time_shift_range[0], self.time_shift_range[1])\n",
    "            augmented = np.roll(augmented, shift, axis=0)\n",
    "            if track:\n",
    "                self.augmentation_stats['time_shift'] += 1\n",
    "                augmentations_applied.append('time_shift')\n",
    "\n",
    "        if track and self.monitor_augmentations and len(augmentations_applied) > 0:\n",
    "            self.sample_augmentations.append({\n",
    "                'original': original,\n",
    "                'augmented': augmented,\n",
    "                'techniques': augmentations_applied\n",
    "            })\n",
    "\n",
    "        return augmented\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        batch_x = np.zeros((self.batch_size, self.time_steps, self.n_features))\n",
    "        batch_y = np.zeros(self.batch_size)\n",
    "        \n",
    "        if self.mode == 'train':\n",
    "            # Random sampling for training\n",
    "            pos_indices = np.random.choice(\n",
    "                self.pos_indices,\n",
    "                size=self.pos_samples_per_batch,\n",
    "                replace=True\n",
    "            )\n",
    "            neg_indices = np.random.choice(\n",
    "                self.neg_indices,\n",
    "                size=self.neg_samples_per_batch,\n",
    "                replace=True\n",
    "            )\n",
    "            \n",
    "            # Fill and augment positive samples\n",
    "            for i, start_idx in enumerate(pos_indices):\n",
    "                if start_idx + self.time_steps <= len(self.data):\n",
    "                    sequence = self.data[start_idx:start_idx + self.time_steps]\n",
    "                    batch_x[i] = self._augment_sequence(sequence)\n",
    "                    batch_y[i] = 1\n",
    "\n",
    "            # Fill and augment negative samples\n",
    "            for i, start_idx in enumerate(neg_indices):\n",
    "                if start_idx + self.time_steps <= len(self.data):\n",
    "                    sequence = self.data[start_idx:start_idx + self.time_steps]\n",
    "                    batch_x[i + self.pos_samples_per_batch] = self._augment_sequence(sequence)\n",
    "                    batch_y[i + self.pos_samples_per_batch] = 0\n",
    "\n",
    "        else:  # Validation/Test mode\n",
    "            # Sequential sampling\n",
    "            pos_start = (idx * self.pos_samples_per_batch) % len(self.pos_indices)\n",
    "            neg_start = (idx * self.neg_samples_per_batch) % len(self.neg_indices)\n",
    "            \n",
    "            pos_indices = self.pos_indices[pos_start:pos_start + self.pos_samples_per_batch]\n",
    "            neg_indices = self.neg_indices[neg_start:neg_start + self.neg_samples_per_batch]\n",
    "            \n",
    "            # Fill without augmentation\n",
    "            for i, start_idx in enumerate(pos_indices):\n",
    "                if start_idx + self.time_steps <= len(self.data):\n",
    "                    batch_x[i] = self.data[start_idx:start_idx + self.time_steps]\n",
    "                    batch_y[i] = 1\n",
    "            \n",
    "            for i, start_idx in enumerate(neg_indices):\n",
    "                if start_idx + self.time_steps <= len(self.data):\n",
    "                    batch_x[i + self.pos_samples_per_batch] = self.data[start_idx:start_idx + self.time_steps]\n",
    "                    batch_y[i + self.pos_samples_per_batch] = 0\n",
    "\n",
    "        return batch_x, batch_y\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.steps\n",
    "\n",
    "    def get_class_weights(self):\n",
    "        \"\"\"Return class weights\"\"\"\n",
    "        return self.class_weights\n",
    "\n",
    "    def visualize_augmentations(self, n_samples=5):\n",
    "        \"\"\"Visualize sample augmentations\"\"\"\n",
    "        if not self.sample_augmentations:\n",
    "            print(\"No augmentations to visualize yet.\")\n",
    "            return\n",
    "        \n",
    "        samples = self.sample_augmentations[:n_samples]\n",
    "        fig, axes = plt.subplots(n_samples, 2, figsize=(12, 4*n_samples))\n",
    "        \n",
    "        for i, sample in enumerate(samples):\n",
    "            # Plot original\n",
    "            axes[i, 0].plot(sample['original'])\n",
    "            axes[i, 0].set_title(f'Original Sequence {i+1}')\n",
    "            \n",
    "            # Plot augmented\n",
    "            axes[i, 1].plot(sample['augmented'])\n",
    "            axes[i, 1].set_title(f'Augmented ({\"->\".join(sample[\"techniques\"])})')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        return fig\n",
    "\n",
    "    def get_augmentation_stats(self):\n",
    "        \"\"\"Get statistics about applied augmentations\"\"\"\n",
    "        total = sum(self.augmentation_stats.values())\n",
    "        if total == 0:\n",
    "            return \"No augmentations tracked yet.\"\n",
    "        \n",
    "        stats = {k: f\"{v/total*100:.1f}%\" for k, v in self.augmentation_stats.items()}\n",
    "        return {\n",
    "            'total_samples': total,\n",
    "            'augmentation_distribution': stats,\n",
    "            'parameters': {\n",
    "                'probability': self.augmentation_probability,\n",
    "                'noise_scale': self.noise_scale,\n",
    "                'scale_range': self.scale_range,\n",
    "                'magnitude_range': self.magnitude_range,\n",
    "                'time_shift_range': self.time_shift_range,\n",
    "                'window_slice_probability': self.window_slice_probability,\n",
    "                'window_warp_probability': self.window_warp_probability,\n",
    "                'window_warp_scale_range': self.window_warp_scale_range\n",
    "            }\n",
    "        }\n",
    "\n",
    "    def _log_initialization(self):\n",
    "        \"\"\"Log initialization details\"\"\"\n",
    "        print(f\"\\n{self.name} Generator Setup ({self.mode}):\")\n",
    "        print(f\"Total samples: {len(self.targets)}\")\n",
    "        print(f\"Positive samples: {len(self.pos_indices)} ({len(self.pos_indices)/len(self.targets):.3f})\")\n",
    "        print(f\"Batch size: {self.batch_size} (Pos: {self.pos_samples_per_batch}, Neg: {self.neg_samples_per_batch})\")\n",
    "        print(f\"Steps per epoch: {self.steps}\")\n",
    "        print(f\"Class weights: {self.class_weights}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da09d9cb-2e5f-472a-839f-544bd9c9de9e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eac3280c-49e3-4426-acef-db86ded5f064",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bcecb75-2565-4354-8348-c6db68071671",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Default training generator (balanced augmentation)\n",
    "train_generator = MonitoredTimeSeriesGenerator(\n",
    "    data=X_train,\n",
    "    targets=y_train['target'],\n",
    "    batch_size=32,\n",
    "    time_steps=30,\n",
    "    n_features=X_train.shape[1],\n",
    "    mode='train',\n",
    "    name='default_train',\n",
    "    # Default balanced settings\n",
    "    augmentation_probability=0.5,\n",
    "    noise_scale=0.01,\n",
    "    scale_range=(0.95, 1.05),\n",
    "    magnitude_range=(0.9, 1.1),\n",
    "    time_shift_range=(-2, 2),\n",
    "    window_slice_probability=0.3,\n",
    "    window_warp_probability=0.3,\n",
    "    window_warp_scale_range=(0.8, 1.2)\n",
    ")\n",
    "\n",
    "# 2. Validation generator (no augmentation)\n",
    "val_generator = MonitoredTimeSeriesGenerator(\n",
    "    data=X_val,\n",
    "    targets=y_val['target'],\n",
    "    batch_size=32,\n",
    "    time_steps=30,\n",
    "    n_features=X_train.shape[1],\n",
    "    mode='val',\n",
    "    name='validation',\n",
    "    monitor_augmentations=False\n",
    ")\n",
    "\n",
    "# 3. Test generator (no augmentation)\n",
    "test_generator = MonitoredTimeSeriesGenerator(\n",
    "    data=X_test,\n",
    "    targets=y_test['target'],\n",
    "    batch_size=32,\n",
    "    time_steps=30,\n",
    "    n_features=X_train.shape[1],\n",
    "    mode='test',\n",
    "    name='test',\n",
    "    monitor_augmentations=False\n",
    ")\n",
    "\n",
    "# 4. Aggressive augmentation generator\n",
    "train_generator_aggressive = MonitoredTimeSeriesGenerator(\n",
    "    data=X_train,\n",
    "    targets=y_train['target'],\n",
    "    batch_size=32,\n",
    "    time_steps=30,\n",
    "    n_features=X_train.shape[1],\n",
    "    mode='train',\n",
    "    name='aggressive_train',\n",
    "    # Aggressive settings\n",
    "    augmentation_probability=0.7,    # Higher probability\n",
    "    noise_scale=0.02,               # More noise\n",
    "    scale_range=(0.9, 1.1),        # Wider scaling\n",
    "    magnitude_range=(0.8, 1.2),     # Wider magnitude\n",
    "    time_shift_range=(-3, 3),       # Larger shifts\n",
    "    window_slice_probability=0.5,    # More window slicing\n",
    "    window_warp_probability=0.5,     # More window warping\n",
    "    window_warp_scale_range=(0.7, 1.3)  # More aggressive warping\n",
    ")\n",
    "\n",
    "# 5. Mild augmentation generator\n",
    "train_generator_mild = MonitoredTimeSeriesGenerator(\n",
    "    data=X_train,\n",
    "    targets=y_train['target'],\n",
    "    batch_size=32,\n",
    "    time_steps=30,\n",
    "    n_features=X_train.shape[1],\n",
    "    mode='train',\n",
    "    name='mild_train',\n",
    "    # Mild settings\n",
    "    augmentation_probability=0.3,    # Lower probability\n",
    "    noise_scale=0.005,              # Less noise\n",
    "    scale_range=(0.98, 1.02),       # Narrower scaling\n",
    "    magnitude_range=(0.95, 1.05),   # Narrower magnitude\n",
    "    time_shift_range=(-1, 1),       # Smaller shifts\n",
    "    window_slice_probability=0.2,    # Less window slicing\n",
    "    window_warp_probability=0.2,     # Less window warping\n",
    "    window_warp_scale_range=(0.9, 1.1)  # Milder warping\n",
    ")\n",
    "\n",
    "# Function to compare augmentation strategies\n",
    "def compare_augmentation_strategies():\n",
    "    generators = {\n",
    "        'Default': train_generator,\n",
    "        'Aggressive': train_generator_aggressive,\n",
    "        'Mild': train_generator_mild\n",
    "    }\n",
    "    \n",
    "    for name, gen in generators.items():\n",
    "        print(f\"\\n{name} Generator Augmentation Stats:\")\n",
    "        # Generate a few batches to collect statistics\n",
    "        for _ in range(3):\n",
    "            X_batch, y_batch = gen[0]\n",
    "        \n",
    "        stats = gen.get_augmentation_stats()\n",
    "        print(stats)\n",
    "        \n",
    "        # Visualize examples\n",
    "        print(f\"\\nVisualizing {name} augmentations:\")\n",
    "        gen.visualize_augmentations(n_samples=2)\n",
    "        plt.show()\n",
    "\n",
    "# Compare the different strategies\n",
    "compare_augmentation_strategies()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "4e653387-b735-4d30-a4a9-f1c7b22eaf40",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-31T13:50:19.682458Z",
     "iopub.status.busy": "2024-10-31T13:50:19.681786Z",
     "iopub.status.idle": "2024-10-31T13:50:19.866691Z",
     "shell.execute_reply": "2024-10-31T13:50:19.866307Z",
     "shell.execute_reply.started": "2024-10-31T13:50:19.682438Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generator Verification Report\n",
      "==================================================\n",
      "\n",
      "Regular Training Generator Analysis:\n",
      "------------------------------\n",
      "Batch shapes:\n",
      "X: (32, 30, 1846)\n",
      "y: (32,)\n",
      "\n",
      "Class distribution in batch:\n",
      "Class 0: 28 samples (87.50%)\n",
      "Class 1: 4 samples (12.50%)\n",
      "\n",
      "Batch statistics:\n",
      "X mean: 0.155\n",
      "X std: 0.211\n",
      "X range: [-0.037, 1.084]\n",
      "\n",
      "Augmentation test (5 samples):\n",
      "Sample 1 mean abs difference: 0.009\n",
      "Sample 2 mean abs difference: 0.000\n",
      "Sample 3 mean abs difference: 0.000\n",
      "Sample 4 mean abs difference: 0.012\n",
      "Sample 5 mean abs difference: 0.006\n",
      "\n",
      "Aggressive Aug Training Generator Analysis:\n",
      "------------------------------\n",
      "Batch shapes:\n",
      "X: (32, 30, 1846)\n",
      "y: (32,)\n",
      "\n",
      "Class distribution in batch:\n",
      "Class 0: 28 samples (87.50%)\n",
      "Class 1: 4 samples (12.50%)\n",
      "\n",
      "Batch statistics:\n",
      "X mean: 0.153\n",
      "X std: 0.210\n",
      "X range: [-0.084, 1.169]\n",
      "\n",
      "Augmentation test (5 samples):\n",
      "Sample 1 mean abs difference: 0.024\n",
      "Sample 2 mean abs difference: 0.000\n",
      "Sample 3 mean abs difference: 0.000\n",
      "Sample 4 mean abs difference: 0.000\n",
      "Sample 5 mean abs difference: 0.000\n",
      "\n",
      "Mild Aug Training Generator Analysis:\n",
      "------------------------------\n",
      "Batch shapes:\n",
      "X: (32, 30, 1846)\n",
      "y: (32,)\n",
      "\n",
      "Class distribution in batch:\n",
      "Class 0: 28 samples (87.50%)\n",
      "Class 1: 4 samples (12.50%)\n",
      "\n",
      "Batch statistics:\n",
      "X mean: 0.156\n",
      "X std: 0.213\n",
      "X range: [-0.022, 1.057]\n",
      "\n",
      "Augmentation test (5 samples):\n",
      "Sample 1 mean abs difference: 0.000\n",
      "Sample 2 mean abs difference: 0.000\n",
      "Sample 3 mean abs difference: 0.000\n",
      "Sample 4 mean abs difference: 0.000\n",
      "Sample 5 mean abs difference: 0.000\n",
      "\n",
      "Validation Generator Analysis:\n",
      "------------------------------\n",
      "Batch shapes:\n",
      "X: (32, 30, 1846)\n",
      "y: (32,)\n",
      "\n",
      "Class distribution in batch:\n",
      "Class 0: 27 samples (84.38%)\n",
      "Class 1: 5 samples (15.62%)\n",
      "\n",
      "Batch statistics:\n",
      "X mean: 0.236\n",
      "X std: 0.184\n",
      "X range: [0.000, 1.000]\n",
      "\n",
      "Test Generator Analysis:\n",
      "------------------------------\n",
      "Batch shapes:\n",
      "X: (32, 30, 1846)\n",
      "y: (32,)\n",
      "\n",
      "Class distribution in batch:\n",
      "Class 0: 29 samples (90.62%)\n",
      "Class 1: 3 samples (9.38%)\n",
      "\n",
      "Batch statistics:\n",
      "X mean: 0.340\n",
      "X std: 0.240\n",
      "X range: [0.000, 1.000]\n"
     ]
    }
   ],
   "source": [
    "def verify_generators(generators_dict):\n",
    "    \"\"\"\n",
    "    Verify behavior of multiple generators\n",
    "    \"\"\"\n",
    "    print(\"\\nGenerator Verification Report\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    for name, gen in generators_dict.items():\n",
    "        print(f\"\\n{name} Generator Analysis:\")\n",
    "        print(\"-\" * 30)\n",
    "        \n",
    "        # Get a batch\n",
    "        x_batch, y_batch = gen[0]\n",
    "        \n",
    "        # Basic shape information\n",
    "        print(f\"Batch shapes:\")\n",
    "        print(f\"X: {x_batch.shape}\")\n",
    "        print(f\"y: {y_batch.shape}\")\n",
    "        \n",
    "        # Class distribution in batch\n",
    "        unique, counts = np.unique(y_batch, return_counts=True)\n",
    "        print(\"\\nClass distribution in batch:\")\n",
    "        for val, count in zip(unique, counts):\n",
    "            print(f\"Class {int(val)}: {count} samples ({count/len(y_batch):.2%})\")\n",
    "        \n",
    "        # Data statistics\n",
    "        print(\"\\nBatch statistics:\")\n",
    "        print(f\"X mean: {x_batch.mean():.3f}\")\n",
    "        print(f\"X std: {x_batch.std():.3f}\")\n",
    "        print(f\"X range: [{x_batch.min():.3f}, {x_batch.max():.3f}]\")\n",
    "        \n",
    "        # If training mode, check augmentation\n",
    "        if gen.mode == 'train':\n",
    "            print(\"\\nAugmentation test (5 samples):\")\n",
    "            original = x_batch[0]\n",
    "            for i in range(5):\n",
    "                augmented = gen._augment_sequence(original)\n",
    "                diff = np.mean(np.abs(original - augmented))\n",
    "                print(f\"Sample {i+1} mean abs difference: {diff:.3f}\")\n",
    "\n",
    "# Create dictionary of generators\n",
    "generators = {\n",
    "    'Regular Training': train_generator,\n",
    "    'Aggressive Aug Training': train_generator_aggressive,\n",
    "    'Mild Aug Training': train_generator_mild,\n",
    "    'Validation': val_generator,\n",
    "    'Test': test_generator\n",
    "}\n",
    "\n",
    "# Verify all generators\n",
    "verify_generators(generators)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "f30a965a-f3be-4399-9abd-11425cec4234",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-31T13:50:20.418299Z",
     "iopub.status.busy": "2024-10-31T13:50:20.417020Z",
     "iopub.status.idle": "2024-10-31T13:50:20.506530Z",
     "shell.execute_reply": "2024-10-31T13:50:20.506111Z",
     "shell.execute_reply.started": "2024-10-31T13:50:20.418276Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Default Settings augmentation settings:\n",
      "Original sequence range: [-0.042, 1.133]\n",
      "Augmented 1 range: [-0.042, 1.133]\n",
      "Mean absolute difference: 0.000\n",
      "Augmented 2 range: [-0.057, 1.194]\n",
      "Mean absolute difference: 0.013\n",
      "Augmented 3 range: [-0.042, 1.119]\n",
      "Mean absolute difference: 0.002\n",
      "Augmented 4 range: [-0.041, 1.096]\n",
      "Mean absolute difference: 0.020\n",
      "Augmented 5 range: [-0.044, 1.175]\n",
      "Mean absolute difference: 0.007\n",
      "\n",
      "Testing Aggressive Settings augmentation settings:\n",
      "Original sequence range: [0.000, 0.919]\n",
      "Augmented 1 range: [0.000, 0.906]\n",
      "Mean absolute difference: 0.002\n",
      "Augmented 2 range: [0.000, 0.919]\n",
      "Mean absolute difference: 0.000\n",
      "Augmented 3 range: [-0.069, 1.054]\n",
      "Mean absolute difference: 0.027\n",
      "Augmented 4 range: [0.000, 1.093]\n",
      "Mean absolute difference: 0.028\n",
      "Augmented 5 range: [-0.073, 0.971]\n",
      "Mean absolute difference: 0.016\n",
      "\n",
      "Testing Mild Settings augmentation settings:\n",
      "Original sequence range: [0.000, 1.000]\n",
      "Augmented 1 range: [0.000, 1.000]\n",
      "Mean absolute difference: 0.000\n",
      "Augmented 2 range: [0.000, 1.000]\n",
      "Mean absolute difference: 0.000\n",
      "Augmented 3 range: [0.000, 0.966]\n",
      "Mean absolute difference: 0.005\n",
      "Augmented 4 range: [0.000, 1.001]\n",
      "Mean absolute difference: 0.000\n",
      "Augmented 5 range: [0.000, 1.000]\n",
      "Mean absolute difference: 0.000\n"
     ]
    }
   ],
   "source": [
    "def test_augmentation_settings(generator, name, n_samples=5):\n",
    "    print(f\"\\nTesting {name} augmentation settings:\")\n",
    "    x, y = generator[0]  # Get first batch\n",
    "    \n",
    "    # Get original and augmented sequences\n",
    "    original = x[0]  # First sequence in batch\n",
    "    augmented_sequences = [generator._augment_sequence(original) for _ in range(n_samples)]\n",
    "    \n",
    "    # Print statistics\n",
    "    print(f\"Original sequence range: [{original.min():.3f}, {original.max():.3f}]\")\n",
    "    for i, aug in enumerate(augmented_sequences):\n",
    "        print(f\"Augmented {i+1} range: [{aug.min():.3f}, {aug.max():.3f}]\")\n",
    "        print(f\"Mean absolute difference: {np.mean(np.abs(original - aug)):.3f}\")\n",
    "\n",
    "# Test different configurations\n",
    "test_augmentation_settings(train_generator, \"Default Settings\")\n",
    "test_augmentation_settings(train_generator_aggressive, \"Aggressive Settings\")\n",
    "test_augmentation_settings(train_generator_mild, \"Mild Settings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "1043119b-a975-4df1-8503-880fd6fa8e4f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-31T13:50:21.197159Z",
     "iopub.status.busy": "2024-10-31T13:50:21.195795Z",
     "iopub.status.idle": "2024-10-31T13:50:21.425533Z",
     "shell.execute_reply": "2024-10-31T13:50:21.424969Z",
     "shell.execute_reply.started": "2024-10-31T13:50:21.197135Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Training Generator:\n",
      "Checking first batch shape and values...\n",
      "X shape: (32, 30, 1846)\n",
      "y shape: (32,)\n",
      "X value range: [-0.040, 1.101]\n",
      "Batch 0:\n",
      "  Size=32\n",
      "  Positive samples=4 (0.125)\n",
      "  Negative samples=28 (0.875)\n",
      "  Data shape=(32, 30, 1846)\n",
      "Batch 1:\n",
      "  Size=32\n",
      "  Positive samples=4 (0.125)\n",
      "  Negative samples=28 (0.875)\n",
      "  Data shape=(32, 30, 1846)\n",
      "Batch 2:\n",
      "  Size=32\n",
      "  Positive samples=4 (0.125)\n",
      "  Negative samples=28 (0.875)\n",
      "  Data shape=(32, 30, 1846)\n",
      "Batch 3:\n",
      "  Size=32\n",
      "  Positive samples=4 (0.125)\n",
      "  Negative samples=28 (0.875)\n",
      "  Data shape=(32, 30, 1846)\n",
      "Batch 4:\n",
      "  Size=32\n",
      "  Positive samples=4 (0.125)\n",
      "  Negative samples=28 (0.875)\n",
      "  Data shape=(32, 30, 1846)\n",
      "\n",
      "Overall Distribution Summary:\n",
      "Total samples: 160\n",
      "Positive samples: 20 (0.125)\n",
      "Negative samples: 140 (0.875)\n",
      "\n",
      "Testing Validation Generator:\n",
      "Checking first batch shape and values...\n",
      "X shape: (32, 30, 1846)\n",
      "y shape: (32,)\n",
      "X value range: [0.000, 1.000]\n",
      "Batch 0:\n",
      "  Size=32\n",
      "  Positive samples=5 (0.156)\n",
      "  Negative samples=27 (0.844)\n",
      "  Data shape=(32, 30, 1846)\n",
      "Batch 1:\n",
      "  Size=32\n",
      "  Positive samples=5 (0.156)\n",
      "  Negative samples=27 (0.844)\n",
      "  Data shape=(32, 30, 1846)\n",
      "Batch 2:\n",
      "  Size=32\n",
      "  Positive samples=5 (0.156)\n",
      "  Negative samples=27 (0.844)\n",
      "  Data shape=(32, 30, 1846)\n",
      "Batch 3:\n",
      "  Size=32\n",
      "  Positive samples=5 (0.156)\n",
      "  Negative samples=27 (0.844)\n",
      "  Data shape=(32, 30, 1846)\n",
      "Batch 4:\n",
      "  Size=32\n",
      "  Positive samples=5 (0.156)\n",
      "  Negative samples=27 (0.844)\n",
      "  Data shape=(32, 30, 1846)\n",
      "\n",
      "Overall Distribution Summary:\n",
      "Total samples: 160\n",
      "Positive samples: 25 (0.156)\n",
      "Negative samples: 135 (0.844)\n",
      "\n",
      "Testing Test Generator:\n",
      "Checking first batch shape and values...\n",
      "X shape: (32, 30, 1846)\n",
      "y shape: (32,)\n",
      "X value range: [0.000, 1.000]\n",
      "Batch 0:\n",
      "  Size=32\n",
      "  Positive samples=3 (0.094)\n",
      "  Negative samples=29 (0.906)\n",
      "  Data shape=(32, 30, 1846)\n",
      "Batch 1:\n",
      "  Size=32\n",
      "  Positive samples=3 (0.094)\n",
      "  Negative samples=29 (0.906)\n",
      "  Data shape=(32, 30, 1846)\n",
      "Batch 2:\n",
      "  Size=32\n",
      "  Positive samples=3 (0.094)\n",
      "  Negative samples=29 (0.906)\n",
      "  Data shape=(32, 30, 1846)\n",
      "Batch 3:\n",
      "  Size=32\n",
      "  Positive samples=3 (0.094)\n",
      "  Negative samples=29 (0.906)\n",
      "  Data shape=(32, 30, 1846)\n",
      "Batch 4:\n",
      "  Size=32\n",
      "  Positive samples=3 (0.094)\n",
      "  Negative samples=29 (0.906)\n",
      "  Data shape=(32, 30, 1846)\n",
      "\n",
      "Overall Distribution Summary:\n",
      "Total samples: 160\n",
      "Positive samples: 15 (0.094)\n",
      "Negative samples: 145 (0.906)\n"
     ]
    }
   ],
   "source": [
    "def verify_generator_distribution(gen, name, num_batches=5):\n",
    "    print(f\"\\nTesting {name} Generator:\")\n",
    "    total_pos = 0\n",
    "    total_neg = 0\n",
    "    total_samples = 0\n",
    "    \n",
    "    # Add shape verification\n",
    "    print(f\"Checking first batch shape and values...\")\n",
    "    first_batch_x, first_batch_y = gen[0]\n",
    "    print(f\"X shape: {first_batch_x.shape}\")\n",
    "    print(f\"y shape: {first_batch_y.shape}\")\n",
    "    print(f\"X value range: [{first_batch_x.min():.3f}, {first_batch_x.max():.3f}]\")\n",
    "    \n",
    "    for i in range(num_batches):\n",
    "        x, y = gen[i]\n",
    "        pos = np.sum(y == 1)\n",
    "        neg = np.sum(y == 0)\n",
    "        total_pos += pos\n",
    "        total_neg += neg\n",
    "        total_samples += len(y)\n",
    "        \n",
    "        print(f\"Batch {i}:\")\n",
    "        print(f\"  Size={len(y)}\")\n",
    "        print(f\"  Positive samples={pos} ({pos/len(y):.3f})\")\n",
    "        print(f\"  Negative samples={neg} ({neg/len(y):.3f})\")\n",
    "        print(f\"  Data shape={x.shape}\")\n",
    "    \n",
    "    print(f\"\\nOverall Distribution Summary:\")\n",
    "    print(f\"Total samples: {total_samples}\")\n",
    "    print(f\"Positive samples: {total_pos} ({total_pos/total_samples:.3f})\")\n",
    "    print(f\"Negative samples: {total_neg} ({total_neg/total_samples:.3f})\")\n",
    "\n",
    "# Run verification\n",
    "for name, gen in [('Training', train_generator), \n",
    "                  ('Validation', val_generator), \n",
    "                  ('Test', test_generator)]:\n",
    "    verify_generator_distribution(gen, name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9036991-ccb7-46d5-b3f4-bc197dd4abd8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "719d9790-e28c-4a7c-b3cb-b20c25ecbd7c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-31T13:50:22.872298Z",
     "iopub.status.busy": "2024-10-31T13:50:22.871780Z",
     "iopub.status.idle": "2024-10-31T13:50:22.899994Z",
     "shell.execute_reply": "2024-10-31T13:50:22.899585Z",
     "shell.execute_reply.started": "2024-10-31T13:50:22.872278Z"
    }
   },
   "outputs": [],
   "source": [
    "a,b = train_generator[200]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "2c562f2c-1a5e-48f4-a930-ef7613aad989",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-31T13:50:23.477114Z",
     "iopub.status.busy": "2024-10-31T13:50:23.476480Z",
     "iopub.status.idle": "2024-10-31T13:50:23.479970Z",
     "shell.execute_reply": "2024-10-31T13:50:23.479645Z",
     "shell.execute_reply.started": "2024-10-31T13:50:23.477112Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 30, 1846)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "ccb8778c-05c5-4380-9c7f-0839ca4828fb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-31T13:50:28.126903Z",
     "iopub.status.busy": "2024-10-31T13:50:28.126168Z",
     "iopub.status.idle": "2024-10-31T13:50:28.129943Z",
     "shell.execute_reply": "2024-10-31T13:50:28.129584Z",
     "shell.execute_reply.started": "2024-10-31T13:50:28.126883Z"
    }
   },
   "outputs": [],
   "source": [
    "def focal_loss(gamma=4.0, alpha=0.85):  # Adjusted for your distribution\n",
    "    \"\"\"\n",
    "    Focal Loss for highly imbalanced dataset\n",
    "    gamma: Higher value (4.0) because of severe imbalance (13.4% vs 86.6%)\n",
    "    alpha: 0.85 to match roughly your positive class proportion\n",
    "    \"\"\"\n",
    "    def focal_loss_fixed(y_true, y_pred):\n",
    "        epsilon = 1e-7\n",
    "        y_pred = tf.clip_by_value(y_pred, epsilon, 1 - epsilon)\n",
    "        ce = -y_true * tf.math.log(y_pred)\n",
    "        weight = tf.math.pow(1 - y_pred, gamma)\n",
    "        fl = alpha * weight * ce\n",
    "        return tf.reduce_mean(fl)\n",
    "    return focal_loss_fixed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd369705-1adc-446d-86d3-700e11decf1c",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-10-31T14:48:36.814161Z",
     "iopub.status.idle": "2024-10-31T14:48:36.814644Z",
     "shell.execute_reply": "2024-10-31T14:48:36.814555Z",
     "shell.execute_reply.started": "2024-10-31T14:48:36.814542Z"
    }
   },
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    'units_0': 512,  # Good size\n",
    "    'units_1': 256, \n",
    "    'units_2': 128,\n",
    "    'activation_fxn': 'gelu',  # Try GELU instead of tanh\n",
    "    'recurrent_activation_fxn': 'sigmoid',\n",
    "    'kr': tf.keras.regularizers.L2(0.0005),  # Reduced regularization\n",
    "    'rr': tf.keras.regularizers.L2(0.0005),\n",
    "    'br': tf.keras.regularizers.L2(0.0001),\n",
    "    'timestep': 30,\n",
    "    'input_shape': X_train.shape[1],\n",
    "    'dropout_rate': 0.3,  # Reduced dropout\n",
    "    'activation_fxn_2': 'sigmoid'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e201ebbe-7604-4027-aecd-114d4831a29e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "0b5ed3f5-4467-44e2-93b4-9a778bc33c9d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-31T13:50:31.389872Z",
     "iopub.status.busy": "2024-10-31T13:50:31.389210Z",
     "iopub.status.idle": "2024-10-31T13:50:31.395998Z",
     "shell.execute_reply": "2024-10-31T13:50:31.395642Z",
     "shell.execute_reply.started": "2024-10-31T13:50:31.389833Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_lstm_model(parameters):\n",
    "    # Set float32 precision to avoid dtype mismatches\n",
    "    tf.keras.mixed_precision.set_global_policy('float32')\n",
    "    \n",
    "    lr_schedule = tf.keras.optimizers.schedules.CosineDecayRestarts(\n",
    "        initial_learning_rate=0.0001,  # Initial learning rate\n",
    "        first_decay_steps=1000,        # Steps for first decay cycle\n",
    "        t_mul=2.0,                     # Multiplier for next cycle's steps\n",
    "        m_mul=0.9,                     # Multiplier for next cycle's lr\n",
    "        alpha=0.0001                   # Minimum learning rate\n",
    "    )\n",
    "    \n",
    "    # Extract parameters\n",
    "    units_0 = parameters['units_0']\n",
    "    activation_fxn = parameters['activation_fxn']\n",
    "    recurrent_activation_fxn = parameters['recurrent_activation_fxn']\n",
    "    kr = parameters['kr']\n",
    "    rr = parameters['rr']\n",
    "    br = parameters['br']\n",
    "    timestep = parameters['timestep']\n",
    "    input_shape = parameters['input_shape']\n",
    "    units_1 = parameters['units_1']\n",
    "    units_2 = parameters['units_2']\n",
    "    dropout_rate = parameters['dropout_rate']\n",
    "    activation_fxn_2 = parameters['activation_fxn_2']\n",
    "    \n",
    "    model = Sequential()\n",
    "\n",
    "    # First LSTM Layer\n",
    "    model.add(LSTM(\n",
    "        units=units_0,\n",
    "        activation=activation_fxn,\n",
    "        recurrent_activation=recurrent_activation_fxn,\n",
    "        use_bias=True,\n",
    "        kernel_initializer='glorot_uniform',\n",
    "        recurrent_initializer='orthogonal',\n",
    "        bias_initializer='zeros',\n",
    "        kernel_regularizer=kr,\n",
    "        recurrent_regularizer=rr,\n",
    "        bias_regularizer=br,\n",
    "        unit_forget_bias=True,\n",
    "        return_sequences=True,\n",
    "        input_shape=(timestep, input_shape)\n",
    "    ))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(dropout_rate))\n",
    "\n",
    "    # Second LSTM Layer\n",
    "    model.add(LSTM(\n",
    "        units=units_1,\n",
    "        activation=activation_fxn,\n",
    "        recurrent_activation=recurrent_activation_fxn,\n",
    "        use_bias=True,\n",
    "        kernel_initializer='glorot_uniform',\n",
    "        recurrent_initializer='orthogonal',\n",
    "        bias_initializer='zeros',\n",
    "        kernel_regularizer=kr,\n",
    "        recurrent_regularizer=rr,\n",
    "        bias_regularizer=br,\n",
    "        unit_forget_bias=True,\n",
    "        return_sequences=True\n",
    "    ))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(dropout_rate))\n",
    "\n",
    "    # Third LSTM Layer\n",
    "    model.add(LSTM(\n",
    "        units=units_2,\n",
    "        activation=activation_fxn,\n",
    "        recurrent_activation=recurrent_activation_fxn,\n",
    "        use_bias=True,\n",
    "        kernel_initializer='glorot_uniform',\n",
    "        recurrent_initializer='orthogonal',\n",
    "        bias_initializer='zeros',\n",
    "        kernel_regularizer=kr,\n",
    "        recurrent_regularizer=rr,\n",
    "        bias_regularizer=br,\n",
    "        unit_forget_bias=True,\n",
    "        return_sequences=False\n",
    "    ))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(dropout_rate))\n",
    "\n",
    "    # Output Layer\n",
    "    model.add(Dense(units=1, activation=activation_fxn_2, kernel_regularizer=kr))\n",
    "\n",
    "    # Compile model with AdamW optimizer and focal loss\n",
    "    model.compile(\n",
    "        optimizer=AdamW(\n",
    "            learning_rate=lr_schedule,\n",
    "            weight_decay=0.005,\n",
    "            clipnorm=0.5\n",
    "        ),\n",
    "        loss=focal_loss(gamma=4, alpha=0.85),\n",
    "        metrics=[\n",
    "            'accuracy',\n",
    "            tf.keras.metrics.Precision(),\n",
    "            tf.keras.metrics.Recall(),\n",
    "            tf.keras.metrics.AUC(),\n",
    "            tf.keras.metrics.F1Score(threshold=0.5)\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "4179a33c-3ca1-417c-b449-f009a14b45f7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-31T13:50:32.698634Z",
     "iopub.status.busy": "2024-10-31T13:50:32.698142Z",
     "iopub.status.idle": "2024-10-31T13:50:32.700830Z",
     "shell.execute_reply": "2024-10-31T13:50:32.700524Z",
     "shell.execute_reply.started": "2024-10-31T13:50:32.698615Z"
    }
   },
   "outputs": [],
   "source": [
    "# from sklearn.utils import class_weight\n",
    "# import numpy as np\n",
    "\n",
    "# # Assuming 'y_train' contains the class labels (0s and 1s)\n",
    "# classes = np.unique(y_train)  # Unique class labels in your target data\n",
    "\n",
    "# # Compute class weights\n",
    "# class_weights = class_weight.compute_class_weight(\n",
    "#     class_weight='balanced',  # Set to 'balanced' to compute weights inversely proportional to class frequencies\n",
    "#     classes=classes,  # Provide the unique class labels\n",
    "#     y=y_train['target']  # The target labels\n",
    "# )\n",
    "\n",
    "# # Convert to dictionary format {class_label: weight}\n",
    "# class_weight_dict = dict(zip(classes, class_weights))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "a62df252-5cea-4eb4-9f70-dc418c3a33d6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-31T13:50:33.531749Z",
     "iopub.status.busy": "2024-10-31T13:50:33.531081Z",
     "iopub.status.idle": "2024-10-31T13:50:33.533823Z",
     "shell.execute_reply": "2024-10-31T13:50:33.533511Z",
     "shell.execute_reply.started": "2024-10-31T13:50:33.531727Z"
    }
   },
   "outputs": [],
   "source": [
    "# from sklearn.utils import class_weight\n",
    "# import numpy as np\n",
    "\n",
    "# # Get class distribution\n",
    "# n_samples = len(y_train)\n",
    "# n_classes = len(np.unique(y_train))\n",
    "# counts = y_train['target'].value_counts()\n",
    "\n",
    "# # Calculate custom weights\n",
    "# minority_weight = (n_samples / (n_classes * counts[1])) * 4  # Multiply by 3 for more emphasis\n",
    "# majority_weight = n_samples / (n_classes * counts[0])\n",
    "\n",
    "# class_weight_dict = {\n",
    "#     0: majority_weight,\n",
    "#     1: minority_weight\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "97e94a9c-1c77-4a96-b89c-1573aaa14fce",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-31T13:50:34.608484Z",
     "iopub.status.busy": "2024-10-31T13:50:34.607897Z",
     "iopub.status.idle": "2024-10-31T13:50:34.626242Z",
     "shell.execute_reply": "2024-10-31T13:50:34.625658Z",
     "shell.execute_reply.started": "2024-10-31T13:50:34.608464Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'class_weight_dict' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[100], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mclass_weight_dict\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'class_weight_dict' is not defined"
     ]
    }
   ],
   "source": [
    "class_weight_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "8c5a0afb-9533-4e76-9f6e-cc122ffaaf31",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-31T13:50:35.783715Z",
     "iopub.status.busy": "2024-10-31T13:50:35.782978Z",
     "iopub.status.idle": "2024-10-31T13:50:35.819968Z",
     "shell.execute_reply": "2024-10-31T13:50:35.819616Z",
     "shell.execute_reply.started": "2024-10-31T13:50:35.783694Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "target\n",
       "0    92489\n",
       "1    14308\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[\"target\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "9bf2689f-c799-46d9-b14c-b90dbcd71872",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-31T13:50:36.602200Z",
     "iopub.status.busy": "2024-10-31T13:50:36.601727Z",
     "iopub.status.idle": "2024-10-31T13:50:36.604324Z",
     "shell.execute_reply": "2024-10-31T13:50:36.604006Z",
     "shell.execute_reply.started": "2024-10-31T13:50:36.602182Z"
    }
   },
   "outputs": [],
   "source": [
    "# # First, define a custom precision metric\n",
    "# class CustomPrecision(tf.keras.metrics.Precision):\n",
    "#     def __init__(self, name='custom_precision', **kwargs):\n",
    "#         super().__init__(name=name, **kwargs)\n",
    "    \n",
    "#     def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "#         y_true = tf.cast(y_true, tf.float32)\n",
    "#         y_pred = tf.cast(y_pred, tf.float32)\n",
    "#         return super().update_state(y_true, y_pred, sample_weight)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "be412f07-b395-4942-8193-1a54f3c3cafc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-31T13:50:37.513206Z",
     "iopub.status.busy": "2024-10-31T13:50:37.512589Z",
     "iopub.status.idle": "2024-10-31T13:50:37.515944Z",
     "shell.execute_reply": "2024-10-31T13:50:37.515619Z",
     "shell.execute_reply.started": "2024-10-31T13:50:37.513185Z"
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Bidirectional, LSTM\n",
    "# If using just keras:\n",
    "# from keras.layers import Bidirectional, LSTM\n",
    "\n",
    "# Full imports you might need:\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import (\n",
    "    Bidirectional,\n",
    "    LSTM,\n",
    "    Dense,\n",
    "    Dropout,\n",
    "    BatchNormalization\n",
    ")\n",
    "from tensorflow.keras.optimizers import AdamW\n",
    "from tensorflow.keras.callbacks import (\n",
    "    TensorBoard, \n",
    "    CSVLogger, \n",
    "    EarlyStopping, \n",
    "    ReduceLROnPlateau,\n",
    "    ModelCheckpoint\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "1b2a03ed-e94d-471a-b126-25e53ddb0993",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-31T13:50:39.515095Z",
     "iopub.status.busy": "2024-10-31T13:50:39.514428Z",
     "iopub.status.idle": "2024-10-31T14:48:36.813219Z",
     "shell.execute_reply": "2024-10-31T14:48:36.809429Z",
     "shell.execute_reply.started": "2024-10-31T13:50:39.515055Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_6 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_6 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_6 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_7 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_7 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_7 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_8 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_8 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_8 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/50\n",
      "3303/3303 [==============================] - ETA: 0s - loss: 1.5195 - accuracy: 0.6016 - precision_2: 0.2308 - recall_2: 0.9379 - auc_2: 0.9064 - f1_score: 0.3705"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3303/3303 [==============================] - 616s 183ms/step - loss: 1.5195 - accuracy: 0.6016 - precision_2: 0.2308 - recall_2: 0.9379 - auc_2: 0.9064 - f1_score: 0.3705 - val_loss: 0.7288 - val_accuracy: 0.2542 - val_precision_2: 0.1627 - val_recall_2: 0.9096 - val_auc_2: 0.5246 - val_f1_score: 0.2760\n",
      "Epoch 2/50\n",
      "3303/3303 [==============================] - 616s 186ms/step - loss: 0.3466 - accuracy: 0.4601 - precision_2: 0.1859 - recall_2: 0.9825 - auc_2: 0.9304 - f1_score: 0.3127 - val_loss: 0.2468 - val_accuracy: 0.1569 - val_precision_2: 0.1561 - val_recall_2: 0.9982 - val_auc_2: 0.4985 - val_f1_score: 0.2700\n",
      "Epoch 3/50\n",
      "3303/3303 [==============================] - 624s 189ms/step - loss: 0.1639 - accuracy: 0.2569 - precision_2: 0.1425 - recall_2: 0.9853 - auc_2: 0.8956 - f1_score: 0.2490 - val_loss: 0.1323 - val_accuracy: 0.2392 - val_precision_2: 0.1520 - val_recall_2: 0.8451 - val_auc_2: 0.4851 - val_f1_score: 0.2577\n",
      "Epoch 4/50\n",
      "3303/3303 [==============================] - 620s 188ms/step - loss: 0.0817 - accuracy: 0.1295 - precision_2: 0.1254 - recall_2: 0.9986 - auc_2: 0.8298 - f1_score: 0.2229 - val_loss: 0.0704 - val_accuracy: 0.1562 - val_precision_2: 0.1562 - val_recall_2: 1.0000 - val_auc_2: 0.8310 - val_f1_score: 0.2703\n",
      "Epoch 5/50\n",
      "3303/3303 [==============================] - 605s 183ms/step - loss: 0.0634 - accuracy: 0.1250 - precision_2: 0.1250 - recall_2: 1.0000 - auc_2: 0.6656 - f1_score: 0.2222 - val_loss: 0.0471 - val_accuracy: 0.1562 - val_precision_2: 0.1562 - val_recall_2: 1.0000 - val_auc_2: 0.5521 - val_f1_score: 0.2703\n",
      "Epoch 6/50\n",
      "2213/3303 [===================>..........] - ETA: 3:09 - loss: 0.0366 - accuracy: 0.1250 - precision_2: 0.1250 - recall_2: 1.0000 - auc_2: 0.5006 - f1_score: 0.2222"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[104], line 30\u001b[0m\n\u001b[1;32m      9\u001b[0m callbacks \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     10\u001b[0m     EarlyStopping(\n\u001b[1;32m     11\u001b[0m         monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_f1_score\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     26\u001b[0m     )\n\u001b[1;32m     27\u001b[0m ]\n\u001b[1;32m     29\u001b[0m model \u001b[38;5;241m=\u001b[39m create_lstm_model(parameters)\n\u001b[0;32m---> 30\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_generator_aggressive\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_generator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;66;03m# Enhanced prediction function with progress bar\u001b[39;00m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtqdm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tqdm\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/engine/training.py:1807\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1799\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[1;32m   1800\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1801\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1804\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m   1805\u001b[0m ):\n\u001b[1;32m   1806\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1807\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1808\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[1;32m   1809\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:832\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    829\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    831\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 832\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    834\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    835\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:868\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    865\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    866\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    867\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 868\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtracing_compilation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    869\u001b[0m \u001b[43m      \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_no_variable_creation_config\u001b[49m\n\u001b[1;32m    870\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    871\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    872\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    873\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[1;32m    874\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m bound_args \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    138\u001b[0m flat_inputs \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39munpack_inputs(bound_args)\n\u001b[0;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[1;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py:1323\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1319\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1320\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1321\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1322\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1323\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_preflattened\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1324\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1325\u001b[0m     args,\n\u001b[1;32m   1326\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1327\u001b[0m     executing_eagerly)\n\u001b[1;32m   1328\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:216\u001b[0m, in \u001b[0;36mAtomicFunction.call_preflattened\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    215\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 216\u001b[0m   flat_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    217\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mpack_output(flat_outputs)\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:251\u001b[0m, in \u001b[0;36mAtomicFunction.call_flat\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[1;32m    250\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[0;32m--> 251\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bound_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    256\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    257\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\n\u001b[1;32m    258\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    259\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[1;32m    260\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mfunction_call_options\u001b[38;5;241m.\u001b[39mas_attrs(),\n\u001b[1;32m    261\u001b[0m     )\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/context.py:1486\u001b[0m, in \u001b[0;36mContext.call_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1484\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[1;32m   1485\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1486\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1487\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1488\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1489\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1490\u001b[0m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1491\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1492\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1493\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1494\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m   1495\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1496\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1500\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[1;32m   1501\u001b[0m   )\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "from tensorflow.keras.callbacks import TensorBoard, CSVLogger, EarlyStopping, ReduceLROnPlateau\n",
    "import os\n",
    "# Create log directory\n",
    "log_dir = \"logs/fit/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "# Updated callbacks\n",
    "callbacks = [\n",
    "    EarlyStopping(\n",
    "        monitor='val_f1_score',\n",
    "        mode='max',\n",
    "        patience=10,\n",
    "        restore_best_weights=True\n",
    "    ),\n",
    "    # Remove ReduceLROnPlateau since we're using lr_schedule\n",
    "    ModelCheckpoint(\n",
    "        'best_model.h5',\n",
    "        monitor='val_f1_score',\n",
    "        mode='max',\n",
    "        save_best_only=True\n",
    "    ),\n",
    "    CSVLogger(\n",
    "        'training_log.csv',\n",
    "        append=True\n",
    "    )\n",
    "]\n",
    "\n",
    "model = create_lstm_model(parameters)\n",
    "history = model.fit(\n",
    "    train_generator_aggressive,\n",
    "    validation_data=val_generator,\n",
    "    epochs=50,\n",
    "    callbacks=callbacks\n",
    ")\n",
    "\n",
    "# Enhanced prediction function with progress bar\n",
    "from tqdm import tqdm\n",
    "def get_predictions(model, generator):\n",
    "    predictions = []\n",
    "    true_values = []\n",
    "    \n",
    "    for i in tqdm(range(len(generator)), desc=\"Getting predictions\"):\n",
    "        x, y = generator[i]\n",
    "        pred = model.predict(x, verbose=0)\n",
    "        predictions.extend(pred.flatten())\n",
    "        true_values.extend(y)\n",
    "        \n",
    "    return np.array(predictions), np.array(true_values)\n",
    "\n",
    "# Find optimal threshold\n",
    "def find_optimal_threshold(y_true, y_pred):\n",
    "    precisions, recalls, thresholds = precision_recall_curve(y_true, y_pred)\n",
    "    f1_scores = 2 * (precisions * recalls) / (precisions + recalls + 1e-7)\n",
    "    optimal_idx = np.argmax(f1_scores)\n",
    "    return thresholds[optimal_idx], f1_scores[optimal_idx]\n",
    "\n",
    "# Plot metrics\n",
    "def plot_metrics(y_true, y_pred, title_prefix=\"\"):\n",
    "    # ROC Curve\n",
    "    fpr, tpr, _ = roc_curve(y_true, y_pred)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    \n",
    "    plt.figure(figsize=(15, 5))\n",
    "    \n",
    "    # ROC curve\n",
    "    plt.subplot(131)\n",
    "    plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.2f})')\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title(f'{title_prefix} ROC Curve')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    \n",
    "    # Precision-Recall curve\n",
    "    precisions, recalls, _ = precision_recall_curve(y_true, y_pred)\n",
    "    plt.subplot(132)\n",
    "    plt.plot(recalls, precisions, color='blue', lw=2)\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.title(f'{title_prefix} Precision-Recall Curve')\n",
    "    \n",
    "    # Prediction distribution\n",
    "    plt.subplot(133)\n",
    "    plt.hist(y_pred[y_true==0], bins=50, alpha=0.5, label='Negative class', density=True)\n",
    "    plt.hist(y_pred[y_true==1], bins=50, alpha=0.5, label='Positive class', density=True)\n",
    "    plt.xlabel('Prediction value')\n",
    "    plt.ylabel('Density')\n",
    "    plt.title(f'{title_prefix} Prediction Distribution')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{title_prefix.lower().replace(\" \", \"_\")}_metrics.png')\n",
    "    plt.show()\n",
    "\n",
    "# Get predictions\n",
    "print(\"Getting validation predictions...\")\n",
    "val_pred, val_true = get_predictions(model, val_generator)\n",
    "print(\"\\nGetting test predictions...\")\n",
    "test_pred, test_true = get_predictions(model, test_generator)\n",
    "\n",
    "# Find optimal threshold on validation set\n",
    "optimal_threshold, best_f1 = find_optimal_threshold(val_true, val_pred)\n",
    "print(f\"\\nOptimal threshold: {optimal_threshold:.3f} (F1: {best_f1:.3f})\")\n",
    "\n",
    "# Print metrics with optimal threshold\n",
    "print(\"\\nValidation Metrics:\")\n",
    "print(classification_report(val_true, (val_pred > optimal_threshold).astype(int)))\n",
    "print(\"\\nConfusion Matrix (Validation):\")\n",
    "print(confusion_matrix(val_true, (val_pred > optimal_threshold).astype(int)))\n",
    "\n",
    "print(\"\\nTest Metrics:\")\n",
    "print(classification_report(test_true, (test_pred > optimal_threshold).astype(int)))\n",
    "print(\"\\nConfusion Matrix (Test):\")\n",
    "print(confusion_matrix(test_true, (test_pred > optimal_threshold).astype(int)))\n",
    "\n",
    "# Plot metrics\n",
    "plot_metrics(val_true, val_pred, \"Validation\")\n",
    "plot_metrics(test_true, test_pred, \"Test\")\n",
    "\n",
    "# Plot training history\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.subplot(121)\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Model Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.plot(history.history['f1_score'], label='Training F1')\n",
    "plt.plot(history.history['val_f1_score'], label='Validation F1')\n",
    "plt.title('Model F1 Score')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('F1 Score')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('training_history.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "da4d73b0-7435-44cb-95ce-652e6e0d9391",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-30T06:33:12.944553Z",
     "iopub.status.busy": "2024-10-30T06:33:12.944067Z",
     "iopub.status.idle": "2024-10-30T07:49:24.940187Z",
     "shell.execute_reply": "2024-10-30T07:49:24.939694Z",
     "shell.execute_reply.started": "2024-10-30T06:33:12.944528Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-30 06:33:19.172028: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:454] Loaded cuDNN version 8907\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-30 06:33:32.628836: I external/local_xla/xla/service/service.cc:168] XLA service 0x7fcabc17a290 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2024-10-30 06:33:32.628886: I external/local_xla/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA RTX A6000, Compute Capability 8.6\n",
      "2024-10-30 06:33:32.651589: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1730270012.773791     128 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3337/3337 [==============================] - 106s 27ms/step - loss: 2.5312 - accuracy: 0.2886 - precision: 0.1397 - recall: 0.8352 - val_loss: 0.7388 - val_accuracy: 0.1533 - val_precision: 0.1533 - val_recall: 0.9998\n",
      "Epoch 2/50\n",
      "3337/3337 [==============================] - 84s 25ms/step - loss: 0.4247 - accuracy: 0.1387 - precision: 0.1340 - recall: 0.9941 - val_loss: 0.2222 - val_accuracy: 0.1533 - val_precision: 0.1533 - val_recall: 1.0000\n",
      "Epoch 3/50\n",
      "3337/3337 [==============================] - 86s 26ms/step - loss: 0.1560 - accuracy: 0.1355 - precision: 0.1340 - recall: 0.9986 - val_loss: 0.1143 - val_accuracy: 0.1533 - val_precision: 0.1533 - val_recall: 1.0000\n",
      "Epoch 4/50\n",
      "3337/3337 [==============================] - 84s 25ms/step - loss: 0.0746 - accuracy: 0.1347 - precision: 0.1339 - recall: 0.9987 - val_loss: 0.0460 - val_accuracy: 0.1533 - val_precision: 0.1533 - val_recall: 1.0000\n",
      "Epoch 5/50\n",
      "3337/3337 [==============================] - 93s 28ms/step - loss: 0.0432 - accuracy: 0.1363 - precision: 0.1341 - recall: 0.9980 - val_loss: 0.0260 - val_accuracy: 0.1533 - val_precision: 0.1533 - val_recall: 1.0000\n",
      "Epoch 6/50\n",
      "3337/3337 [==============================] - 88s 26ms/step - loss: 0.0152 - accuracy: 0.1340 - precision: 0.1340 - recall: 1.0000 - val_loss: 0.0080 - val_accuracy: 0.1533 - val_precision: 0.1533 - val_recall: 1.0000\n",
      "Epoch 7/50\n",
      "3337/3337 [==============================] - 78s 23ms/step - loss: 0.0044 - accuracy: 0.1340 - precision: 0.1340 - recall: 1.0000 - val_loss: 0.0018 - val_accuracy: 0.1533 - val_precision: 0.1533 - val_recall: 1.0000\n",
      "Epoch 8/50\n",
      "3337/3337 [==============================] - 89s 27ms/step - loss: 9.7365e-04 - accuracy: 0.1340 - precision: 0.1340 - recall: 1.0000 - val_loss: 3.6376e-04 - val_accuracy: 0.1533 - val_precision: 0.1533 - val_recall: 1.0000\n",
      "Epoch 9/50\n",
      "3337/3337 [==============================] - 87s 26ms/step - loss: 2.7430e-04 - accuracy: 0.1340 - precision: 0.1340 - recall: 1.0000 - val_loss: 1.7189e-04 - val_accuracy: 0.1533 - val_precision: 0.1533 - val_recall: 1.0000\n",
      "Epoch 10/50\n",
      "3337/3337 [==============================] - 81s 24ms/step - loss: 1.6555e-04 - accuracy: 0.1340 - precision: 0.1340 - recall: 1.0000 - val_loss: 1.1795e-04 - val_accuracy: 0.1533 - val_precision: 0.1533 - val_recall: 1.0000\n",
      "Epoch 11/50\n",
      "3337/3337 [==============================] - 89s 27ms/step - loss: 1.1565e-04 - accuracy: 0.1340 - precision: 0.1340 - recall: 1.0000 - val_loss: 8.3933e-05 - val_accuracy: 0.1533 - val_precision: 0.1533 - val_recall: 1.0000\n",
      "Epoch 12/50\n",
      "3337/3337 [==============================] - 87s 26ms/step - loss: 8.4003e-05 - accuracy: 0.1340 - precision: 0.1340 - recall: 1.0000 - val_loss: 6.2261e-05 - val_accuracy: 0.1533 - val_precision: 0.1533 - val_recall: 1.0000\n",
      "Epoch 13/50\n",
      "3337/3337 [==============================] - 89s 27ms/step - loss: 6.3136e-05 - accuracy: 0.1340 - precision: 0.1340 - recall: 1.0000 - val_loss: 4.4316e-05 - val_accuracy: 0.1533 - val_precision: 0.1533 - val_recall: 1.0000\n",
      "Epoch 14/50\n",
      "3337/3337 [==============================] - 91s 27ms/step - loss: 4.8794e-05 - accuracy: 0.1340 - precision: 0.1340 - recall: 1.0000 - val_loss: 3.6829e-05 - val_accuracy: 0.1533 - val_precision: 0.1533 - val_recall: 1.0000\n",
      "Epoch 15/50\n",
      "3337/3337 [==============================] - 83s 25ms/step - loss: 3.8606e-05 - accuracy: 0.1340 - precision: 0.1340 - recall: 1.0000 - val_loss: 2.5502e-05 - val_accuracy: 0.1533 - val_precision: 0.1533 - val_recall: 1.0000\n",
      "Epoch 16/50\n",
      "3337/3337 [==============================] - 87s 26ms/step - loss: 2.1433e-05 - accuracy: 0.1340 - precision: 0.1340 - recall: 1.0000 - val_loss: 1.5058e-05 - val_accuracy: 0.1533 - val_precision: 0.1533 - val_recall: 1.0000\n",
      "Epoch 19/50\n",
      "3337/3337 [==============================] - 78s 23ms/step - loss: 1.7993e-05 - accuracy: 0.1340 - precision: 0.1340 - recall: 1.0000 - val_loss: 1.4327e-05 - val_accuracy: 0.1533 - val_precision: 0.1533 - val_recall: 1.0000\n",
      "Epoch 20/50\n",
      "3337/3337 [==============================] - 94s 28ms/step - loss: 1.5368e-05 - accuracy: 0.1340 - precision: 0.1340 - recall: 1.0000 - val_loss: 1.2711e-05 - val_accuracy: 0.1533 - val_precision: 0.1533 - val_recall: 1.0000\n",
      "Epoch 21/50\n",
      "3337/3337 [==============================] - 89s 27ms/step - loss: 1.3222e-05 - accuracy: 0.1340 - precision: 0.1340 - recall: 1.0000 - val_loss: 1.0250e-05 - val_accuracy: 0.1533 - val_precision: 0.1533 - val_recall: 1.0000\n",
      "Epoch 22/50\n",
      "3337/3337 [==============================] - 84s 25ms/step - loss: 1.1532e-05 - accuracy: 0.1340 - precision: 0.1340 - recall: 1.0000 - val_loss: 8.0978e-06 - val_accuracy: 0.1533 - val_precision: 0.1533 - val_recall: 1.0000\n",
      "Epoch 23/50\n",
      "3337/3337 [==============================] - 89s 27ms/step - loss: 1.0061e-05 - accuracy: 0.1340 - precision: 0.1340 - recall: 1.0000 - val_loss: 6.7303e-06 - val_accuracy: 0.1533 - val_precision: 0.1533 - val_recall: 1.0000\n",
      "Epoch 24/50\n",
      "3337/3337 [==============================] - 89s 27ms/step - loss: 8.9249e-06 - accuracy: 0.1340 - precision: 0.1340 - recall: 1.0000 - val_loss: 6.2178e-06 - val_accuracy: 0.1533 - val_precision: 0.1533 - val_recall: 1.0000\n",
      "Epoch 25/50\n",
      "3337/3337 [==============================] - 78s 24ms/step - loss: 7.8705e-06 - accuracy: 0.1340 - precision: 0.1340 - recall: 1.0000 - val_loss: 5.8824e-06 - val_accuracy: 0.1533 - val_precision: 0.1533 - val_recall: 1.0000\n",
      "Epoch 26/50\n",
      "3337/3337 [==============================] - 91s 27ms/step - loss: 6.9545e-06 - accuracy: 0.1340 - precision: 0.1340 - recall: 1.0000 - val_loss: 4.8756e-06 - val_accuracy: 0.1533 - val_precision: 0.1533 - val_recall: 1.0000\n",
      "Epoch 27/50\n",
      "3337/3337 [==============================] - 88s 26ms/step - loss: 6.2086e-06 - accuracy: 0.1340 - precision: 0.1340 - recall: 1.0000 - val_loss: 3.9341e-06 - val_accuracy: 0.1533 - val_precision: 0.1533 - val_recall: 1.0000\n",
      "Epoch 28/50\n",
      "3337/3337 [==============================] - 81s 24ms/step - loss: 5.5259e-06 - accuracy: 0.1340 - precision: 0.1340 - recall: 1.0000 - val_loss: 4.2229e-06 - val_accuracy: 0.1533 - val_precision: 0.1533 - val_recall: 1.0000\n",
      "Epoch 29/50\n",
      "3337/3337 [==============================] - 87s 26ms/step - loss: 4.9415e-06 - accuracy: 0.1340 - precision: 0.1340 - recall: 1.0000 - val_loss: 3.0710e-06 - val_accuracy: 0.1533 - val_precision: 0.1533 - val_recall: 1.0000\n",
      "Epoch 30/50\n",
      "3337/3337 [==============================] - 87s 26ms/step - loss: 4.4055e-06 - accuracy: 0.1340 - precision: 0.1340 - recall: 1.0000 - val_loss: 3.1517e-06 - val_accuracy: 0.1533 - val_precision: 0.1533 - val_recall: 1.0000\n",
      "Epoch 31/50\n",
      "3337/3337 [==============================] - 88s 26ms/step - loss: 3.9352e-06 - accuracy: 0.1340 - precision: 0.1340 - recall: 1.0000 - val_loss: 2.8493e-06 - val_accuracy: 0.1533 - val_precision: 0.1533 - val_recall: 1.0000\n",
      "Epoch 32/50\n",
      "3337/3337 [==============================] - 81s 24ms/step - loss: 3.5455e-06 - accuracy: 0.1340 - precision: 0.1340 - recall: 1.0000 - val_loss: 1.8255e-06 - val_accuracy: 0.1533 - val_precision: 0.1533 - val_recall: 1.0000\n",
      "Epoch 33/50\n",
      "3337/3337 [==============================] - 93s 28ms/step - loss: 3.1568e-06 - accuracy: 0.1340 - precision: 0.1340 - recall: 1.0000 - val_loss: 1.3712e-06 - val_accuracy: 0.1533 - val_precision: 0.1533 - val_recall: 1.0000\n",
      "Epoch 34/50\n",
      "3337/3337 [==============================] - 88s 26ms/step - loss: 2.8672e-06 - accuracy: 0.1340 - precision: 0.1340 - recall: 1.0000 - val_loss: 2.2307e-06 - val_accuracy: 0.1533 - val_precision: 0.1533 - val_recall: 1.0000\n",
      "Epoch 35/50\n",
      "3337/3337 [==============================] - 82s 25ms/step - loss: 2.5395e-06 - accuracy: 0.1340 - precision: 0.1340 - recall: 1.0000 - val_loss: 1.5448e-06 - val_accuracy: 0.1533 - val_precision: 0.1533 - val_recall: 1.0000\n",
      "Epoch 36/50\n",
      "3337/3337 [==============================] - 89s 27ms/step - loss: 2.2677e-06 - accuracy: 0.1340 - precision: 0.1340 - recall: 1.0000 - val_loss: 1.6118e-06 - val_accuracy: 0.1533 - val_precision: 0.1533 - val_recall: 1.0000\n",
      "Epoch 37/50\n",
      "3337/3337 [==============================] - 88s 26ms/step - loss: 2.0489e-06 - accuracy: 0.1340 - precision: 0.1340 - recall: 1.0000 - val_loss: 1.1684e-06 - val_accuracy: 0.1533 - val_precision: 0.1533 - val_recall: 1.0000\n",
      "Epoch 38/50\n",
      "3337/3337 [==============================] - 81s 24ms/step - loss: 1.7918e-06 - accuracy: 0.1340 - precision: 0.1340 - recall: 1.0000 - val_loss: 1.3677e-06 - val_accuracy: 0.1533 - val_precision: 0.1533 - val_recall: 1.0000\n",
      "Epoch 39/50\n",
      "3337/3337 [==============================] - 69s 21ms/step - loss: 1.5773e-06 - accuracy: 0.1340 - precision: 0.1340 - recall: 1.0000 - val_loss: 9.4761e-07 - val_accuracy: 0.1533 - val_precision: 0.1533 - val_recall: 1.0000\n",
      "Epoch 40/50\n",
      "3337/3337 [==============================] - 70s 21ms/step - loss: 1.4000e-06 - accuracy: 0.1340 - precision: 0.1340 - recall: 1.0000 - val_loss: 1.1435e-06 - val_accuracy: 0.1533 - val_precision: 0.1533 - val_recall: 1.0000\n",
      "Epoch 41/50\n",
      "3337/3337 [==============================] - 75s 22ms/step - loss: 1.2117e-06 - accuracy: 0.1340 - precision: 0.1340 - recall: 1.0000 - val_loss: 4.8851e-07 - val_accuracy: 0.1533 - val_precision: 0.1533 - val_recall: 1.0000\n",
      "Epoch 42/50\n",
      "3337/3337 [==============================] - 93s 28ms/step - loss: 1.0484e-06 - accuracy: 0.1340 - precision: 0.1340 - recall: 1.0000 - val_loss: 3.2446e-07 - val_accuracy: 0.1533 - val_precision: 0.1533 - val_recall: 1.0000\n",
      "Epoch 43/50\n",
      "3337/3337 [==============================] - 92s 27ms/step - loss: 9.0683e-07 - accuracy: 0.1340 - precision: 0.1340 - recall: 1.0000 - val_loss: 3.7239e-07 - val_accuracy: 0.1533 - val_precision: 0.1533 - val_recall: 1.0000\n",
      "Epoch 44/50\n",
      "3337/3337 [==============================] - 83s 25ms/step - loss: 7.8833e-07 - accuracy: 0.1340 - precision: 0.1340 - recall: 1.0000 - val_loss: 1.8277e-07 - val_accuracy: 0.1533 - val_precision: 0.1533 - val_recall: 1.0000\n",
      "Epoch 45/50\n",
      "3337/3337 [==============================] - 88s 26ms/step - loss: 6.7509e-07 - accuracy: 0.1340 - precision: 0.1340 - recall: 1.0000 - val_loss: 3.3909e-07 - val_accuracy: 0.1533 - val_precision: 0.1533 - val_recall: 1.0000\n",
      "Epoch 46/50\n",
      "3337/3337 [==============================] - 91s 27ms/step - loss: 5.8234e-07 - accuracy: 0.1340 - precision: 0.1340 - recall: 1.0000 - val_loss: 3.6732e-07 - val_accuracy: 0.1533 - val_precision: 0.1533 - val_recall: 1.0000\n",
      "Epoch 47/50\n",
      "3337/3337 [==============================] - 83s 25ms/step - loss: 4.8807e-07 - accuracy: 0.1340 - precision: 0.1340 - recall: 1.0000 - val_loss: 1.4530e-07 - val_accuracy: 0.1533 - val_precision: 0.1533 - val_recall: 1.0000\n",
      "Epoch 48/50\n",
      "3337/3337 [==============================] - 89s 27ms/step - loss: 4.0954e-07 - accuracy: 0.1340 - precision: 0.1340 - recall: 1.0000 - val_loss: 5.3955e-08 - val_accuracy: 0.1533 - val_precision: 0.1533 - val_recall: 1.0000\n",
      "Epoch 49/50\n",
      "3337/3337 [==============================] - 92s 27ms/step - loss: 3.4209e-07 - accuracy: 0.1340 - precision: 0.1340 - recall: 1.0000 - val_loss: 6.3435e-08 - val_accuracy: 0.1533 - val_precision: 0.1533 - val_recall: 1.0000\n",
      "Epoch 50/50\n",
      "3337/3337 [==============================] - 79s 24ms/step - loss: 2.8869e-07 - accuracy: 0.1340 - precision: 0.1340 - recall: 1.0000 - val_loss: 2.3522e-08 - val_accuracy: 0.1533 - val_precision: 0.1533 - val_recall: 1.0000\n"
     ]
    }
   ],
   "source": [
    "model = create_lstm_model(parameters)\n",
    "history = model.fit(\n",
    "    train_generator,   # Generator object\n",
    "    validation_data=val_generator,  # Validation generator\n",
    "    epochs=50,         # Number of epochs\n",
    "    class_weight=class_weight_dict,  # Class weights to handle imbalance\n",
    "    steps_per_epoch=len(train_generator),  # Total steps per epoch\n",
    "    validation_steps=len(val_generator),\n",
    "    callbacks=[early_stopping, csv_logger, tensorboard_callback],\n",
    "    verbose=1          # Verbose level for logging training progress\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7444aa56-6e60-496f-b4dd-b8c5593a208a",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-10-26T18:54:54.097755Z",
     "iopub.status.idle": "2024-10-26T18:54:54.097937Z",
     "shell.execute_reply": "2024-10-26T18:54:54.097861Z",
     "shell.execute_reply.started": "2024-10-26T18:54:54.097851Z"
    }
   },
   "outputs": [],
   "source": [
    "Data - >. 1. Training, Val, Test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4a4565aa-adce-4332-9b1b-fce12b3a2853",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-11T12:56:41.412998Z",
     "iopub.status.busy": "2024-09-11T12:56:41.412859Z",
     "iopub.status.idle": "2024-09-11T12:56:41.417725Z",
     "shell.execute_reply": "2024-09-11T12:56:41.417145Z",
     "shell.execute_reply.started": "2024-09-11T12:56:41.412984Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "import logging\n",
    "from tensorflow.keras.optimizers import (\n",
    "    Adam, SGD, Nadam, RMSprop, Adadelta, Adagrad, Adamax, Ftrl, SGD, AdamW)\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from tensorflow.keras.utils import Sequence\n",
    "from tensorflow.keras.layers import LeakyReLU\n",
    "from tensorflow.keras.callbacks import EarlyStopping  # Import EarlyStopping\n",
    "\n",
    "\n",
    "logging.basicConfig(\n",
    "    filename='training_log.log',  # log output file\n",
    "    level=logging.INFO,  # Correct logging level\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',  # log format\n",
    "    datefmt='%y-%m-%d %H:%M:%S'  # date format\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "optimizers = {\n",
    "    # 'Adam': Adam(learning_rate=0.0009),\n",
    "    # 'SGD': SGD(learning_rate=0.0009),\n",
    "    # 'Nadam': Nadam(learning_rate=0.0009),\n",
    "    # 'RMSprop': RMSprop(learning_rate=0.0009),\n",
    "    # 'Adadelta': Adadelta(learning_rate=0.0009),\n",
    "    # 'Adagrad': Adagrad(learning_rate=0.0009),\n",
    "    # 'Adamax': Adamax(learning_rate=0.0009),\n",
    "    # 'FTRL': Ftrl(learning_rate=0.0009),\n",
    "    # 'SGDW': SGD(learning_rate=0.0009, weight_decay=0.01),\n",
    "    # 'AdamW-0.0005': AdamW(learning_rate=0.0005, weight_decay=0.01),\n",
    "    'AdamW-0.0006': AdamW(learning_rate=0.0006, weight_decay=0.01)\n",
    "}\n",
    "# Generator class for batch processing\n",
    "class DataGenerator(Sequence):\n",
    "    def __init__(self, data, batch_size=128):  # Reduced batch size\n",
    "        self.data = data\n",
    "        self.batch_size = batch_size\n",
    "        self.indices = np.arange(len(data))\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.data) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        batch_indices = self.indices[index * self.batch_size:(index + 1) * self.batch_size]\n",
    "        batch_data = self.data.iloc[batch_indices].values  # Use .iloc to ensure correct indexing\n",
    "        return batch_data, batch_data  # For autoencoder, both input and output are the same\n",
    "\n",
    "# Function to build the autoencoder model\n",
    "def build_autoencoder(input_dim, encoding_dim, optimizer):\n",
    "    input_layer = Input(shape=(input_dim,))\n",
    "\n",
    "    # encoder layers\n",
    "    encoder = Dense(512, activation=\"relu\")(input_layer)  # first hidden layer with 512 nodes\n",
    "    encoder = Dense(256, activation=\"relu\")(encoder)      # second hidden layer with 256 nodes\n",
    "    encoder = Dense(encoding_dim, activation=\"relu\")(encoder)  # bottleneck layer with `encoding_dim` nodes\n",
    "\n",
    "    # decoder layers\n",
    "    decoder = Dense(256, activation=\"relu\")(encoder)      # mirror the encoder layers (256 nodes)\n",
    "    decoder = Dense(512, activation=\"relu\")(decoder)      # mirror the encoder layers (512 nodes)\n",
    "    decoder = Dense(input_dim, activation='sigmoid')(decoder)  # output layer with `input_dim` nodes\n",
    "\n",
    "    autoencoder = Model(inputs=input_layer, outputs=decoder)\n",
    "    autoencoder.compile(optimizer=optimizer, loss=\"mse\")  # use the provided optimizer\n",
    "    return autoencoder, Model(inputs=input_layer, outputs=encoder)\n",
    "\n",
    "# Function to train the autoencoder with batch learning\n",
    "def train_autoencoder(autoencoder, data, epochs=35, batch_size=128):  # increased epochs, batch size 200\n",
    "    x_train, x_val = train_test_split(data, test_size=0.1, random_state=42)\n",
    "    \n",
    "    # create generators\n",
    "    train_generator = DataGenerator(x_train, batch_size=batch_size)\n",
    "    val_generator = DataGenerator(x_val, batch_size=batch_size)\n",
    "    \n",
    "    # early stopping callback\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "    \n",
    "    history = autoencoder.fit(train_generator,\n",
    "                              epochs=epochs,\n",
    "                              validation_data=val_generator,\n",
    "                              callbacks=[early_stopping])  # add the early stopping callback here\n",
    "    return history\n",
    "\n",
    "# Function to evaluate the model and find the optimal dimensionality\n",
    "def find_optimal_dimension(df_scaled, optimizers, epochs=35, batch_size=128):  \n",
    "    input_dim = df_scaled.shape[1]\n",
    "    errors = []\n",
    "    encoding_dims = [60]  # You can expand this list to try other dimensions\n",
    "\n",
    "    for encoding_dim in encoding_dims:\n",
    "        for optimizer_name, optimizer in optimizers.items():\n",
    "            logging.info(f\"Training with optimizer: {optimizer_name}\")\n",
    "            autoencoder, encoder = build_autoencoder(input_dim, encoding_dim, optimizer)\n",
    "            train_autoencoder(autoencoder, df_scaled, epochs=epochs, batch_size=batch_size)\n",
    "\n",
    "            # Use the trained autoencoder to encode and decode the data in batches\n",
    "            encoded_data = []\n",
    "            decoded_data = []\n",
    "            for i in range(0, len(df_scaled), batch_size):\n",
    "                batch = df_scaled.iloc[i:i + batch_size].values  # Use .iloc to ensure correct indexing\n",
    "                encoded_batch = encoder.predict(batch)\n",
    "                decoded_batch = autoencoder.predict(batch)\n",
    "                encoded_data.append(encoded_batch)\n",
    "                decoded_data.append(decoded_batch)\n",
    "\n",
    "            encoded_data = np.vstack(encoded_data)\n",
    "            decoded_data = np.vstack(decoded_data)\n",
    "\n",
    "            # Calculate reconstruction error\n",
    "            mse = mean_squared_error(df_scaled.values, decoded_data)\n",
    "            errors.append((encoding_dim, optimizer_name, mse))\n",
    "            logging.info(f\"Encoding Dim: {encoding_dim}, Optimizer: {optimizer_name}, MSE: {mse}\")\n",
    "\n",
    "            # Free up memory\n",
    "            del autoencoder, encoder, encoded_data, decoded_data\n",
    "            gc.collect()\n",
    "\n",
    "    # Find the encoding dimension with the smallest error\n",
    "    optimal_dim, optimal_optimizer_name, min_mse = min(errors, key=lambda x: x[2])\n",
    "    logging.info(f\"Optimal Encoding Dimension: {optimal_dim}, Optimizer: {optimal_optimizer_name}, MSE: {min_mse}\")\n",
    "\n",
    "    # Return the optimal dimension and the corresponding encoder\n",
    "    return optimal_dim, build_autoencoder(input_dim, optimal_dim, optimizers[optimal_optimizer_name])[1]\n",
    "\n",
    "\n",
    "\n",
    "# Find the optimal dimension\n",
    "optimal_dim, optimal_encoder = find_optimal_dimension(df, optimizers, epochs=35, batch_size=128)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9ee36c01-a342-4c5b-b3ac-39a3bb8daf3d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-11T12:56:41.418578Z",
     "iopub.status.busy": "2024-09-11T12:56:41.418354Z",
     "iopub.status.idle": "2024-09-11T12:56:41.425244Z",
     "shell.execute_reply": "2024-09-11T12:56:41.424622Z",
     "shell.execute_reply.started": "2024-09-11T12:56:41.418565Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24-09-09 07:37:27 - INFO - Training with optimizer: Adam\n",
      "24-09-09 07:42:58 - INFO - Encoding Dim: 60, Optimizer: Adam, MSE: 0.00039134073902094736\n",
      "24-09-09 07:42:58 - INFO - Training with optimizer: SGD\n",
      "24-09-09 07:48:10 - INFO - Encoding Dim: 60, Optimizer: SGD, MSE: 0.019344559345294512\n",
      "24-09-09 07:48:10 - INFO - Training with optimizer: Nadam\n",
      "24-09-09 07:52:58 - INFO - Encoding Dim: 60, Optimizer: Nadam, MSE: 0.0004165755366704315\n",
      "24-09-09 07:52:58 - INFO - Training with optimizer: RMSprop\n",
      "24-09-09 07:58:02 - INFO - Encoding Dim: 60, Optimizer: RMSprop, MSE: 0.001750231789881493\n",
      "24-09-09 07:58:03 - INFO - Training with optimizer: Adadelta\n",
      "24-09-09 08:03:13 - INFO - Encoding Dim: 60, Optimizer: Adadelta, MSE: 0.02012077012827505\n",
      "24-09-09 08:03:13 - INFO - Training with optimizer: Adagrad\n",
      "24-09-09 08:08:17 - INFO - Encoding Dim: 60, Optimizer: Adagrad, MSE: 0.015115404734940944\n",
      "24-09-09 08:08:18 - INFO - Training with optimizer: Adamax\n",
      "24-09-09 08:13:20 - INFO - Encoding Dim: 60, Optimizer: Adamax, MSE: 0.000525860126003413\n",
      "24-09-09 08:13:20 - INFO - Training with optimizer: FTRL\n",
      "24-09-09 15:58:44 - INFO - Training with optimizer: FTRL\n",
      "24-09-09 16:03:30 - INFO - Encoding Dim: 60, Optimizer: FTRL, MSE: 0.12035803139232165\n",
      "24-09-09 16:03:30 - INFO - Training with optimizer: SGDW\n",
      "24-09-09 16:08:25 - INFO - Encoding Dim: 60, Optimizer: SGDW, MSE: 0.021841895117691135\n",
      "24-09-09 16:08:25 - INFO - Training with optimizer: AdamW\n",
      "24-09-09 16:13:31 - INFO - Encoding Dim: 60, Optimizer: AdamW, MSE: 0.0004266640576258005\n",
      "24-09-09 16:13:31 - INFO - Optimal Encoding Dimension: 60, Optimizer: AdamW, MSE: 0.0004266640576258005\n",
      "24-09-09 16:22:55 - INFO - Training with optimizer: Adam\n",
      "24-09-09 16:28:17 - INFO - Encoding Dim: 60, Optimizer: Adam, MSE: 0.000633563765379747\n",
      "24-09-09 16:28:17 - INFO - Training with optimizer: SGD\n",
      "24-09-09 16:33:27 - INFO - Encoding Dim: 60, Optimizer: SGD, MSE: 0.019119386343005133\n",
      "24-09-09 16:33:28 - INFO - Training with optimizer: Nadam\n",
      "24-09-09 16:36:59 - INFO - Encoding Dim: 60, Optimizer: Nadam, MSE: 0.0006476806549614504\n",
      "24-09-09 16:36:59 - INFO - Training with optimizer: RMSprop\n",
      "24-09-09 16:41:56 - INFO - Encoding Dim: 60, Optimizer: RMSprop, MSE: 0.0011563050667090101\n",
      "24-09-09 16:41:57 - INFO - Training with optimizer: Adadelta\n",
      "24-09-09 16:46:55 - INFO - Encoding Dim: 60, Optimizer: Adadelta, MSE: 0.01883450426527251\n",
      "24-09-09 16:46:56 - INFO - Training with optimizer: Adagrad\n",
      "24-09-09 16:55:56 - INFO - Training with optimizer: Adagrad\n",
      "24-09-09 17:00:29 - INFO - Encoding Dim: 60, Optimizer: Adagrad, MSE: 0.01585810855577322\n",
      "24-09-09 17:00:29 - INFO - Training with optimizer: Adamax\n",
      "24-09-09 17:05:09 - INFO - Encoding Dim: 60, Optimizer: Adamax, MSE: 0.0003489191823373623\n",
      "24-09-09 17:05:10 - INFO - Training with optimizer: FTRL\n",
      "24-09-09 17:09:52 - INFO - Encoding Dim: 60, Optimizer: FTRL, MSE: 0.1203580406223587\n",
      "24-09-09 17:09:52 - INFO - Training with optimizer: SGDW\n",
      "24-09-09 17:14:50 - INFO - Encoding Dim: 60, Optimizer: SGDW, MSE: 0.019365144348051613\n",
      "24-09-09 17:14:50 - INFO - Training with optimizer: AdamW\n",
      "24-09-09 17:20:11 - INFO - Encoding Dim: 60, Optimizer: AdamW, MSE: 0.0002078600568211503\n",
      "24-09-09 17:20:12 - INFO - Optimal Encoding Dimension: 60, Optimizer: AdamW, MSE: 0.0002078600568211503\n",
      "24-09-09 18:12:57 - INFO - Training with optimizer: AdamW-0.0009\n",
      "24-09-09 18:18:09 - INFO - Encoding Dim: 60, Optimizer: AdamW-0.0009, MSE: 0.000246963290393047\n",
      "24-09-09 18:18:10 - INFO - Training with optimizer: AdamW-0.0011\n",
      "24-09-09 18:23:28 - INFO - Encoding Dim: 60, Optimizer: AdamW-0.0011, MSE: 0.0003326209534390893\n",
      "24-09-09 18:23:29 - INFO - Training with optimizer: AdamW-0.0007\n",
      "24-09-09 18:28:49 - INFO - Encoding Dim: 60, Optimizer: AdamW-0.0007, MSE: 0.00019835247349895275\n",
      "24-09-09 18:28:50 - INFO - Optimal Encoding Dimension: 60, Optimizer: AdamW-0.0007, MSE: 0.00019835247349895275\n",
      "24-09-10 09:22:47 - INFO - Training with optimizer: AdamW-0.0005\n",
      "24-09-10 09:28:15 - INFO - Encoding Dim: 60, Optimizer: AdamW-0.0005, MSE: 0.0002740595766179933\n",
      "24-09-10 09:28:15 - INFO - Training with optimizer: AdamW-0.0007\n",
      "24-09-10 09:32:15 - INFO - Encoding Dim: 60, Optimizer: AdamW-0.0007, MSE: 0.0003598786411621432\n",
      "24-09-10 09:32:15 - INFO - Optimal Encoding Dimension: 60, Optimizer: AdamW-0.0005, MSE: 0.0002740595766179933\n",
      "24-09-10 09:56:05 - INFO - Training with optimizer: AdamW-0.0007\n",
      "24-09-10 10:00:25 - INFO - Encoding Dim: 60, Optimizer: AdamW-0.0007, MSE: 0.00030134205428094786\n",
      "24-09-10 10:00:25 - INFO - Optimal Encoding Dimension: 60, Optimizer: AdamW-0.0007, MSE: 0.00030134205428094786\n",
      "24-09-10 10:03:34 - INFO - Training with optimizer: AdamW-0.0007\n",
      "24-09-10 10:08:44 - INFO - Encoding Dim: 60, Optimizer: AdamW-0.0007, MSE: 0.0002574492192810999\n",
      "24-09-10 10:08:44 - INFO - Optimal Encoding Dimension: 60, Optimizer: AdamW-0.0007, MSE: 0.0002574492192810999\n",
      "24-09-10 10:11:25 - INFO - Training with optimizer: AdamW-0.0008\n",
      "24-09-10 10:15:06 - INFO - Encoding Dim: 60, Optimizer: AdamW-0.0008, MSE: 0.0005478524931635507\n",
      "24-09-10 10:15:06 - INFO - Optimal Encoding Dimension: 60, Optimizer: AdamW-0.0008, MSE: 0.0005478524931635507\n",
      "24-09-10 10:17:01 - INFO - Training with optimizer: AdamW-0.0006\n",
      "24-09-10 10:21:56 - INFO - Encoding Dim: 60, Optimizer: AdamW-0.0006, MSE: 0.00031997811659368306\n",
      "24-09-10 10:21:57 - INFO - Optimal Encoding Dimension: 60, Optimizer: AdamW-0.0006, MSE: 0.00031997811659368306\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "with open('training_log.log', 'r') as f:\n",
    "    print(f.read())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5328f1b1-40cc-4dc2-9ce8-c154f65e6a06",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-11T11:38:36.259829Z",
     "iopub.status.busy": "2024-09-11T11:38:36.258886Z",
     "iopub.status.idle": "2024-09-11T11:41:35.296290Z",
     "shell.execute_reply": "2024-09-11T11:41:35.295414Z",
     "shell.execute_reply.started": "2024-09-11T11:38:36.259809Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/35\n",
      "1176/1176 [==============================] - 6s 4ms/step - loss: 0.0034 - mse: 0.0034 - val_loss: 0.0011 - val_mse: 0.0011 - lr: 0.0010\n",
      "Epoch 2/35\n",
      "1176/1176 [==============================] - 5s 4ms/step - loss: 0.0327 - mse: 0.0327 - val_loss: 0.0029 - val_mse: 0.0029 - lr: 0.0010\n",
      "Epoch 3/35\n",
      "1176/1176 [==============================] - 5s 4ms/step - loss: 0.0025 - mse: 0.0025 - val_loss: 0.0021 - val_mse: 0.0021 - lr: 0.0010\n",
      "Epoch 4/35\n",
      "1176/1176 [==============================] - 4s 4ms/step - loss: 0.0018 - mse: 0.0018 - val_loss: 0.0015 - val_mse: 0.0015 - lr: 0.0010\n",
      "Epoch 5/35\n",
      "1176/1176 [==============================] - 5s 4ms/step - loss: 0.0014 - mse: 0.0014 - val_loss: 0.0013 - val_mse: 0.0013 - lr: 5.0000e-04\n",
      "Epoch 6/35\n",
      "1176/1176 [==============================] - 5s 4ms/step - loss: 0.0012 - mse: 0.0012 - val_loss: 0.0011 - val_mse: 0.0011 - lr: 5.0000e-04\n",
      "Epoch 7/35\n",
      "1176/1176 [==============================] - 5s 4ms/step - loss: 0.0010 - mse: 0.0010 - val_loss: 9.2550e-04 - val_mse: 9.2550e-04 - lr: 5.0000e-04\n",
      "Epoch 8/35\n",
      "1176/1176 [==============================] - 5s 4ms/step - loss: 8.8675e-04 - mse: 8.8675e-04 - val_loss: 8.1072e-04 - val_mse: 8.1072e-04 - lr: 5.0000e-04\n",
      "Epoch 9/35\n",
      "1176/1176 [==============================] - 5s 4ms/step - loss: 7.8953e-04 - mse: 7.8953e-04 - val_loss: 7.7864e-04 - val_mse: 7.7864e-04 - lr: 5.0000e-04\n",
      "Epoch 10/35\n",
      "1176/1176 [==============================] - 5s 4ms/step - loss: 7.4733e-04 - mse: 7.4733e-04 - val_loss: 6.9944e-04 - val_mse: 6.9944e-04 - lr: 5.0000e-04\n",
      "Epoch 11/35\n",
      "1176/1176 [==============================] - 5s 4ms/step - loss: 0.0010 - mse: 0.0010 - val_loss: 7.7617e-04 - val_mse: 7.7617e-04 - lr: 5.0000e-04\n",
      "Epoch 12/35\n",
      "1176/1176 [==============================] - 5s 4ms/step - loss: 6.3549e-04 - mse: 6.3549e-04 - val_loss: 6.2928e-04 - val_mse: 6.2928e-04 - lr: 5.0000e-04\n",
      "Epoch 13/35\n",
      "1176/1176 [==============================] - 4s 4ms/step - loss: 5.8667e-04 - mse: 5.8667e-04 - val_loss: 5.8624e-04 - val_mse: 5.8624e-04 - lr: 5.0000e-04\n",
      "Epoch 14/35\n",
      "1176/1176 [==============================] - 4s 4ms/step - loss: 5.6574e-04 - mse: 5.6574e-04 - val_loss: 5.3374e-04 - val_mse: 5.3374e-04 - lr: 5.0000e-04\n",
      "Epoch 15/35\n",
      "1176/1176 [==============================] - 5s 4ms/step - loss: 0.0010 - mse: 0.0010 - val_loss: 0.0011 - val_mse: 0.0011 - lr: 5.0000e-04\n",
      "Epoch 16/35\n",
      "1176/1176 [==============================] - 5s 4ms/step - loss: 6.7654e-04 - mse: 6.7654e-04 - val_loss: 5.5165e-04 - val_mse: 5.5165e-04 - lr: 5.0000e-04\n",
      "Epoch 17/35\n",
      "1176/1176 [==============================] - 5s 4ms/step - loss: 5.2009e-04 - mse: 5.2009e-04 - val_loss: 5.1202e-04 - val_mse: 5.1202e-04 - lr: 2.5000e-04\n",
      "Epoch 18/35\n",
      "1176/1176 [==============================] - 5s 4ms/step - loss: 4.9749e-04 - mse: 4.9749e-04 - val_loss: 4.8564e-04 - val_mse: 4.8564e-04 - lr: 2.5000e-04\n",
      "Epoch 19/35\n",
      "1176/1176 [==============================] - 5s 4ms/step - loss: 4.8129e-04 - mse: 4.8129e-04 - val_loss: 4.7890e-04 - val_mse: 4.7890e-04 - lr: 2.5000e-04\n",
      "Epoch 20/35\n",
      "1176/1176 [==============================] - 4s 4ms/step - loss: 4.7698e-04 - mse: 4.7698e-04 - val_loss: 4.7295e-04 - val_mse: 4.7295e-04 - lr: 2.5000e-04\n",
      "Epoch 21/35\n",
      "1176/1176 [==============================] - 5s 5ms/step - loss: 4.7506e-04 - mse: 4.7506e-04 - val_loss: 5.0316e-04 - val_mse: 5.0316e-04 - lr: 2.5000e-04\n",
      "Epoch 22/35\n",
      "1176/1176 [==============================] - 5s 4ms/step - loss: 4.6011e-04 - mse: 4.6011e-04 - val_loss: 4.6554e-04 - val_mse: 4.6554e-04 - lr: 1.2500e-04\n",
      "Epoch 23/35\n",
      "1176/1176 [==============================] - 4s 4ms/step - loss: 4.5999e-04 - mse: 4.5999e-04 - val_loss: 4.6499e-04 - val_mse: 4.6499e-04 - lr: 1.2500e-04\n",
      "Epoch 24/35\n",
      "1176/1176 [==============================] - 5s 4ms/step - loss: 4.5795e-04 - mse: 4.5795e-04 - val_loss: 4.6134e-04 - val_mse: 4.6134e-04 - lr: 1.2500e-04\n",
      "Epoch 25/35\n",
      "1176/1176 [==============================] - 5s 4ms/step - loss: 4.5030e-04 - mse: 4.5030e-04 - val_loss: 4.5508e-04 - val_mse: 4.5508e-04 - lr: 6.2500e-05\n",
      "Epoch 26/35\n",
      "1176/1176 [==============================] - 5s 4ms/step - loss: 4.4579e-04 - mse: 4.4579e-04 - val_loss: 4.4994e-04 - val_mse: 4.4994e-04 - lr: 6.2500e-05\n",
      "Epoch 27/35\n",
      "1176/1176 [==============================] - 5s 4ms/step - loss: 4.4074e-04 - mse: 4.4074e-04 - val_loss: 4.4409e-04 - val_mse: 4.4409e-04 - lr: 6.2500e-05\n",
      "Epoch 28/35\n",
      "1176/1176 [==============================] - 5s 4ms/step - loss: 4.3496e-04 - mse: 4.3496e-04 - val_loss: 4.3986e-04 - val_mse: 4.3986e-04 - lr: 3.1250e-05\n",
      "Epoch 29/35\n",
      "1176/1176 [==============================] - 4s 4ms/step - loss: 4.3235e-04 - mse: 4.3235e-04 - val_loss: 4.3785e-04 - val_mse: 4.3785e-04 - lr: 3.1250e-05\n",
      "Epoch 30/35\n",
      "1176/1176 [==============================] - 5s 4ms/step - loss: 4.3090e-04 - mse: 4.3090e-04 - val_loss: 4.3702e-04 - val_mse: 4.3702e-04 - lr: 3.1250e-05\n",
      "Epoch 31/35\n",
      "1176/1176 [==============================] - 4s 4ms/step - loss: 4.2957e-04 - mse: 4.2957e-04 - val_loss: 4.3592e-04 - val_mse: 4.3592e-04 - lr: 1.5625e-05\n",
      "Epoch 32/35\n",
      "1176/1176 [==============================] - 5s 4ms/step - loss: 4.2919e-04 - mse: 4.2919e-04 - val_loss: 4.3551e-04 - val_mse: 4.3551e-04 - lr: 1.5625e-05\n",
      "Epoch 33/35\n",
      "1176/1176 [==============================] - 5s 4ms/step - loss: 4.2884e-04 - mse: 4.2884e-04 - val_loss: 4.3511e-04 - val_mse: 4.3511e-04 - lr: 1.5625e-05\n",
      "Epoch 34/35\n",
      "1176/1176 [==============================] - 5s 4ms/step - loss: 4.2828e-04 - mse: 4.2828e-04 - val_loss: 4.3477e-04 - val_mse: 4.3477e-04 - lr: 7.8125e-06\n",
      "Epoch 35/35\n",
      "1176/1176 [==============================] - 5s 4ms/step - loss: 4.2812e-04 - mse: 4.2812e-04 - val_loss: 4.3463e-04 - val_mse: 4.3463e-04 - lr: 7.8125e-06\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "from tensorflow.keras.optimizers import AdamW  # Import AdamW optimizer\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau  # Early stopping\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tensorflow.keras.utils import Sequence\n",
    "from tensorflow.keras.layers import LeakyReLU\n",
    "\n",
    "# Generator class for batch processing\n",
    "class DataGenerator(Sequence):\n",
    "    def __init__(self, data, batch_size=128):\n",
    "        self.data = data\n",
    "        self.batch_size = batch_size\n",
    "        self.indices = np.arange(len(data))\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.data) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        batch_indices = self.indices[index * self.batch_size:(index + 1) * self.batch_size]\n",
    "        batch_data = self.data.iloc[batch_indices].values\n",
    "        return batch_data, batch_data\n",
    "\n",
    "# Function to build the autoencoder model with customizable activations\n",
    "def build_autoencoder(input_dim, encoding_dim, optimizer, activation_fn_encoder='LeakyReLU', activation_fn_decoder='linear'):\n",
    "    input_layer = Input(shape=(input_dim,))\n",
    "\n",
    "    # Encoder layers\n",
    "    encoder = Dense(512, activation=activation_fn_encoder)(input_layer)\n",
    "    encoder = Dense(256, activation=activation_fn_encoder)(encoder)\n",
    "    encoder = Dense(encoding_dim, activation=activation_fn_encoder)(encoder)\n",
    "\n",
    "    # Decoder layers\n",
    "    decoder = Dense(256, activation=activation_fn_decoder)(encoder)\n",
    "    decoder = Dense(512, activation=activation_fn_decoder)(decoder)\n",
    "    decoder = Dense(input_dim, activation=activation_fn_decoder)(decoder)\n",
    "\n",
    "    autoencoder = Model(inputs=input_layer, outputs=decoder)\n",
    "    autoencoder.compile(optimizer=optimizer, loss=\"mse\", metrics=['mse'])\n",
    "    \n",
    "    return autoencoder, Model(inputs=input_layer, outputs=encoder)\n",
    "\n",
    "\n",
    "\n",
    "# Train the model and save it\n",
    "def train_and_save_autoencoder(df, encoding_dim=60, epochs=35, batch_size=128, activation_fn_encoder='LeakyReLU', activation_fn_decoder='linear'):\n",
    "    input_dim = df.shape[1]\n",
    "    optimizer = AdamW(learning_rate=0.001, weight_decay=0.01)  # Using AdamW optimizer\n",
    "\n",
    "    # Build the autoencoder\n",
    "    autoencoder, encoder = build_autoencoder(\n",
    "        input_dim, encoding_dim, optimizer,\n",
    "        activation_fn_encoder=activation_fn_encoder,\n",
    "        activation_fn_decoder=activation_fn_decoder\n",
    "    )\n",
    "\n",
    "    # Train the model\n",
    "    x_train, x_val = train_test_split(df, test_size=0.1, random_state=42)\n",
    "    train_generator = DataGenerator(x_train, batch_size=batch_size)\n",
    "    val_generator = DataGenerator(x_val, batch_size=batch_size)\n",
    "\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=1e-6)\n",
    "\n",
    "    history = autoencoder.fit(train_generator,\n",
    "                              epochs=epochs,\n",
    "                              validation_data=val_generator,\n",
    "                              callbacks=[early_stopping,reduce_lr])\n",
    "    \n",
    "    # Log the final loss\n",
    "    final_loss = history.history['val_loss'][-1]\n",
    "    logging.info(f\"Final Validation Loss: {final_loss}\")\n",
    "\n",
    "    # Save the autoencoder and encoder\n",
    "    autoencoder.save('autoencoder_model.h5')\n",
    "    encoder.save('encoder_model.h5')\n",
    "\n",
    "    return final_loss\n",
    "\n",
    "# Assuming `df` is your DataFrame with scaled data\n",
    "final_loss = train_and_save_autoencoder(df, encoding_dim=60, epochs=35, batch_size=128, activation_fn_encoder='LeakyReLU', activation_fn_decoder='linear')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "be36b294-3a34-45df-bd68-6e6882ddf55a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-27T09:47:06.690529Z",
     "iopub.status.busy": "2024-09-27T09:47:06.690304Z",
     "iopub.status.idle": "2024-09-27T09:47:15.386149Z",
     "shell.execute_reply": "2024-09-27T09:47:15.385596Z",
     "shell.execute_reply.started": "2024-09-27T09:47:06.690512Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-27 09:47:06.827977: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-27 09:47:06.881429: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-09-27 09:47:06.881613: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-09-27 09:47:06.882635: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-09-27 09:47:06.882770: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-09-27 09:47:06.882859: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-09-27 09:47:08.604644: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-09-27 09:47:08.604808: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-09-27 09:47:08.604914: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-09-27 09:47:08.604993: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 46689 MB memory:  -> device: 0, name: NVIDIA RTX A6000, pci bus id: 0000:00:05.0, compute capability: 8.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 2s 2ms/step\n",
      "4/4 [==============================] - 0s 1ms/step\n",
      "4/4 [==============================] - 0s 1ms/step\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "4/4 [==============================] - 0s 1ms/step\n",
      "4/4 [==============================] - 0s 1ms/step\n",
      "4/4 [==============================] - 0s 1ms/step\n",
      "4/4 [==============================] - 0s 1ms/step\n",
      "4/4 [==============================] - 0s 1ms/step\n",
      "4/4 [==============================] - 0s 1ms/step\n",
      "4/4 [==============================] - 0s 1ms/step\n",
      "4/4 [==============================] - 0s 1ms/step\n",
      "4/4 [==============================] - 0s 1ms/step\n",
      "4/4 [==============================] - 0s 978us/step\n",
      "4/4 [==============================] - 0s 1ms/step\n",
      "4/4 [==============================] - 0s 1ms/step\n",
      "4/4 [==============================] - 0s 1ms/step\n",
      "4/4 [==============================] - 0s 1ms/step\n",
      "4/4 [==============================] - 0s 1ms/step\n",
      "4/4 [==============================] - 0s 1ms/step\n",
      "4/4 [==============================] - 0s 1ms/step\n",
      "4/4 [==============================] - 0s 1ms/step\n",
      "4/4 [==============================] - 0s 1ms/step\n",
      "4/4 [==============================] - 0s 1ms/step\n",
      "4/4 [==============================] - 0s 1ms/step\n",
      "4/4 [==============================] - 0s 1ms/step\n",
      "4/4 [==============================] - 0s 1ms/step\n",
      "4/4 [==============================] - 0s 1ms/step\n",
      "4/4 [==============================] - 0s 1ms/step\n",
      "4/4 [==============================] - 0s 1ms/step\n",
      "4/4 [==============================] - 0s 1ms/step\n",
      "4/4 [==============================] - 0s 1ms/step\n",
      "4/4 [==============================] - 0s 1ms/step\n",
      "4/4 [==============================] - 0s 1ms/step\n",
      "4/4 [==============================] - 0s 1ms/step\n",
      "4/4 [==============================] - 0s 1ms/step\n",
      "4/4 [==============================] - 0s 1ms/step\n",
      "4/4 [==============================] - 0s 1ms/step\n",
      "4/4 [==============================] - 0s 1ms/step\n",
      "4/4 [==============================] - 0s 1ms/step\n",
      "4/4 [==============================] - 0s 1ms/step\n",
      "4/4 [==============================] - 0s 1ms/step\n",
      "4/4 [==============================] - 0s 1ms/step\n",
      "4/4 [==============================] - 0s 1ms/step\n",
      "4/4 [==============================] - 0s 1ms/step\n",
      "4/4 [==============================] - 0s 1ms/step\n",
      "4/4 [==============================] - 0s 1ms/step\n",
      "4/4 [==============================] - 0s 1ms/step\n",
      "4/4 [==============================] - 0s 1ms/step\n",
      "4/4 [==============================] - 0s 1ms/step\n",
      "4/4 [==============================] - 0s 1ms/step\n",
      "4/4 [==============================] - 0s 1ms/step\n",
      "4/4 [==============================] - 0s 1ms/step\n",
      "4/4 [==============================] - 0s 1ms/step\n",
      "4/4 [==============================] - 0s 1ms/step\n",
      "4/4 [==============================] - 0s 1ms/step\n",
      "4/4 [==============================] - 0s 1ms/step\n",
      "4/4 [==============================] - 0s 1ms/step\n",
      "4/4 [==============================] - 0s 1ms/step\n",
      "4/4 [==============================] - 0s 1ms/step\n",
      "4/4 [==============================] - 0s 1ms/step\n",
      "4/4 [==============================] - 0s 1ms/step\n",
      "4/4 [==============================] - 0s 1ms/step\n",
      "4/4 [==============================] - 0s 1ms/step\n",
      "4/4 [==============================] - 0s 1ms/step\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "4/4 [==============================] - 0s 1ms/step\n",
      "4/4 [==============================] - 0s 1ms/step\n",
      "4/4 [==============================] - 0s 1ms/step\n",
      "4/4 [==============================] - 0s 1ms/step\n",
      "4/4 [==============================] - 0s 1ms/step\n",
      "4/4 [==============================] - 0s 1ms/step\n",
      "4/4 [==============================] - 0s 1ms/step\n",
      "4/4 [==============================] - 0s 1ms/step\n",
      "4/4 [==============================] - 0s 1ms/step\n",
      "4/4 [==============================] - 0s 1ms/step\n",
      "4/4 [==============================] - 0s 1ms/step\n",
      "4/4 [==============================] - 0s 1ms/step\n",
      "4/4 [==============================] - 0s 1ms/step\n",
      "4/4 [==============================] - 0s 1ms/step\n",
      "4/4 [==============================] - 0s 1ms/step\n",
      "4/4 [==============================] - 0s 1ms/step\n",
      "4/4 [==============================] - 0s 1ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Load the saved encoder model\n",
    "encoder = load_model('encoder_model.h5')\n",
    "\n",
    "# Function to reduce the DataFrame using the encoder model\n",
    "def reduce_dataframe(df, encoder, batch_size=128):\n",
    "    df_reduced = []\n",
    "    \n",
    "    # Iterate over the DataFrame in batches\n",
    "    for i in range(0, len(df), batch_size):\n",
    "        batch = df.iloc[i:i + batch_size].values  # Convert batch to numpy array\n",
    "        reduced_batch = encoder.predict(batch)    # Get the reduced representation\n",
    "        df_reduced.append(reduced_batch)\n",
    "\n",
    "    # Convert the reduced data back into a DataFrame\n",
    "    df_reduced = pd.DataFrame(np.vstack(df_reduced), index=df.index)\n",
    "    \n",
    "    return df_reduced\n",
    "\n",
    "# Assuming `df` is your original DataFrame with the data to be reduced\n",
    "df_reduced = reduce_dataframe(df, encoder)\n",
    "\n",
    "# Now `df_reduced` contains the reduced-dimensional representation of the original `df`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4184961e-834e-49e6-abd2-87f51f8fa97c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-27T09:48:53.936467Z",
     "iopub.status.busy": "2024-09-27T09:48:53.935977Z",
     "iopub.status.idle": "2024-09-27T09:48:54.311835Z",
     "shell.execute_reply": "2024-09-27T09:48:54.311268Z",
     "shell.execute_reply.started": "2024-09-27T09:48:53.936447Z"
    }
   },
   "outputs": [],
   "source": [
    "df_reduced.to_csv(\"reduced_testing.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ef2d51b-ff44-46a1-a31e-4dd137da7359",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-09-09T17:21:18.949636Z",
     "iopub.status.idle": "2024-09-09T17:21:18.950066Z",
     "shell.execute_reply": "2024-09-09T17:21:18.949990Z",
     "shell.execute_reply.started": "2024-09-09T17:21:18.949980Z"
    }
   },
   "outputs": [],
   "source": [
    "61, 0.000529 --"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35869d93-57b2-4f6b-a018-8c6a9e9364b8",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-09-09T17:21:18.950500Z",
     "iopub.status.idle": "2024-09-09T17:21:18.950910Z",
     "shell.execute_reply": "2024-09-09T17:21:18.950837Z",
     "shell.execute_reply.started": "2024-09-09T17:21:18.950827Z"
    }
   },
   "outputs": [],
   "source": [
    "24-09-09 07:37:27 - INFO - Training with optimizer: Adam\n",
    "24-09-09 07:42:58 - INFO - Encoding Dim: 60, Optimizer: Adam, MSE: 0.00039134073902094736\n",
    "24-09-09 07:42:58 - INFO - Training with optimizer: SGD\n",
    "24-09-09 07:48:10 - INFO - Encoding Dim: 60, Optimizer: SGD, MSE: 0.019344559345294512\n",
    "24-09-09 07:48:10 - INFO - Training with optimizer: Nadam\n",
    "24-09-09 07:52:58 - INFO - Encoding Dim: 60, Optimizer: Nadam, MSE: 0.0004165755366704315\n",
    "24-09-09 07:52:58 - INFO - Training with optimizer: RMSprop\n",
    "24-09-09 07:58:02 - INFO - Encoding Dim: 60, Optimizer: RMSprop, MSE: 0.001750231789881493\n",
    "24-09-09 07:58:03 - INFO - Training with optimizer: Adadelta\n",
    "24-09-09 08:03:13 - INFO - Encoding Dim: 60, Optimizer: Adadelta, MSE: 0.02012077012827505\n",
    "24-09-09 08:03:13 - INFO - Training with optimizer: Adagrad\n",
    "24-09-09 08:08:17 - INFO - Encoding Dim: 60, Optimizer: Adagrad, MSE: 0.015115404734940944\n",
    "24-09-09 08:08:18 - INFO - Training with optimizer: Adamax\n",
    "24-09-09 08:13:20 - INFO - Encoding Dim: 60, Optimizer: Adamax, MSE: 0.000525860126003413\n",
    "24-09-09 08:13:20 - INFO - Training with optimizer: FTRL\n",
    "24-09-09 15:58:44 - INFO - Training with optimizer: FTRL\n",
    "24-09-09 16:03:30 - INFO - Encoding Dim: 60, Optimizer: FTRL, MSE: 0.12035803139232165\n",
    "24-09-09 16:03:30 - INFO - Training with optimizer: SGDW\n",
    "24-09-09 16:08:25 - INFO - Encoding Dim: 60, Optimizer: SGDW, MSE: 0.021841895117691135\n",
    "24-09-09 16:08:25 - INFO - Training with optimizer: AdamW\n",
    "24-09-09 16:13:31 - INFO - Encoding Dim: 60, Optimizer: AdamW, MSE: 0.0004266640576258005\n",
    "24-09-09 16:13:31 - INFO - Optimal Encoding Dimension: 60, Optimizer: AdamW, MSE: 0.0004266640576258005\n",
    "24-09-09 16:22:55 - INFO - Training with optimizer: Adam\n",
    "24-09-09 16:28:17 - INFO - Encoding Dim: 60, Optimizer: Adam, MSE: 0.000633563765379747\n",
    "24-09-09 16:28:17 - INFO - Training with optimizer: SGD\n",
    "24-09-09 16:33:27 - INFO - Encoding Dim: 60, Optimizer: SGD, MSE: 0.019119386343005133\n",
    "24-09-09 16:33:28 - INFO - Training with optimizer: Nadam\n",
    "24-09-09 16:36:59 - INFO - Encoding Dim: 60, Optimizer: Nadam, MSE: 0.0006476806549614504\n",
    "24-09-09 16:36:59 - INFO - Training with optimizer: RMSprop\n",
    "24-09-09 16:41:56 - INFO - Encoding Dim: 60, Optimizer: RMSprop, MSE: 0.0011563050667090101\n",
    "24-09-09 16:41:57 - INFO - Training with optimizer: Adadelta\n",
    "24-09-09 16:46:55 - INFO - Encoding Dim: 60, Optimizer: Adadelta, MSE: 0.01883450426527251\n",
    "24-09-09 16:46:56 - INFO - Training with optimizer: Adagrad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18b9e89a-014c-4ecd-915f-79c7bbf54583",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efe78ffc-0360-40ec-9191-301b8daccf25",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05a4b3a7-c3f3-458b-ace8-191043c9f919",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e43cd208-7f2d-4b49-a88a-3f79e96e55f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1daaffb-1626-460c-85e2-c7c090c5ae6f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a938bea-9e16-47a7-915f-2003aeb3caf3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ee45bad-92c7-48fe-a0d1-7c499d43b9d9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
